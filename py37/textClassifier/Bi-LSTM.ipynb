{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [256, 256]  # 单层LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 1  # 二分类设置为1，多分类设置为类别的数目\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "youre不存在于词向量中\n",
      "youll不存在于词向量中\n",
      "theyre不存在于词向量中\n",
      "youve不存在于词向量中\n",
      "werent不存在于词向量中\n",
      "youd不存在于词向量中\n",
      "hasnt不存在于词向量中\n",
      "shouldnt不存在于词向量中\n",
      "weve不存在于词向量中\n",
      "theyve不存在于词向量中\n",
      "1010不存在于词向量中\n",
      "wouldve不存在于词向量中\n",
      "hed不存在于词向量中\n",
      "andor不存在于词向量中\n",
      "couldve不存在于词向量中\n",
      "810不存在于词向量中\n",
      "itthe不存在于词向量中\n",
      "710不存在于词向量中\n",
      "theyd不存在于词向量中\n",
      "writerdirector不存在于词向量中\n",
      "moviei不存在于词向量中\n",
      "iti不存在于词向量中\n",
      "theyll不存在于词向量中\n",
      "310不存在于词向量中\n",
      "910不存在于词向量中\n",
      "410不存在于词向量中\n",
      "itll不存在于词向量中\n",
      "lees不存在于词向量中\n",
      "familys不存在于词向量中\n",
      "disneys不存在于词向量中\n",
      "filmi不存在于词向量中\n",
      "210不存在于词向量中\n",
      "shouldve不存在于词向量中\n",
      "*12不存在于词向量中\n",
      "shakespeares不存在于词向量中\n",
      "hitlers不存在于词向量中\n",
      "freddys不存在于词向量中\n",
      "brazil:不存在于词向量中\n",
      "fords不存在于词向量中\n",
      "storys不存在于词向量中\n",
      "ohara不存在于词向量中\n",
      "stewarts不存在于词向量中\n",
      "kellys不存在于词向量中\n",
      "itthis不存在于词向量中\n",
      "tonys不存在于词向量中\n",
      "scotts不存在于词向量中\n",
      "keatons不存在于词向量中\n",
      "rosemarys不存在于词向量中\n",
      "ches不存在于词向量中\n",
      "timethe不存在于词向量中\n",
      "themthe不存在于词向量中\n",
      "branaghs不存在于词向量中\n",
      "mustve不存在于词向量中\n",
      "kubricks不存在于词向量中\n",
      "hitchcocks不存在于词向量中\n",
      "lynchs不存在于词向量中\n",
      "bakshis不存在于词向量中\n",
      "allthe不存在于词向量中\n",
      "whove不存在于词向量中\n",
      "whod不存在于词向量中\n",
      "itit不存在于词向量中\n",
      "smiths不存在于词向量中\n",
      "obrien不存在于词向量中\n",
      "cravens不存在于词向量中\n",
      "palmas不存在于词向量中\n",
      "timei不存在于词向量中\n",
      "movieit不存在于词向量中\n",
      "waythe不存在于词向量中\n",
      "himthe不存在于词向量中\n",
      "hartleys不存在于词向量中\n",
      "altmans不存在于词向量中\n",
      "itbut不存在于词向量中\n",
      "storythe不存在于词向量中\n",
      "taime不存在于词向量中\n",
      "methe不存在于词向量中\n",
      "verhoevens不存在于词向量中\n",
      "filmthis不存在于词向量中\n",
      "romeros不存在于词向量中\n",
      "filmit不存在于词向量中\n",
      "oneill不存在于词向量中\n",
      "miyazakis不存在于词向量中\n",
      "countrys不存在于词向量中\n",
      "eastwoods不存在于词向量中\n",
      "sinatras不存在于词向量中\n",
      "010不存在于词向量中\n",
      "movieand不存在于词向量中\n",
      "cmon不存在于词向量中\n",
      "victorias不存在于词向量中\n",
      "harrys不存在于词向量中\n",
      "movieits不存在于词向量中\n",
      "everybodys不存在于词向量中\n",
      "thatll不存在于词向量中\n",
      "citys不存在于词向量中\n",
      "spielbergs不存在于词向量中\n",
      "pauls不存在于词向量中\n",
      "hisher不存在于词向量中\n",
      "itd不存在于词向量中\n",
      "goodthe不存在于词向量中\n",
      "imdbs不存在于词向量中\n",
      "streisands不存在于词向量中\n",
      "moviethis不存在于词向量中\n",
      "gilligans不存在于词向量中\n",
      "otoole不存在于词向量中\n",
      "ladys不存在于词向量中\n",
      "itits不存在于词向量中\n",
      "pacinos不存在于词向量中\n",
      "teds不存在于词向量中\n",
      "actorsactresses不存在于词向量中\n",
      "heshe不存在于词向量中\n",
      "directorwriter不存在于词向量中\n",
      "austens不存在于词向量中\n",
      "herethe不存在于词向量中\n",
      "onethe不存在于词向量中\n",
      "alli不存在于词向量中\n",
      "badthe不存在于词向量中\n",
      "osullivan不存在于词向量中\n",
      "bens不存在于词向量中\n",
      "fulcis不存在于词向量中\n",
      "therethe不存在于词向量中\n",
      "singin不存在于词向量中\n",
      "elviras不存在于词向量中\n",
      "brandos不存在于词向量中\n",
      "arthurs不存在于词向量中\n",
      "cusacks不存在于词向量中\n",
      "taylors不存在于词向量中\n",
      "wellthe不存在于词向量中\n",
      "nobodys不存在于词向量中\n",
      "betterthe不存在于词向量中\n",
      "thisthe不存在于词向量中\n",
      "lifethe不存在于词向量中\n",
      "toothe不存在于词向量中\n",
      "goldsworthys不存在于词向量中\n",
      "jims不存在于词向量中\n",
      "londons不存在于词向量中\n",
      "manns不存在于词向量中\n",
      "itif不存在于词向量中\n",
      "burtons不存在于词向量中\n",
      "youngs不存在于词向量中\n",
      "joes不存在于词向量中\n",
      "dereks不存在于词向量中\n",
      "thati不存在于词向量中\n",
      "santas不存在于词向量中\n",
      "kurosawas不存在于词向量中\n",
      "horrorthriller不存在于词向量中\n",
      "johnsons不存在于词向量中\n",
      "toms不存在于词向量中\n",
      "lincolns不存在于词向量中\n",
      "moviesthe不存在于词向量中\n",
      "dawsons不存在于词向量中\n",
      "moviethere不存在于词向量中\n",
      "oneal不存在于词向量中\n",
      "powells不存在于词向量中\n",
      "onthe不存在于词向量中\n",
      "soderberghs不存在于词向量中\n",
      "stanwycks不存在于词向量中\n",
      "caines不存在于词向量中\n",
      "chaplins不存在于词向量中\n",
      "thisi不存在于词向量中\n",
      "isthe不存在于词向量中\n",
      "outthe不存在于词向量中\n",
      "themthis不存在于词向量中\n",
      "againthe不存在于词向量中\n",
      "tolkiens不存在于词向量中\n",
      "worldthe不存在于词向量中\n",
      "flynns不存在于词向量中\n",
      "stallones不存在于词向量中\n",
      "fromthe不存在于词向量中\n",
      "logans不存在于词向量中\n",
      "andys不存在于词向量中\n",
      "funnyi不存在于词向量中\n",
      "erics不存在于词向量中\n",
      "herthe不存在于词向量中\n",
      ";-不存在于词向量中\n",
      "oni不存在于词向量中\n",
      "annas不存在于词向量中\n",
      "marys不存在于词向量中\n",
      "neednt不存在于词向量中\n",
      "yorks不存在于词向量中\n",
      "anns不存在于词向量中\n",
      "rolethe不存在于词向量中\n",
      "brontes不存在于词向量中\n",
      "jackies不存在于词向量中\n",
      "antonionis不存在于词向量中\n",
      "henrys不存在于词向量中\n",
      "moviebut不存在于词向量中\n",
      "charactersthe不存在于词向量中\n",
      "sirks不存在于词向量中\n",
      "funnythe不存在于词向量中\n",
      "capotes不存在于词向量中\n",
      "satans不存在于词向量中\n",
      "sutherlands不存在于词向量中\n",
      "mightve不存在于词向量中\n",
      "gilliams不存在于词向量中\n",
      "oclock不存在于词向量中\n",
      "lukes不存在于词向量中\n",
      "washingtons不存在于词向量中\n",
      "vonneguts不存在于词向量中\n",
      "morethe不存在于词向量中\n",
      "sullivans不存在于词向量中\n",
      "goodi不存在于词向量中\n",
      "polanskis不存在于词向量中\n",
      "dannys不存在于词向量中\n",
      "macarthurs不存在于词向量中\n",
      "510不存在于词向量中\n",
      "russells不存在于词向量中\n",
      "***12不存在于词向量中\n",
      "glovers不存在于词向量中\n",
      "timesthe不存在于词向量中\n",
      "610不存在于词向量中\n",
      "drews不存在于词向量中\n",
      "plotthe不存在于词向量中\n",
      "carreys不存在于词向量中\n",
      "rachels不存在于词向量中\n",
      "isi不存在于词向量中\n",
      "endthe不存在于词向量中\n",
      "itand不存在于词向量中\n",
      "zizeks不存在于词向量中\n",
      "societys不存在于词向量中\n",
      "screenthe不存在于词向量中\n",
      "companys不存在于词向量中\n",
      "endi不存在于词向量中\n",
      "madonnas不存在于词向量中\n",
      "itin不存在于词向量中\n",
      "filmif不存在于词向量中\n",
      "wayi不存在于词向量中\n",
      "dangelo不存在于词向量中\n",
      "minutesthe不存在于词向量中\n",
      "themi不存在于词向量中\n",
      "shaws不存在于词向量中\n",
      "betties不存在于词向量中\n",
      "seenthe不存在于词向量中\n",
      "aboutthe不存在于词向量中\n",
      "imdbcom不存在于词向量中\n",
      "beattys不存在于词向量中\n",
      "scorseses不存在于词向量中\n",
      "garbos不存在于词向量中\n",
      "mgms不存在于词向量中\n",
      "carlas不存在于词向量中\n",
      "girlfight不存在于词向量中\n",
      "lifei不存在于词向量中\n",
      "merk不存在于词向量中\n",
      "7510不存在于词向量中\n",
      "premingers不存在于词向量中\n",
      "filmbut不存在于词向量中\n",
      "threes不存在于词向量中\n",
      "scenesthe不存在于词向量中\n",
      "langs不存在于词向量中\n",
      "als不存在于词向量中\n",
      "seriesthe不存在于词向量中\n",
      "filmand不存在于词向量中\n",
      "clarks不存在于词向量中\n",
      "hardys不存在于词向量中\n",
      "supermans不存在于词向量中\n",
      "lugosis不存在于词向量中\n",
      "8510不存在于词向量中\n",
      "upthe不存在于词向量中\n",
      "matthaus不存在于词向量中\n",
      "millers不存在于词向量中\n",
      "parkers不存在于词向量中\n",
      "badi不存在于词向量中\n",
      "rochesters不存在于词向量中\n",
      "bethe不存在于词向量中\n",
      "rockin不存在于词向量中\n",
      "wholl不存在于词向量中\n",
      "oliviers不存在于词向量中\n",
      "eddies不存在于词向量中\n",
      "penns不存在于词向量中\n",
      "kyles不存在于词向量中\n",
      "actionadventure不存在于词向量中\n",
      "walkens不存在于词向量中\n",
      "hoffmans不存在于词向量中\n",
      "cuckoos不存在于词向量中\n",
      "gordons不存在于词向量中\n",
      "filma不存在于词向量中\n",
      "movieif不存在于词向量中\n",
      "characterthe不存在于词向量中\n",
      "det不存在于词向量中\n",
      "storyi不存在于词向量中\n",
      "cohens不存在于词向量中\n",
      "adv不存在于词向量中\n",
      "gilleys不存在于词向量中\n",
      "lilys不存在于词向量中\n",
      "fullers不存在于词向量中\n",
      "goauld不存在于词向量中\n",
      "kates不存在于词向量中\n",
      "cains不存在于词向量中\n",
      "fondas不存在于词向量中\n",
      "lennons不存在于词向量中\n",
      "dexters不存在于词向量中\n",
      "harlins不存在于词向量中\n",
      "sp不存在于词向量中\n",
      "thompsons不存在于词向量中\n",
      "anybodys不存在于词向量中\n",
      "moviesthis不存在于词向量中\n",
      "etcthe不存在于词向量中\n",
      "mummys不存在于词向量中\n",
      "johnnys不存在于词向量中\n",
      "welli不存在于词向量中\n",
      "bos不存在于词向量中\n",
      "freemans不存在于词向量中\n",
      "bikos不存在于词向量中\n",
      "emmas不存在于词向量中\n",
      "moviea不存在于词向量中\n",
      "japans不存在于词向量中\n",
      "bruces不存在于词向量中\n",
      "eighttitle不存在于词向量中\n",
      "wasi不存在于词向量中\n",
      "paulies不存在于词向量中\n",
      "muchthe不存在于词向量中\n",
      "woodys不存在于词向量中\n",
      "panahis不存在于词向量中\n",
      "novaks不存在于词向量中\n",
      "jrs不存在于词向量中\n",
      "visiteurs不存在于词向量中\n",
      "dixons不存在于词向量中\n",
      "bolls不存在于词向量中\n",
      "crashers不存在于词向量中\n",
      "porkys不存在于词向量中\n",
      "levins不存在于词向量中\n",
      "filmsthe不存在于词向量中\n",
      "brosnans不存在于词向量中\n",
      "onei不存在于词向量中\n",
      "nicholsons不存在于词向量中\n",
      "alexs不存在于词向量中\n",
      "annes不存在于词向量中\n",
      "damato不存在于词向量中\n",
      "widmarks不存在于词向量中\n",
      "rukhs不存在于词向量中\n",
      "actingthe不存在于词向量中\n",
      "alices不存在于词向量中\n",
      "storythis不存在于词向量中\n",
      "ollies不存在于词向量中\n",
      "daythe不存在于词向量中\n",
      "filmin不存在于词向量中\n",
      "jerrys不存在于词向量中\n",
      "maughams不存在于词向量中\n",
      "wagners不存在于词向量中\n",
      "familythe不存在于词向量中\n",
      "nancys不存在于词向量中\n",
      "englands不存在于词向量中\n",
      "youthe不存在于词向量中\n",
      "morgans不存在于词向量中\n",
      "frankies不存在于词向量中\n",
      "hestons不存在于词向量中\n",
      "movieso不存在于词向量中\n",
      "lumets不存在于词向量中\n",
      "perrys不存在于词向量中\n",
      "itthere不存在于词向量中\n",
      "actoractress不存在于词向量中\n",
      "viscontis不存在于词向量中\n",
      "moviesi不存在于词向量中\n",
      "lovehate不存在于词向量中\n",
      "notthe不存在于词向量中\n",
      "gibsons不存在于词向量中\n",
      "christys不存在于词向量中\n",
      "watchthe不存在于词向量中\n",
      "rohmers不存在于词向量中\n",
      "thirdly不存在于词向量中\n",
      "workthe不存在于词向量中\n",
      "attenboroughs不存在于词向量中\n",
      "lundgrens不存在于词向量中\n",
      "$10000不存在于词向量中\n",
      "camerons不存在于词向量中\n",
      "carters不存在于词向量中\n",
      "cormans不存在于词向量中\n",
      "poes不存在于词向量中\n",
      "khouris不存在于词向量中\n",
      "lis不存在于词向量中\n",
      "karloffs不存在于词向量中\n",
      "bbcs不存在于词向量中\n",
      "putain不存在于词向量中\n",
      "neds不存在于词向量中\n",
      "againi不存在于词向量中\n",
      "christs不存在于词向量中\n",
      "betteri不存在于词向量中\n",
      "connerys不存在于词向量中\n",
      "timethis不存在于词向量中\n",
      "didthe不存在于词向量中\n",
      "maries不存在于词向量中\n",
      "timons不存在于词向量中\n",
      "filmmy不存在于词向量中\n",
      "pecks不存在于词向量中\n",
      "anywaythe不存在于词向量中\n",
      "wilders不存在于词向量中\n",
      "peckinpahs不存在于词向量中\n",
      "quinns不存在于词向量中\n",
      "falks不存在于词向量中\n",
      "filmso不存在于词向量中\n",
      "goodbut不存在于词向量中\n",
      "heri不存在于词向量中\n",
      "streeps不存在于词向量中\n",
      "sidneys不存在于词向量中\n",
      "thoughthe不存在于词向量中\n",
      "oshea不存在于词向量中\n",
      "corbetts不存在于词向量中\n",
      "naschys不存在于词向量中\n",
      "funi不存在于词向量中\n",
      "jabbas不存在于词向量中\n",
      "ita不存在于词向量中\n",
      "argentos不存在于词向量中\n",
      "fishburnes不存在于词向量中\n",
      "timmys不存在于词向量中\n",
      "filmthere不存在于词向量中\n",
      "showthe不存在于词向量中\n",
      "universals不存在于词向量中\n",
      "madethe不存在于词向量中\n",
      "steves不存在于词向量中\n",
      "itwhat不存在于词向量中\n",
      "wrongi不存在于词向量中\n",
      "crowes不存在于词向量中\n",
      "martinos不存在于词向量中\n",
      "filmsi不存在于词向量中\n",
      "endthis不存在于词向量中\n",
      "alisons不存在于词向量中\n",
      "somebodys不存在于词向量中\n",
      "lesters不存在于词向量中\n",
      "ritters不存在于词向量中\n",
      "australias不存在于词向量中\n",
      "seventitle不存在于词向量中\n",
      "sarahs不存在于词向量中\n",
      "dominos不存在于词向量中\n",
      "macys不存在于词向量中\n",
      "dont!不存在于词向量中\n",
      "bergmans不存在于词向量中\n",
      "no1不存在于词向量中\n",
      "thisthis不存在于词向量中\n",
      "gershwins不存在于词向量中\n",
      "newmans不存在于词向量中\n",
      "donnas不存在于词向量中\n",
      "leos不存在于词向量中\n",
      "candys不存在于词向量中\n",
      "rien不存在于词向量中\n",
      "petes不存在于词向量中\n",
      "tooi不存在于词向量中\n",
      "tarantinos不存在于词向量中\n",
      "moviein不存在于词向量中\n",
      "seeni不存在于词向量中\n",
      "geddes不存在于词向量中\n",
      "felixs不存在于词向量中\n",
      "filmits不存在于词向量中\n",
      "amazoncom不存在于词向量中\n",
      "youi不存在于词向量中\n",
      "herei不存在于词向量中\n",
      "gannons不存在于词向量中\n",
      "fallons不存在于词向量中\n",
      "lifethis不存在于词向量中\n",
      "duvalls不存在于词向量中\n",
      "audiencethe不存在于词向量中\n",
      "greatthe不存在于词向量中\n",
      "jimmys不存在于词向量中\n",
      "oconnor不存在于词向量中\n",
      "wendys不存在于词向量中\n",
      "barkers不存在于词向量中\n",
      "kazans不存在于词向量中\n",
      "razors不存在于词向量中\n",
      "ityou不存在于词向量中\n",
      "babys不存在于词向量中\n",
      "wellit不存在于词向量中\n",
      "mabels不存在于词向量中\n",
      "fellinis不存在于词向量中\n",
      "tealc不存在于词向量中\n",
      "wrongthe不存在于词向量中\n",
      "dothe不存在于词向量中\n",
      "hustons不存在于词向量中\n",
      "nowi不存在于词向量中\n",
      "einsteins不存在于词向量中\n",
      "manthe不存在于词向量中\n",
      "joeys不存在于词向量中\n",
      "jennifers不存在于词向量中\n",
      "itas不存在于词向量中\n",
      "stillers不存在于词向量中\n",
      "timein不存在于词向量中\n",
      "guevaras不存在于词向量中\n",
      "noti不存在于词向量中\n",
      "harts不存在于词向量中\n",
      "burtynskys不存在于词向量中\n",
      "wilsons不存在于词向量中\n",
      "kaufmans不存在于词向量中\n",
      "wellthis不存在于词向量中\n",
      "eberts不存在于词向量中\n",
      "hepburns不存在于词向量中\n",
      "belushis不存在于词向量中\n",
      "otherthe不存在于词向量中\n",
      "movieyou不存在于词向量中\n",
      "godards不存在于词向量中\n",
      "roys不存在于词向量中\n",
      "everythings不存在于词向量中\n",
      "sic不存在于词向量中\n",
      "luzhins不存在于词向量中\n",
      "leonards不存在于词向量中\n",
      "steiners不存在于词向量中\n",
      "greenaways不存在于词向量中\n",
      "abcs不存在于词向量中\n",
      "poseys不存在于词向量中\n",
      "brontës不存在于词向量中\n",
      "hoopers不存在于词向量中\n",
      "comedys不存在于词向量中\n",
      "batmans不存在于词向量中\n",
      "susans不存在于词向量中\n",
      "ritchies不存在于词向量中\n",
      "methis不存在于词向量中\n",
      "lilas不存在于词向量中\n",
      "lemmons不存在于词向量中\n",
      "thingi不存在于词向量中\n",
      "disappointedthe不存在于词向量中\n",
      "showi不存在于词向量中\n",
      "peoplethe不存在于词向量中\n",
      "alans不存在于词向量中\n",
      "versionthe不存在于词向量中\n",
      "pointthe不存在于词向量中\n",
      "roegs不存在于词向量中\n",
      "tarzans不存在于词向量中\n",
      "carells不存在于词向量中\n",
      "gandhis不存在于词向量中\n",
      "therei不存在于词向量中\n",
      "sebergs不存在于词向量中\n",
      "kiplings不存在于词向量中\n",
      "experiencethe不存在于词向量中\n",
      "tracys不存在于词向量中\n",
      "unfilmable不存在于词向量中\n",
      "snitchd不存在于词向量中\n",
      "fishers不存在于词向量中\n",
      "2the不存在于词向量中\n",
      "boyles不存在于词向量中\n",
      "grendels不存在于词向量中\n",
      "filmsthis不存在于词向量中\n",
      "itso不存在于词向量中\n",
      "astaires不存在于词向量中\n",
      "humanitys不存在于词向量中\n",
      "odonnell不存在于词向量中\n",
      "modestys不存在于词向量中\n",
      "corsaut不存在于词向量中\n",
      "niros不存在于词向量中\n",
      "hilariousthe不存在于词向量中\n",
      "cushings不存在于词向量中\n",
      "muchi不存在于词向量中\n",
      "eitherthe不存在于词向量中\n",
      "harlows不存在于词向量中\n",
      "familyi不存在于词向量中\n",
      "onthis不存在于词向量中\n",
      "reasonthe不存在于词向量中\n",
      "eitheri不存在于词向量中\n",
      "stroker不存在于词向量中\n",
      "togetherthe不存在于词向量中\n",
      "boringi不存在于词向量中\n",
      "gables不存在于词向量中\n",
      "iberia不存在于词向量中\n",
      "josephs不存在于词向量中\n",
      "lewiss不存在于词向量中\n",
      "decameron不存在于词向量中\n",
      "oconor不存在于词向量中\n",
      "mamets不存在于词向量中\n",
      "lintrus不存在于词向量中\n",
      "celestes不存在于词向量中\n",
      "oconnell不存在于词向量中\n",
      "seriesi不存在于词向量中\n",
      "moviedont不存在于词向量中\n",
      "moviewell不存在于词向量中\n",
      "beforethe不存在于词向量中\n",
      "jamess不存在于词向量中\n",
      "antwones不存在于词向量中\n",
      "farrells不存在于词向量中\n",
      "crapi不存在于词向量中\n",
      "itselfi不存在于词向量中\n",
      "pixars不存在于词向量中\n",
      "rolesthe不存在于词向量中\n",
      "tpol不存在于词向量中\n",
      "codys不存在于词向量中\n",
      "himand不存在于词向量中\n",
      "ryans不存在于词向量中\n",
      "worldi不存在于词向量中\n",
      "herebut不存在于词向量中\n",
      "leones不存在于词向量中\n",
      "cagneys不存在于词向量中\n",
      "wallaces不存在于词向量中\n",
      "yetis不存在于词向量中\n",
      "partthe不存在于词向量中\n",
      "fassbinders不存在于词向量中\n",
      "scriptthe不存在于词向量中\n",
      "kims不存在于词向量中\n",
      "movieone不存在于词向量中\n",
      "bournes不存在于词向量中\n",
      "kristoffersons不存在于词向量中\n",
      "rooneys不存在于词向量中\n",
      "hudsons不存在于词向量中\n",
      "thatthis不存在于词向量中\n",
      "oneand不存在于词向量中\n",
      "placethe不存在于词向量中\n",
      "casethe不存在于词向量中\n",
      "churchs不存在于词向量中\n",
      "hydes不存在于词向量中\n",
      "foxs不存在于词向量中\n",
      "realthe不存在于词向量中\n",
      "pasolinis不存在于词向量中\n",
      "daysthe不存在于词向量中\n",
      "elliotts不存在于词向量中\n",
      "himthis不存在于词向量中\n",
      "oneil不存在于词向量中\n",
      "ursulas不存在于词向量中\n",
      "doone不存在于词向量中\n",
      "esthers不存在于词向量中\n",
      "filmas不存在于词向量中\n",
      "$1000000不存在于词向量中\n",
      "thingsthe不存在于词向量中\n",
      "ummm不存在于词向量中\n",
      "withthe不存在于词向量中\n",
      "ruths不存在于词向量中\n",
      "shin-aes不存在于词向量中\n",
      "greenes不存在于词向量中\n",
      "warthe不存在于词向量中\n",
      "porters不存在于词向量中\n",
      "leighs不存在于词向量中\n",
      "timesbut不存在于词向量中\n",
      "interestingthe不存在于词向量中\n",
      "sweetin不存在于词向量中\n",
      "worsethe不存在于词向量中\n",
      "nelsons不存在于词向量中\n",
      "wellbut不存在于词向量中\n",
      "dahmers不存在于词向量中\n",
      "columbos不存在于词向量中\n",
      "onethis不存在于词向量中\n",
      "foxxs不存在于词向量中\n",
      "bacons不存在于词向量中\n",
      "segals不存在于词向量中\n",
      "schumachers不存在于词向量中\n",
      "wests不存在于词向量中\n",
      "colmans不存在于词向量中\n",
      "timeand不存在于词向量中\n",
      "cassidys不存在于词向量中\n",
      "robertsons不存在于词向量中\n",
      "cb4不存在于词向量中\n",
      "isthis不存在于词向量中\n",
      "arnolds不存在于词向量中\n",
      "sauras不存在于词向量中\n",
      "todds不存在于词向量中\n",
      "alberts不存在于词向量中\n",
      "filmwhat不存在于词向量中\n",
      "kidmans不存在于词向量中\n",
      "lovethe不存在于词向量中\n",
      "billys不存在于词向量中\n",
      "toi不存在于词向量中\n",
      "himi不存在于词向量中\n",
      "goodthis不存在于词向量中\n",
      "badit不存在于词向量中\n",
      "daddys不存在于词向量中\n",
      "recap:不存在于词向量中\n",
      "funthe不存在于词向量中\n",
      "rocknroll不存在于词向量中\n",
      "fatherthe不存在于词向量中\n",
      "karens不存在于词向量中\n",
      "10000不存在于词向量中\n",
      "sagemiller不存在于词向量中\n",
      "knowthe不存在于词向量中\n",
      "nwh不存在于词向量中\n",
      "rev不存在于词向量中\n",
      "joans不存在于词向量中\n",
      "effectsthe不存在于词向量中\n",
      "bronsons不存在于词向量中\n",
      "tothe不存在于词向量中\n",
      "vice-versa不存在于词向量中\n",
      "cheadles不存在于词向量中\n",
      "terriblethe不存在于词向量中\n",
      "doyles不存在于词向量中\n",
      "moviesbut不存在于词向量中\n",
      "itim不存在于词向量中\n",
      "addy不存在于词向量中\n",
      "umm不存在于词向量中\n",
      "bestthe不存在于词向量中\n",
      "crapthe不存在于词向量中\n",
      "sabrinas不存在于词向量中\n",
      "badbut不存在于词向量中\n",
      "francos不存在于词向量中\n",
      "demilles不存在于词向量中\n",
      "booki不存在于词向量中\n",
      "greendale不存在于词向量中\n",
      "vasey不存在于词向量中\n",
      "historythe不存在于词向量中\n",
      "marias不存在于词向量中\n",
      "spoilersi不存在于词向量中\n",
      "granddaughters不存在于词向量中\n",
      "timesi不存在于词向量中\n",
      "horrorcomedy不存在于词向量中\n",
      "outi不存在于词向量中\n",
      "daviss不存在于词向量中\n",
      "yentl不存在于词向量中\n",
      "filmone不存在于词向量中\n",
      "moviemy不存在于词向量中\n",
      "dor不存在于词向量中\n",
      "comedythe不存在于词向量中\n",
      "caligari不存在于词向量中\n",
      "presque不存在于词向量中\n",
      "offi不存在于词向量中\n",
      "fatherson不存在于词向量中\n",
      "experiencei不存在于词向量中\n",
      "realitythe不存在于词向量中\n",
      "latters不存在于词向量中\n",
      "allit不存在于词向量中\n",
      "jaffa不存在于词向量中\n",
      "yikes不存在于词向量中\n",
      "indias不存在于词向量中\n",
      "screeni不存在于词向量中\n",
      "goyokin不存在于词向量中\n",
      "roths不存在于词向量中\n",
      "brians不存在于词向量中\n",
      "anothers不存在于词向量中\n",
      "nablus不存在于词向量中\n",
      "euripides不存在于词向量中\n",
      "bosss不存在于词向量中\n",
      "lifeit不存在于词向量中\n",
      "againit不存在于词向量中\n",
      "garys不存在于词向量中\n",
      "actorsthe不存在于词向量中\n",
      "moi不存在于词向量中\n",
      "wasthe不存在于词向量中\n",
      "eyesthe不存在于词向量中\n",
      "bunuels不存在于词向量中\n",
      "menstruation不存在于词向量中\n",
      "2036不存在于词向量中\n",
      "draculas不存在于词向量中\n",
      "dogville不存在于词向量中\n",
      "maetel不存在于词向量中\n",
      "coulardeau不存在于词向量中\n",
      "ritas不存在于词向量中\n",
      "peepers不存在于词向量中\n",
      "lucys不存在于词向量中\n",
      "ohearn不存在于词向量中\n",
      "ksun不存在于词向量中\n",
      "oleary不存在于词向量中\n",
      "louiss不存在于词向量中\n",
      "chens不存在于词向量中\n",
      "rafiki不存在于词向量中\n",
      "themthere不存在于词向量中\n",
      "boyers不存在于词向量中\n",
      "anywayi不存在于词向量中\n",
      "itall不存在于词向量中\n",
      "oconnors不存在于词向量中\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagman不存在于词向量中\n",
      "sm不存在于词向量中\n",
      "bloss不存在于词向量中\n",
      "timea不存在于词向量中\n",
      "blithe不存在于词向量中\n",
      "allendes不存在于词向量中\n",
      "worki不存在于词向量中\n",
      "chers不存在于词向量中\n",
      "upi不存在于词向量中\n",
      "himher不存在于词向量中\n",
      "bei不存在于词向量中\n",
      "baldwins不存在于词向量中\n",
      "mayors不存在于词向量中\n",
      "pythons不存在于词向量中\n",
      "walshs不存在于词向量中\n",
      "lius不存在于词向量中\n",
      "spocks不存在于词向量中\n",
      "knowi不存在于词向量中\n",
      "youit不存在于词向量中\n",
      "frankensteins不存在于词向量中\n",
      "reids不存在于词向量中\n",
      "throughi不存在于词向量中\n",
      "okay!不存在于词向量中\n",
      "wasis不存在于词向量中\n",
      "happenthe不存在于词向量中\n",
      "facethe不存在于词向量中\n",
      "pickfords不存在于词向量中\n",
      "barrymores不存在于词向量中\n",
      "yaara不存在于词向量中\n",
      "walkin不存在于词向量中\n",
      "julians不存在于词向量中\n",
      "aroundthe不存在于词向量中\n",
      "$100000不存在于词向量中\n",
      "differentthe不存在于词向量中\n",
      "orwells不存在于词向量中\n",
      "itfor不存在于词向量中\n",
      "mankinds不存在于词向量中\n",
      "crosbys不存在于词向量中\n",
      "starsthe不存在于词向量中\n",
      "gillians不存在于词向量中\n",
      "labours不存在于词向量中\n",
      "videodvd不存在于词向量中\n",
      "timeits不存在于词向量中\n",
      "alienator不存在于词向量中\n",
      "timeif不存在于词向量中\n",
      "toothis不存在于词向量中\n",
      "tierneys不存在于词向量中\n",
      "sants不存在于词向量中\n",
      "graysons不存在于词向量中\n",
      "redgraves不存在于词向量中\n",
      "tronje不存在于词向量中\n",
      "somethingi不存在于词向量中\n",
      "movieall不存在于词向量中\n",
      "prag不存在于词向量中\n",
      "caesars不存在于词向量中\n",
      "moviedo不存在于词向量中\n",
      "therere不存在于词向量中\n",
      "christies不存在于词向量中\n",
      "thingthe不存在于词向量中\n",
      "matata不存在于词向量中\n",
      "voights不存在于词向量中\n",
      "brendans不存在于词向量中\n",
      "grays不存在于词向量中\n",
      "hamlets不存在于词向量中\n",
      "deaththe不存在于词向量中\n",
      "gentileschi不存在于词向量中\n",
      "montanas不存在于词向量中\n",
      "outthis不存在于词向量中\n",
      "genrethe不存在于词向量中\n",
      "sadthe不存在于词向量中\n",
      "donethe不存在于词向量中\n",
      "dahls不存在于词向量中\n",
      "edisons不存在于词向量中\n",
      "harveys不存在于词向量中\n",
      "offthe不存在于词向量中\n",
      "hima不存在于词向量中\n",
      "cmon!不存在于词向量中\n",
      "castthe不存在于词向量中\n",
      "scarlets不存在于词向量中\n",
      "robins不存在于词向量中\n",
      "malfatti不存在于词向量中\n",
      "chibas不存在于词向量中\n",
      "brotherthe不存在于词向量中\n",
      "actioncomedy不存在于词向量中\n",
      "mclaglens不存在于词向量中\n",
      "lubitschs不存在于词向量中\n",
      "coles不存在于词向量中\n",
      "noltes不存在于词向量中\n",
      "momentsthe不存在于词向量中\n",
      "shahs不存在于词向量中\n",
      "backthe不存在于词向量中\n",
      "atlantian不存在于词向量中\n",
      "gromit不存在于词向量中\n",
      "natalis不存在于词向量中\n",
      "shantytown不存在于词向量中\n",
      "armstrongs不存在于词向量中\n",
      "banjos不存在于词向量中\n",
      "peggs不存在于词向量中\n",
      "seans不存在于词向量中\n",
      "clays不存在于词向量中\n",
      "grinchs不存在于词向量中\n",
      "tiffanys不存在于词向量中\n",
      "pandoras不存在于词向量中\n",
      "chinas不存在于词向量中\n",
      "simbas不存在于词向量中\n",
      "canadas不存在于词向量中\n",
      "fays不存在于词向量中\n",
      "inthe不存在于词向量中\n",
      "allits不存在于词向量中\n",
      "999%不存在于词向量中\n",
      "paltrows不存在于词向量中\n",
      "thatbut不存在于词向量中\n",
      "christines不存在于词向量中\n",
      "muertos不存在于词向量中\n",
      "cannavale不存在于词向量中\n",
      "jindabyne不存在于词向量中\n",
      "blakes不存在于词向量中\n",
      "madsens不存在于词向量中\n",
      "soi不存在于词向量中\n",
      "meit不存在于词向量中\n",
      "itselfthe不存在于词向量中\n",
      "depalmas不存在于词向量中\n",
      "mcqueens不存在于词向量中\n",
      "vidals不存在于词向量中\n",
      "untergang不存在于词向量中\n",
      "00s不存在于词向量中\n",
      "phlox不存在于词向量中\n",
      "shotthe不存在于词向量中\n",
      "timeit不存在于词向量中\n",
      "bobbys不存在于词向量中\n",
      "specialthe不存在于词向量中\n",
      "klines不存在于词向量中\n",
      "thereand不存在于词向量中\n",
      "reas不存在于词向量中\n",
      "milvertons不存在于词向量中\n",
      "grahams不存在于词向量中\n",
      "scenesi不存在于词向量中\n",
      "overi不存在于词向量中\n",
      "oooh不存在于词向量中\n",
      "peckers不存在于词向量中\n",
      "therethis不存在于词向量中\n",
      "50000不存在于词向量中\n",
      "hitler:不存在于词向量中\n",
      "stanleys不存在于词向量中\n",
      "gregorys不存在于词向量中\n",
      "themin不存在于词向量中\n",
      "nortons不存在于词向量中\n",
      "185:1不存在于词向量中\n",
      "originalthe不存在于词向量中\n",
      "abouti不存在于词向量中\n",
      "wodehouses不存在于词向量中\n",
      "redline不存在于词向量中\n",
      "lawton不存在于词向量中\n",
      "mysterythriller不存在于词向量中\n",
      "luque不存在于词向量中\n",
      "anyonethe不存在于词向量中\n",
      "kongs不存在于词向量中\n",
      "drivas不存在于词向量中\n",
      "mindthe不存在于词向量中\n",
      "lanzas不存在于词向量中\n",
      "jellybean不存在于词向量中\n",
      "weirs不存在于词向量中\n",
      "rs不存在于词向量中\n",
      "zanes不存在于词向量中\n",
      "genma不存在于词向量中\n",
      "etcand不存在于词向量中\n",
      "actorsi不存在于词向量中\n",
      "itwe不存在于词向量中\n",
      "solomons不存在于词向量中\n",
      "20000不存在于词向量中\n",
      "clooneys不存在于词向量中\n",
      "soso不存在于词向量中\n",
      "longthe不存在于词向量中\n",
      "genies不存在于词向量中\n",
      "schwarzeneggers不存在于词向量中\n",
      "goodand不存在于词向量中\n",
      "niki不存在于词向量中\n",
      "bouzaglo不存在于词向量中\n",
      "prizzis不存在于词向量中\n",
      "characterand不存在于词向量中\n",
      "$399不存在于词向量中\n",
      "throughthe不存在于词向量中\n",
      "arrondissement不存在于词向量中\n",
      "dosent不存在于词向量中\n",
      "donei不存在于词向量中\n",
      "kitchens不存在于词向量中\n",
      "liottas不存在于词向量中\n",
      "wellif不存在于词向量中\n",
      "charactersi不存在于词向量中\n",
      "wasthis不存在于词向量中\n",
      "kirks不存在于词向量中\n",
      "drakes不存在于词向量中\n",
      "everthe不存在于词向量中\n",
      "offthis不存在于词向量中\n",
      "raimis不存在于词向量中\n",
      "himselfthe不存在于词向量中\n",
      "lorens不存在于词向量中\n",
      "allthis不存在于词向量中\n",
      "rhys-meyers不存在于词向量中\n",
      "notthis不存在于词向量中\n",
      "noras不存在于词向量中\n",
      "lengle不存在于词向量中\n",
      "paynes不存在于词向量中\n",
      "macleans不存在于词向量中\n",
      "kopins不存在于词向量中\n",
      "gespenster不存在于词向量中\n",
      "mamas不存在于词向量中\n",
      "trex不存在于词向量中\n",
      "jesss不存在于词向量中\n",
      "perú不存在于词向量中\n",
      "thered不存在于词向量中\n",
      "studi不存在于词向量中\n",
      "geres不存在于词向量中\n",
      "mackendrick不存在于词向量中\n",
      "paines不存在于词向量中\n",
      "ninetitle不存在于词向量中\n",
      "rg不存在于词向量中\n",
      "bolkan不存在于词向量中\n",
      "themas不存在于词向量中\n",
      "releasedthe不存在于词向量中\n",
      "storylinethe不存在于词向量中\n",
      "houseboat不存在于词向量中\n",
      "conaway不存在于词向量中\n",
      "larrys不存在于词向量中\n",
      "thisits不存在于词向量中\n",
      "hoppers不存在于词向量中\n",
      "musici不存在于词向量中\n",
      "announcers不存在于词向量中\n",
      "grot不存在于词向量中\n",
      "mt不存在于词向量中\n",
      "meso不存在于词向量中\n",
      "grasshopper不存在于词向量中\n",
      "bogdanovichs不存在于词向量中\n",
      "quaids不存在于词向量中\n",
      "fiancés不存在于词向量中\n",
      "timesthis不存在于词向量中\n",
      ":o不存在于词向量中\n",
      "againand不存在于词向量中\n",
      "tyrannus不存在于词向量中\n",
      "thatd不存在于词向量中\n",
      "threetitle不存在于词向量中\n",
      "zeffirellis不存在于词向量中\n",
      "ini不存在于词向量中\n",
      "movieeven不存在于词向量中\n",
      "horrorsci-fi不存在于词向量中\n",
      "heartthe不存在于词向量中\n",
      "masseys不存在于词向量中\n",
      "endingi不存在于词向量中\n",
      "basra不存在于词向量中\n",
      "kusturicas不存在于词向量中\n",
      "yearsthe不存在于词向量中\n",
      "thisbut不存在于词向量中\n",
      "themselvesthe不存在于词向量中\n",
      "rightthe不存在于词向量中\n",
      "ashura不存在于词向量中\n",
      "kitamuras不存在于词向量中\n",
      "historys不存在于词向量中\n",
      "spanglish不存在于词向量中\n",
      "himin不存在于词向量中\n",
      "simms不存在于词向量中\n",
      "afis不存在于词向量中\n",
      "scrooges不存在于词向量中\n",
      "becks不存在于词向量中\n",
      "lisas不存在于词向量中\n",
      "10i不存在于词向量中\n",
      "filmdont不存在于词向量中\n",
      "deniros不存在于词向量中\n",
      "musicthe不存在于词向量中\n",
      "cf不存在于词向量中\n",
      "periodthe不存在于词向量中\n",
      "funnyand不存在于词向量中\n",
      "thisit不存在于词向量中\n",
      "johnny!不存在于词向量中\n",
      "etci不存在于词向量中\n",
      "awfulthe不存在于词向量中\n",
      "afflecks不存在于词向量中\n",
      "matarazzo不存在于词向量中\n",
      "earththe不存在于词向量中\n",
      "uhm不存在于词向量中\n",
      "kornman不存在于词向量中\n",
      "familythis不存在于词向量中\n",
      "normans不存在于词向量中\n",
      "aka:不存在于词向量中\n",
      "predictablethe不存在于词向量中\n",
      "mols不存在于词向量中\n",
      "thatif不存在于词向量中\n",
      "clampetts不存在于词向量中\n",
      "sicas不存在于词向量中\n",
      "hannas不存在于词向量中\n",
      "creasys不存在于词向量中\n",
      "onethere不存在于词向量中\n",
      "besti不存在于词向量中\n",
      "helpers不存在于词向量中\n",
      "waythis不存在于词向量中\n",
      "goshas不存在于词向量中\n",
      "slovenia不存在于词向量中\n",
      "kundry不存在于词向量中\n",
      "fourtitle不存在于词向量中\n",
      "todos不存在于词向量中\n",
      "livesthe不存在于词向量中\n",
      "edies不存在于词向量中\n",
      "eds不存在于词向量中\n",
      "joness不存在于词向量中\n",
      "believablethe不存在于词向量中\n",
      "heinleins不存在于词向量中\n",
      "dannings不存在于词向量中\n",
      "seeing!不存在于词向量中\n",
      "awaythe不存在于词向量中\n",
      "$50000不存在于词向量中\n",
      "haenel不存在于词向量中\n",
      "ellens不存在于词向量中\n",
      "gordone不存在于词向量中\n",
      "itthey不存在于词向量中\n",
      "ridiculousthe不存在于词向量中\n",
      "minnellis不存在于词向量中\n",
      "togetheri不存在于词向量中\n",
      "daves不存在于词向量中\n",
      "mine!不存在于词向量中\n",
      "redfords不存在于词向量中\n",
      "moviefor不存在于词向量中\n",
      "247不存在于词向量中\n",
      "amitabhs不存在于词向量中\n",
      "flickthe不存在于词向量中\n",
      "anthonys不存在于词向量中\n",
      "otheri不存在于词向量中\n",
      "malones不存在于词向量中\n",
      "dystrophy不存在于词向量中\n",
      "allin不存在于词向量中\n",
      "elsei不存在于词向量中\n",
      "salaam-e-ishq不存在于词向量中\n",
      "sleepthe不存在于词向量中\n",
      "youif不存在于词向量中\n",
      "ex-mrs不存在于词向量中\n",
      "nextthe不存在于词向量中\n",
      "berkeleys不存在于词向量中\n",
      "exterminators不存在于词向量中\n",
      "chalo不存在于词向量中\n",
      "itwell不存在于词向量中\n",
      "giovannas不存在于词向量中\n",
      "laputa:不存在于词向量中\n",
      "lewtons不存在于词向量中\n",
      "detre不存在于词向量中\n",
      "chavezs不存在于词向量中\n",
      "neer-do-well不存在于词向量中\n",
      "thaws不存在于词向量中\n",
      "mcenroe不存在于词向量中\n",
      "tulipe不存在于词向量中\n",
      "hollys不存在于词向量中\n",
      "incomprehensibility不存在于词向量中\n",
      "abbys不存在于词向量中\n",
      "tur不存在于词向量中\n",
      "mcdiarmid不存在于词向量中\n",
      "himselfi不存在于词向量中\n",
      "235:1不存在于词向量中\n",
      "elizabeths不存在于词向量中\n",
      "checkpoints不存在于词向量中\n",
      "arethe不存在于词向量中\n",
      "sascha不存在于词向量中\n",
      "btas不存在于词向量中\n",
      "ravine不存在于词向量中\n",
      "moviesand不存在于词向量中\n",
      "directori不存在于词向量中\n",
      "haims不存在于词向量中\n",
      "sows不存在于词向量中\n",
      "sandras不存在于词向量中\n",
      "dandys不存在于词向量中\n",
      "moviefirst不存在于词向量中\n",
      "wonderfulthe不存在于词向量中\n",
      "kolchaks不存在于词向量中\n",
      "nightthe不存在于词向量中\n",
      "hungs不存在于词向量中\n",
      "timeas不存在于词向量中\n",
      "dayi不存在于词向量中\n",
      "nivens不存在于词向量中\n",
      "davidtz不存在于词向量中\n",
      "misha不存在于词向量中\n",
      "freds不存在于词向量中\n",
      "doi不存在于词向量中\n",
      "gores不存在于词向量中\n",
      "dibiases不存在于词向量中\n",
      "moneythe不存在于词向量中\n",
      "hadleys不存在于词向量中\n",
      "atthe不存在于词向量中\n",
      "moronie不存在于词向量中\n",
      "rouges不存在于词向量中\n",
      "meow不存在于词向量中\n",
      "nguyen不存在于词向量中\n",
      "wayit不存在于词向量中\n",
      "starlift不存在于词向量中\n",
      "caspers不存在于词向量中\n",
      "spartacus不存在于词向量中\n",
      "situationthe不存在于词向量中\n",
      "usthe不存在于词向量中\n",
      "colombo不存在于词向量中\n",
      "ownthe不存在于词向量中\n",
      "wus不存在于词向量中\n",
      "ow不存在于词向量中\n",
      "sammos不存在于词向量中\n",
      "alta不存在于词向量中\n",
      "sothe不存在于词向量中\n",
      "1895不存在于词向量中\n",
      "holmess不存在于词向量中\n",
      "paxtons不存在于词向量中\n",
      "pharaohs不存在于词向量中\n",
      "bladerunner不存在于词向量中\n",
      "ligabue不存在于词向量中\n",
      "trainor不存在于词向量中\n",
      "timebut不存在于词向量中\n",
      "ryker不存在于词向量中\n",
      "gades不存在于词向量中\n",
      "yearthe不存在于词向量中\n",
      "onemy不存在于词向量中\n",
      "claires不存在于词向量中\n",
      "thisand不存在于词向量中\n",
      "cantillana不存在于词向量中\n",
      "performancethe不存在于词向量中\n",
      "ustinovs不存在于词向量中\n",
      "characteri不存在于词向量中\n",
      "travail不存在于词向量中\n",
      "askeys不存在于词向量中\n",
      "stevensons不存在于词向量中\n",
      "excellentthe不存在于词向量中\n",
      "filmfirst不存在于词向量中\n",
      "chenoweth不存在于词向量中\n",
      "itnow不存在于词向量中\n",
      "tarkovskys不存在于词向量中\n",
      "masterpiecethe不存在于词向量中\n",
      "elsethe不存在于词向量中\n",
      "mccoys不存在于词向量中\n",
      "locataire不存在于词向量中\n",
      "iliad不存在于词向量中\n",
      "cruellas不存在于词向量中\n",
      "feinstones不存在于词向量中\n",
      "insteadthe不存在于词向量中\n",
      "directorco-writer不存在于词向量中\n",
      "06不存在于词向量中\n",
      "onebut不存在于词向量中\n",
      "reali不存在于词向量中\n",
      "ndour不存在于词向量中\n",
      "waltons不存在于词向量中\n",
      "minutesi不存在于词向量中\n",
      "linklaters不存在于词向量中\n",
      "goldblums不存在于词向量中\n",
      "filmsbut不存在于词向量中\n",
      "atlantis:不存在于词向量中\n",
      "pransky不存在于词向量中\n",
      "kikis不存在于词向量中\n",
      "homelessness不存在于词向量中\n",
      "filmnow不存在于词向量中\n",
      "hermans不存在于词向量中\n",
      "jouissance不存在于词向量中\n",
      "coursethe不存在于词向量中\n",
      "willys不存在于词向量中\n",
      "comedyi不存在于词向量中\n",
      "thatand不存在于词向量中\n",
      "overthe不存在于词向量中\n",
      "comstock不存在于词向量中\n",
      "colonels不存在于词向量中\n",
      "sheridans不存在于词向量中\n",
      "thrillerhorror不存在于词向量中\n",
      "pascow不存在于词向量中\n",
      "peoplei不存在于词向量中\n",
      "dir不存在于词向量中\n",
      "springers不存在于词向量中\n",
      "coolio不存在于词向量中\n",
      "lateri不存在于词向量中\n",
      "fitzgeralds不存在于词向量中\n",
      "myrtles不存在于词向量中\n",
      "barbaras不存在于词向量中\n",
      "b:tas不存在于词向量中\n",
      "2s不存在于词向量中\n",
      "denzels不存在于词向量中\n",
      "mole-men不存在于词向量中\n",
      "shant不存在于词向量中\n",
      "homicide:不存在于词向量中\n",
      "newbern不存在于词向量中\n",
      "boormans不存在于词向量中\n",
      "elmos不存在于词向量中\n",
      "kunderas不存在于词向量中\n",
      "mums不存在于词向量中\n",
      "syberbergs不存在于词向量中\n",
      "wastrel不存在于词向量中\n",
      "chongs不存在于词向量中\n",
      "garber不存在于词向量中\n",
      "sorter不存在于词向量中\n",
      "watchand不存在于词向量中\n",
      "dabo不存在于词向量中\n",
      "movienow不存在于词向量中\n",
      "endingthe不存在于词向量中\n",
      "cunninghams不存在于词向量中\n",
      "contra不存在于词向量中\n",
      "germanys不存在于词向量中\n",
      "coronets不存在于词向量中\n",
      "ouch不存在于词向量中\n",
      "madei不存在于词向量中\n",
      "mildreds不存在于词向量中\n",
      "potters不存在于词向量中\n",
      "tx不存在于词向量中\n",
      "sabretooths不存在于词向量中\n",
      "motherthe不存在于词向量中\n",
      "lenas不存在于词向量中\n",
      "whythe不存在于词向量中\n",
      "bradburys不存在于词向量中\n",
      "greati不存在于词向量中\n",
      "themso不存在于词向量中\n",
      "jobthe不存在于词向量中\n",
      "outit不存在于词向量中\n",
      "crimethe不存在于词向量中\n",
      "herethis不存在于词向量中\n",
      "pyewacket不存在于词向量中\n",
      "kordas不存在于词向量中\n",
      "marolla不存在于词向量中\n",
      "airlift不存在于词向量中\n",
      "hilariousi不存在于词向量中\n",
      "shinobi不存在于词向量中\n",
      "approx不存在于词向量中\n",
      "hightower不存在于词向量中\n",
      "kabei不存在于词向量中\n",
      "ieks不存在于词向量中\n",
      "coxs不存在于词向量中\n",
      "pepi不存在于词向量中\n",
      "herren不存在于词向量中\n",
      "claras不存在于词向量中\n",
      "zhu不存在于词向量中\n",
      "wymer不存在于词向量中\n",
      "costners不存在于词向量中\n",
      "kriemhilds不存在于词向量中\n",
      "evas不存在于词向量中\n",
      "beban不存在于词向量中\n",
      "laterthe不存在于词向量中\n",
      "carons不存在于词向量中\n",
      "rostov不存在于词向量中\n",
      "alvins不存在于词向量中\n",
      "solvang不存在于词向量中\n",
      "mid-60s不存在于词向量中\n",
      "haywards不存在于词向量中\n",
      "05不存在于词向量中\n",
      "mathieus不存在于词向量中\n",
      "mustnt不存在于词向量中\n",
      "todaythe不存在于词向量中\n",
      "themif不存在于词向量中\n",
      "clarkes不存在于词向量中\n",
      "womenthe不存在于词向量中\n",
      "wayand不存在于词向量中\n",
      "poppa不存在于词向量中\n",
      "hartnetts不存在于词向量中\n",
      "shos不存在于词向量中\n",
      "finneys不存在于词向量中\n",
      "**12不存在于词向量中\n",
      "genova不存在于词向量中\n",
      "bavas不存在于词向量中\n",
      "hazlehurst不存在于词向量中\n",
      "kapoors不存在于词向量中\n",
      "sweid不存在于词向量中\n",
      "harriss不存在于词向量中\n",
      "mabuse不存在于词向量中\n",
      "marleen不存在于词向量中\n",
      "paulas不存在于词向量中\n",
      "nuyoricans不存在于词向量中\n",
      "himbut不存在于词向量中\n",
      "spoorloos不存在于词向量中\n",
      "stettner不存在于词向量中\n",
      "wardh不存在于词向量中\n",
      "dooleys不存在于词向量中\n",
      "movieas不存在于词向量中\n",
      "appolonia不存在于词向量中\n",
      "faubourg不存在于词向量中\n",
      "fly-fishing不存在于词向量中\n",
      "beckinsales不存在于词向量中\n",
      "ninotchka不存在于词向量中\n",
      "downthe不存在于词向量中\n",
      "ghandi不存在于词向量中\n",
      "alexanders不存在于词向量中\n",
      "mockingbird不存在于词向量中\n",
      "singletons不存在于词向量中\n",
      "entertainingthe不存在于词向量中\n",
      "iteven不存在于词向量中\n",
      "classicthe不存在于词向量中\n",
      "$250000不存在于词向量中\n",
      "bakes不存在于词向量中\n",
      "unicorn不存在于词向量中\n",
      "onand不存在于词向量中\n",
      "corleones不存在于词向量中\n",
      "youthis不存在于词向量中\n",
      "goodbad不存在于词向量中\n",
      "numar不存在于词向量中\n",
      "chomskys不存在于词向量中\n",
      "olan不存在于词向量中\n",
      "annoyingthe不存在于词向量中\n",
      "hinduism不存在于词向量中\n",
      "ha-ha不存在于词向量中\n",
      "lady!不存在于词向量中\n",
      "oneills不存在于词向量中\n",
      "holcomb不存在于词向量中\n",
      "yall不存在于词向量中\n",
      "filmoverall不存在于词向量中\n",
      "youin不存在于词向量中\n",
      "denard不存在于词向量中\n",
      "jasons不存在于词向量中\n",
      "horrorgore不存在于词向量中\n",
      "allbut不存在于词向量中\n",
      "silverware不存在于词向量中\n",
      "bennetts不存在于词向量中\n",
      "volckmans不存在于词向量中\n",
      "peopleand不存在于词向量中\n",
      "ahista不存在于词向量中\n",
      "1the不存在于词向量中\n",
      "toshi不存在于词向量中\n",
      "peoplethis不存在于词向量中\n",
      "blairs不存在于词向量中\n",
      "dilly不存在于词向量中\n",
      "vadis不存在于词向量中\n",
      "garbagethe不存在于词向量中\n",
      "editingthe不存在于词向量中\n",
      "boringthe不存在于词向量中\n",
      "itvery不存在于词向量中\n",
      "altaira不存在于词向量中\n",
      "workit不存在于词向量中\n",
      "zavet不存在于词向量中\n",
      "stupidthe不存在于词向量中\n",
      "dietrichs不存在于词向量中\n",
      "zomcon不存在于词向量中\n",
      "historyi不存在于词向量中\n",
      "roedel不存在于词向量中\n",
      "sub-zero不存在于词向量中\n",
      "anew不存在于词向量中\n",
      "timesin不存在于词向量中\n",
      "wellingtons不存在于词向量中\n",
      "levants不存在于词向量中\n",
      "changs不存在于词向量中\n",
      "kusama不存在于词向量中\n",
      "badthis不存在于词向量中\n",
      "isits不存在于词向量中\n",
      "jennas不存在于词向量中\n",
      "sheedys不存在于词向量中\n",
      "effectsi不存在于词向量中\n",
      "lexs不存在于词向量中\n",
      "fulbright不存在于词向量中\n",
      "apposite不存在于词向量中\n",
      "chahines不存在于词向量中\n",
      "barsi不存在于词向量中\n",
      "stolz不存在于词向量中\n",
      "salems不存在于词向量中\n",
      "exhibitions不存在于词向量中\n",
      "greystoke不存在于词向量中\n",
      "actingand不存在于词向量中\n",
      "sallys不存在于词向量中\n",
      "funnyit不存在于词向量中\n",
      "tchaikovskys不存在于词向量中\n",
      "moes不存在于词向量中\n",
      "237不存在于词向量中\n",
      "shits不存在于词向量中\n",
      "chandon不存在于词向量中\n",
      "edmunds不存在于词向量中\n",
      "cybersix不存在于词向量中\n",
      "wrongthis不存在于词向量中\n",
      "buttgereits不存在于词向量中\n",
      "thema不存在于词向量中\n",
      "levinsons不存在于词向量中\n",
      "andrés不存在于词向量中\n",
      "horriblethe不存在于词向量中\n",
      "miryang不存在于词向量中\n",
      "understandthe不存在于词向量中\n",
      "storiesthe不存在于词向量中\n",
      "ginos不存在于词向量中\n",
      "ringside不存在于词向量中\n",
      "thoughthis不存在于词向量中\n",
      "watsons不存在于词向量中\n",
      "tans不存在于词向量中\n",
      "gurnemanz不存在于词向量中\n",
      "marshalls不存在于词向量中\n",
      "actorthe不存在于词向量中\n",
      "waterworld不存在于词向量中\n",
      "wolfs不存在于词向量中\n",
      "korsmo不存在于词向量中\n",
      "pseudoscience不存在于词向量中\n",
      "italys不存在于词向量中\n",
      "filmsit不存在于词向量中\n",
      "sensethe不存在于词向量中\n",
      "dassins不存在于词向量中\n",
      "wellits不存在于词向量中\n",
      "chestnuts不存在于词向量中\n",
      "martindale不存在于词向量中\n",
      "pallbearer不存在于词向量中\n",
      "actionthe不存在于词向量中\n",
      "filmall不存在于词向量中\n",
      "see!!!不存在于词向量中\n",
      "naught不存在于词向量中\n",
      "thembut不存在于词向量中\n",
      "deads不存在于词向量中\n",
      "moviewhat不存在于词向量中\n",
      "redrum不存在于词向量中\n",
      "slaters不存在于词向量中\n",
      "belaney不存在于词向量中\n",
      "artisans不存在于词向量中\n",
      "takechi不存在于词向量中\n",
      "castorini不存在于词向量中\n",
      "solent不存在于词向量中\n",
      "gft不存在于词向量中\n",
      "gabel不存在于词向量中\n",
      "subor不存在于词向量中\n",
      "zombification不存在于词向量中\n",
      "choule不存在于词向量中\n",
      "d-war不存在于词向量中\n",
      ":=8p不存在于词向量中\n",
      "hornbys不存在于词向量中\n",
      "bromell不存在于词向量中\n",
      "warthog不存在于词向量中\n",
      "morei不存在于词向量中\n",
      "cristinas不存在于词向量中\n",
      "goodin不存在于词向量中\n",
      "finally!不存在于词向量中\n",
      "stroh不存在于词向量中\n",
      "thismovie不存在于词向量中\n",
      "lous不存在于词向量中\n",
      "debenning不存在于词向量中\n",
      "knef不存在于词向量中\n",
      "tremayne不存在于词向量中\n",
      "tora不存在于词向量中\n",
      "nowthe不存在于词向量中\n",
      "itthats不存在于词向量中\n",
      "uh-huh不存在于词向量中\n",
      "kaun不存在于词向量中\n",
      "burts不存在于词向量中\n",
      "milligans不存在于词向量中\n",
      "directorthe不存在于词向量中\n",
      "wongs不存在于词向量中\n",
      "burkina不存在于词向量中\n",
      "tybalt不存在于词向量中\n",
      "larn不存在于词向量中\n",
      "mastersons不存在于词向量中\n",
      "detat不存在于词向量中\n",
      "lommels不存在于词向量中\n",
      "bachs不存在于词向量中\n",
      "itmy不存在于词向量中\n",
      "celies不存在于词向量中\n",
      "70s80s不存在于词向量中\n",
      "remoteness不存在于词向量中\n",
      "jariwala不存在于词向量中\n",
      "shatners不存在于词向量中\n",
      "$25000不存在于词向量中\n",
      "didnt!不存在于词向量中\n",
      "downi不存在于词向量中\n",
      "urbaniak不存在于词向量中\n",
      "upbut不存在于词向量中\n",
      "endit不存在于词向量中\n",
      "aboutand不存在于词向量中\n",
      "julias不存在于词向量中\n",
      "poorthe不存在于词向量中\n",
      "rookies不存在于词向量中\n",
      "themall不存在于词向量中\n",
      "sincethe不存在于词向量中\n",
      "nr不存在于词向量中\n",
      "sabus不存在于词向量中\n",
      "waysthe不存在于词向量中\n",
      "boz不存在于词向量中\n",
      "nowthis不存在于词向量中\n",
      "lovin不存在于词向量中\n",
      "shemps不存在于词向量中\n",
      "mencias不存在于词向量中\n",
      "mistakei不存在于词向量中\n",
      "tets不存在于词向量中\n",
      "coppolas不存在于词向量中\n",
      "danas不存在于词向量中\n",
      "brashears不存在于词向量中\n",
      "itat不存在于词向量中\n",
      "bridgets不存在于词向量中\n",
      "mcandrew不存在于词向量中\n",
      "machi不存在于词向量中\n",
      "pons不存在于词向量中\n",
      "salmans不存在于词向量中\n",
      "entertainmentthe不存在于词向量中\n",
      "itselfthis不存在于词向量中\n",
      "melvilles不存在于词向量中\n",
      "hereits不存在于词向量中\n",
      "trents不存在于词向量中\n",
      "deadthe不存在于词向量中\n",
      "pyuns不存在于词向量中\n",
      "farscape不存在于词向量中\n",
      "aphrodite不存在于词向量中\n",
      "rangi不存在于词向量中\n",
      "fuquas不存在于词向量中\n",
      "shirleys不存在于词向量中\n",
      "cathryn不存在于词向量中\n",
      "itive不存在于词向量中\n",
      "almodovars不存在于词向量中\n",
      "therell不存在于词向量中\n",
      "alls不存在于词向量中\n",
      "uks不存在于词向量中\n",
      "jennys不存在于词向量中\n",
      "film1不存在于词向量中\n",
      "narsimha不存在于词向量中\n",
      "yvelines不存在于词向量中\n",
      "barjatyas不存在于词向量中\n",
      "heyerdahl不存在于词向量中\n",
      "alland不存在于词向量中\n",
      "no2不存在于词向量中\n",
      "storyone不存在于词向量中\n",
      "cameramans不存在于词向量中\n",
      "panaghoy不存在于词向量中\n",
      "arkins不存在于词向量中\n",
      "brideless不存在于词向量中\n",
      "laughthe不存在于词向量中\n",
      "bedi不存在于词向量中\n",
      "schneiders不存在于词向量中\n",
      "unfaithfulness不存在于词向量中\n",
      "fawcetts不存在于词向量中\n",
      "careerthe不存在于词向量中\n",
      "necroborgs不存在于词向量中\n",
      "trautman不存在于词向量中\n",
      "dvdi不存在于词向量中\n",
      "wurb不存在于词向量中\n",
      "versioni不存在于词向量中\n",
      "ariels不存在于词向量中\n",
      "shepards不存在于词向量中\n",
      "dykes不存在于词向量中\n",
      "lloyds不存在于词向量中\n",
      "barmans不存在于词向量中\n",
      "kramers不存在于词向量中\n",
      "ismy不存在于词向量中\n",
      "himmelen不存在于词向量中\n",
      "verger不存在于词向量中\n",
      "paedophilia不存在于词向量中\n",
      "mexicos不存在于词向量中\n",
      "worldin不存在于词向量中\n",
      "movieon不存在于词向量中\n",
      "anistons不存在于词向量中\n",
      "vics不存在于词向量中\n",
      "imamuras不存在于词向量中\n",
      "namethe不存在于词向量中\n",
      "yousef不存在于词向量中\n",
      "laughsthe不存在于词向量中\n",
      "laroche不存在于词向量中\n",
      "agothe不存在于词向量中\n",
      "itlike不存在于词向量中\n",
      "tiempo不存在于词向量中\n",
      "monarchs不存在于词向量中\n",
      "lohans不存在于词向量中\n",
      "themand不存在于词向量中\n",
      "moviesif不存在于词向量中\n",
      "courtland不存在于词向量中\n",
      "cimarron不存在于词向量中\n",
      "shiner不存在于词向量中\n",
      "30000不存在于词向量中\n",
      "momentsi不存在于词向量中\n",
      "frownland不存在于词向量中\n",
      "businessthe不存在于词向量中\n",
      "palances不存在于词向量中\n",
      "two-parter不存在于词向量中\n",
      "frwl不存在于词向量中\n",
      "lovethis不存在于词向量中\n",
      "funnythis不存在于词向量中\n",
      "interestingthis不存在于词向量中\n",
      "youand不存在于词向量中\n",
      "cloudkicker不存在于词向量中\n",
      "mirage不存在于词向量中\n",
      "thurmans不存在于词向量中\n",
      "ratsos不存在于词向量中\n",
      "cronenbergs不存在于词向量中\n",
      "diplomacy不存在于词向量中\n",
      "beerys不存在于词向量中\n",
      "publications不存在于词向量中\n",
      "schwartzmans不存在于词向量中\n",
      "lacan不存在于词向量中\n",
      "scenethe不存在于词向量中\n",
      "societythe不存在于词向量中\n",
      "iswas不存在于词向量中\n",
      "gas!不存在于词向量中\n",
      "ecclestone不存在于词向量中\n",
      "guythe不存在于词向量中\n",
      "casti不存在于词向量中\n",
      "sherrys不存在于词向量中\n",
      "chandlers不存在于词向量中\n",
      "60000不存在于词向量中\n",
      "bromfield不存在于词向量中\n",
      "cest不存在于词向量中\n",
      "lage不存在于词向量中\n",
      "phibes不存在于词向量中\n",
      "remer不存在于词向量中\n",
      "caracas不存在于词向量中\n",
      "fluegel不存在于词向量中\n",
      "arzenta不存在于词向量中\n",
      "vera-ellens不存在于词向量中\n",
      "wasit不存在于词向量中\n",
      "productionthe不存在于词向量中\n",
      "alekos不存在于词向量中\n",
      "madagascar不存在于词向量中\n",
      "dorie不存在于词向量中\n",
      "deniss不存在于词向量中\n",
      "cq不存在于词向量中\n",
      "soloist不存在于词向量中\n",
      "fontaines不存在于词向量中\n",
      "earlys不存在于词向量中\n",
      "moviewhen不存在于词向量中\n",
      "berkowitzs不存在于词向量中\n",
      "huh!不存在于词向量中\n",
      "camerathe不存在于词向量中\n",
      "sorvinos不存在于词向量中\n",
      "flicki不存在于词向量中\n",
      "bibles不存在于词向量中\n",
      "subjectthe不存在于词向量中\n",
      "mehbooba不存在于词向量中\n",
      "yearsbut不存在于词向量中\n",
      "kinnears不存在于词向量中\n",
      "bethis不存在于词向量中\n",
      "luján不存在于词向量中\n",
      "naissance不存在于词向量中\n",
      "4510不存在于词向量中\n",
      "hbos不存在于词向量中\n",
      "spagnolo不存在于词向量中\n",
      "huggaland不存在于词向量中\n",
      "antitrust不存在于词向量中\n",
      "wwes不存在于词向量中\n",
      "fallwell不存在于词向量中\n",
      "gubra不存在于词向量中\n",
      "materialthe不存在于词向量中\n",
      "goodit不存在于词向量中\n",
      "linethe不存在于词向量中\n",
      "rashid不存在于词向量中\n",
      "wauters不存在于词向量中\n",
      "5ive不存在于词向量中\n",
      "1983s不存在于词向量中\n",
      "iswhat不存在于词向量中\n",
      "daysthis不存在于词向量中\n",
      "everi不存在于词向量中\n",
      "hagars不存在于词向量中\n",
      "rambos不存在于词向量中\n",
      "backi不存在于词向量中\n",
      "gentlemans不存在于词向量中\n",
      "thisa不存在于词向量中\n",
      "ryecart不存在于词向量中\n",
      "decors不存在于词向量中\n",
      "shelleys不存在于词向量中\n",
      "gerards不存在于词向量中\n",
      "anythingi不存在于词向量中\n",
      "lifeand不存在于词向量中\n",
      "lube不存在于词向量中\n",
      "coalwood不存在于词向量中\n",
      "shit不存在于词向量中\n",
      "unforgiven不存在于词向量中\n",
      "saras不存在于词向量中\n",
      "missing!不存在于词向量中\n",
      "naudets不存在于词向量中\n",
      "cinemagic不存在于词向量中\n",
      "betterthis不存在于词向量中\n",
      "yearsi不存在于词向量中\n",
      "cornillac不存在于词向量中\n",
      "florianes不存在于词向量中\n",
      "holroyd不存在于词向量中\n",
      "itwhen不存在于词向量中\n",
      "fini不存在于词向量中\n",
      "forthe不存在于词向量中\n",
      "basingers不存在于词向量中\n",
      "agethe不存在于词向量中\n",
      "chabrols不存在于词向量中\n",
      "trumans不存在于词向量中\n",
      "momentsbut不存在于词向量中\n",
      "vampires:不存在于词向量中\n",
      "**possible不存在于词向量中\n",
      "badand不存在于词向量中\n",
      "anotherthe不存在于词向量中\n",
      "bueller不存在于词向量中\n",
      "ardelean不存在于词向量中\n",
      "thatits不存在于词向量中\n",
      "foxworth不存在于词向量中\n",
      "chirila不存在于词向量中\n",
      "andwell不存在于词向量中\n",
      "twothe不存在于词向量中\n",
      "hillyers不存在于词向量中\n",
      "rightthis不存在于词向量中\n",
      "matteis不存在于词向量中\n",
      "lv1不存在于词向量中\n",
      "60searly不存在于词向量中\n",
      "marner不存在于词向量中\n",
      "gato不存在于词向量中\n",
      "stylethe不存在于词向量中\n",
      "shanao不存在于词向量中\n",
      "miraglias不存在于词向量中\n",
      "patheticthe不存在于词向量中\n",
      "seethe不存在于词向量中\n",
      "zapatti不存在于词向量中\n",
      "wolfes不存在于词向量中\n",
      "swifts不存在于词向量中\n",
      "gussie不存在于词向量中\n",
      "bestworst不存在于词向量中\n",
      "gasp!不存在于词向量中\n",
      "meas不存在于词向量中\n",
      "stans不存在于词向量中\n",
      "late-60s不存在于词向量中\n",
      "curtiss不存在于词向量中\n",
      "cragg不存在于词向量中\n",
      "lantana不存在于词向量中\n",
      "crains不存在于词向量中\n",
      "cut!不存在于词向量中\n",
      "pulps不存在于词向量中\n",
      "neenan不存在于词向量中\n",
      "tonalities不存在于词向量中\n",
      "waterdance不存在于词向量中\n",
      "choco不存在于词向量中\n",
      "scolas不存在于词向量中\n",
      "vaughns不存在于词向量中\n",
      "pieuvres不存在于词向量中\n",
      "sukowa不存在于词向量中\n",
      "halprin不存在于词向量中\n",
      "herthis不存在于词向量中\n",
      "watchingthe不存在于词向量中\n",
      "saint-denis不存在于词向量中\n",
      "rounders不存在于词向量中\n",
      "elviss不存在于词向量中\n",
      "yetthe不存在于词向量中\n",
      "hitoto不存在于词向量中\n",
      "stiflers不存在于词向量中\n",
      "andretti不存在于词向量中\n",
      "magruder不存在于词向量中\n",
      "charactersit不存在于词向量中\n",
      "arnies不存在于词向量中\n",
      "outthere不存在于词向量中\n",
      "schneebaums不存在于词向量中\n",
      "firestarter不存在于词向量中\n",
      "watchif不存在于词向量中\n",
      "dellorco不存在于词向量中\n",
      "losti不存在于词向量中\n",
      "murrays不存在于词向量中\n",
      "stupidi不存在于词向量中\n",
      "peopleif不存在于词向量中\n",
      "scorpione不存在于词向量中\n",
      "etcthis不存在于词向量中\n",
      "steppers不存在于词向量中\n",
      "hallgren不存在于词向量中\n",
      "$750不存在于词向量中\n",
      "ofthis不存在于词向量中\n",
      "charnier不存在于词向量中\n",
      "hak不存在于词向量中\n",
      "wrestlemanias不存在于词向量中\n",
      "4-3不存在于词向量中\n",
      "shinae不存在于词向量中\n",
      "goldsmiths不存在于词向量中\n",
      "marions不存在于词向量中\n",
      "pyms不存在于词向量中\n",
      "audiards不存在于词向量中\n",
      "placei不存在于词向量中\n",
      "thatch不存在于词向量中\n",
      "mohawk不存在于词向量中\n",
      "grandmas不存在于词向量中\n",
      "cama不存在于词向量中\n",
      "rataud不存在于词向量中\n",
      "infinitum不存在于词向量中\n",
      "toros不存在于词向量中\n",
      "lawrences不存在于词向量中\n",
      "along!不存在于词向量中\n",
      "gypos不存在于词向量中\n",
      "bradys不存在于词向量中\n",
      "carusos不存在于词向量中\n",
      "thunderchild不存在于词向量中\n",
      "crowd-pleaser不存在于词向量中\n",
      "surprisethe不存在于词向量中\n",
      "momentthe不存在于词向量中\n",
      "donnisons不存在于词向量中\n",
      "awayi不存在于词向量中\n",
      "bjorlin不存在于词向量中\n",
      "againthis不存在于词向量中\n",
      "seenit不存在于词向量中\n",
      "junge不存在于词向量中\n",
      "pabsts不存在于词向量中\n",
      "inand不存在于词向量中\n",
      "poor!不存在于词向量中\n",
      "demmes不存在于词向量中\n",
      "filmwhich不存在于词向量中\n",
      "notebooks不存在于词向量中\n",
      "moneyi不存在于词向量中\n",
      "`queen不存在于词向量中\n",
      "joycelyn不存在于词向量中\n",
      "midlers不存在于词向量中\n",
      "filmim不存在于词向量中\n",
      "centurythe不存在于词向量中\n",
      "isit不存在于词向量中\n",
      "goodoverall不存在于词向量中\n",
      "sydows不存在于词向量中\n",
      "likei不存在于词向量中\n",
      "unsinkable不存在于词向量中\n",
      "wolverines不存在于词向量中\n",
      "gothe不存在于词向量中\n",
      "turners不存在于词向量中\n",
      "mathesons不存在于词向量中\n",
      "platos不存在于词向量中\n",
      "soweto不存在于词向量中\n",
      "stuffthe不存在于词向量中\n",
      "moviemost不存在于词向量中\n",
      "carnality不存在于词向量中\n",
      "trampa不存在于词向量中\n",
      "02不存在于词向量中\n",
      "upthis不存在于词向量中\n",
      "ideathe不存在于词向量中\n",
      "timberlakes不存在于词向量中\n",
      "beit不存在于词向量中\n",
      "canyons不存在于词向量中\n",
      "farnsworths不存在于词向量中\n",
      "windscreen不存在于词向量中\n",
      "faso不存在于词向量中\n",
      "superstardom不存在于词向量中\n",
      "filmsits不存在于词向量中\n",
      "cinemathis不存在于词向量中\n",
      "aymeric不存在于词向量中\n",
      "munshi不存在于词向量中\n",
      "noams不存在于词向量中\n",
      "fitzs不存在于词向量中\n",
      "hiltz不存在于词向量中\n",
      "quantrill不存在于词向量中\n",
      "prairies不存在于词向量中\n",
      "performancesthe不存在于词向量中\n",
      "violencethe不存在于词向量中\n",
      "akhras不存在于词向量中\n",
      "birney不存在于词向量中\n",
      "worldthis不存在于词向量中\n",
      "grangers不存在于词向量中\n",
      "thuy不存在于词向量中\n",
      "katsus不存在于词向量中\n",
      "gruel不存在于词向量中\n",
      "spoonful不存在于词向量中\n",
      "iturbis不存在于词向量中\n",
      "abigails不存在于词向量中\n",
      "malthe不存在于词向量中\n",
      "filmhowever不存在于词向量中\n",
      "ofi不存在于词向量中\n",
      "ick不存在于词向量中\n",
      "kidsi不存在于词向量中\n",
      "khakee不存在于词向量中\n",
      "gorethe不存在于词向量中\n",
      "meds不存在于词向量中\n",
      "tykwers不存在于词向量中\n",
      "oaters不存在于词向量中\n",
      "showsthe不存在于词向量中\n",
      "moroder不存在于词向量中\n",
      "itafter不存在于词向量中\n",
      "parn不存在于词向量中\n",
      "moreira不存在于词向量中\n",
      "nazgul不存在于词向量中\n",
      "climaxthis不存在于词向量中\n",
      "mommys不存在于词向量中\n",
      "narasimha不存在于词向量中\n",
      "heckerling不存在于词向量中\n",
      "liviens不存在于词向量中\n",
      "cinematograph不存在于词向量中\n",
      "merediths不存在于词向量中\n",
      "iswell不存在于词向量中\n",
      "absurdness不存在于词向量中\n",
      "muchit不存在于词向量中\n",
      "horrorthe不存在于词向量中\n",
      "trotter不存在于词向量中\n",
      "http:wwwpetitiononlinecomgh1215petitionhtml不存在于词向量中\n",
      "frankie-boy不存在于词向量中\n",
      "rookers不存在于词向量中\n",
      "nebula不存在于词向量中\n",
      "tesis不存在于词向量中\n",
      "convolutions不存在于词向量中\n",
      "dietrichson不存在于词向量中\n",
      "wrights不存在于词向量中\n",
      "kazakos不存在于词向量中\n",
      "seachd不存在于词向量中\n",
      "glenrowan不存在于词向量中\n",
      "jew!不存在于词向量中\n",
      "burlinson不存在于词向量中\n",
      "hara不存在于词向量中\n",
      "movieshe不存在于词向量中\n",
      "velankar不存在于词向量中\n",
      "kelada不存在于词向量中\n",
      "linderby不存在于词向量中\n",
      "trackersjourneymen!!不存在于词向量中\n",
      "trumpeter不存在于词向量中\n",
      "napoli不存在于词向量中\n",
      "gadar不存在于词向量中\n",
      "byrons不存在于词向量中\n",
      "itnot不存在于词向量中\n",
      "dansu不存在于词向量中\n",
      "catherines不存在于词向量中\n",
      "shekhars不存在于词向量中\n",
      "lockes不存在于词向量中\n",
      "akshays不存在于词向量中\n",
      "hubbys不存在于词向量中\n",
      "brisbane不存在于词向量中\n",
      "annemarie不存在于词向量中\n",
      "rc不存在于词向量中\n",
      "sammis不存在于词向量中\n",
      "artemisias不存在于词向量中\n",
      "bookthe不存在于词向量中\n",
      "dialoguethe不存在于词向量中\n",
      "laudenbach不存在于词向量中\n",
      "foleys不存在于词向量中\n",
      "towelhead不存在于词向量中\n",
      "carrys不存在于词向量中\n",
      "narcolepsy不存在于词向量中\n",
      "centurys不存在于词向量中\n",
      "somethingthis不存在于词向量中\n",
      "watermans不存在于词向量中\n",
      "watling不存在于词向量中\n",
      "andies不存在于词向量中\n",
      "himits不存在于词向量中\n",
      "madea不存在于词向量中\n",
      "playthe不存在于词向量中\n",
      "successthe不存在于词向量中\n",
      "powerthe不存在于词向量中\n",
      "hutson不存在于词向量中\n",
      "minot不存在于词向量中\n",
      "kattans不存在于词向量中\n",
      "beetlejuice不存在于词向量中\n",
      "setthe不存在于词向量中\n",
      "isbut不存在于词向量中\n",
      "schrieber不存在于词向量中\n",
      "gnatpole不存在于词向量中\n",
      "haarman不存在于词向量中\n",
      "goould不存在于词向量中\n",
      "ddt不存在于词向量中\n",
      "movienot不存在于词向量中\n",
      "splat不存在于词向量中\n",
      "timetable不存在于词向量中\n",
      "bungalow不存在于词向量中\n",
      "cheang不存在于词向量中\n",
      "navel-gazing不存在于词向量中\n",
      "tadashis不存在于词向量中\n",
      "haack不存在于词向量中\n",
      "dafoes不存在于词向量中\n",
      "scoops不存在于词向量中\n",
      "piovani不存在于词向量中\n",
      "onit不存在于词向量中\n",
      "jakub不存在于词向量中\n",
      "ploti不存在于词向量中\n",
      "713不存在于词向量中\n",
      "gauris不存在于词向量中\n",
      "aswell不存在于词向量中\n",
      "altair不存在于词向量中\n",
      "pace!不存在于词向量中\n",
      "mules不存在于词向量中\n",
      "uranium不存在于词向量中\n",
      "eadie不存在于词向量中\n",
      "tolstoys不存在于词向量中\n",
      "actingthis不存在于词向量中\n",
      "rushton不存在于词向量中\n",
      "flux不存在于词向量中\n",
      "headthe不存在于词向量中\n",
      "schaffer不存在于词向量中\n",
      "fidos不存在于词向量中\n",
      "rubys不存在于词向量中\n",
      "tftc不存在于词向量中\n",
      "+-不存在于词向量中\n",
      "philanthropist不存在于词向量中\n",
      "1-2-3不存在于词向量中\n",
      "milford不存在于词向量中\n",
      "cias不存在于词向量中\n",
      "ajas不存在于词向量中\n",
      "realismthe不存在于词向量中\n",
      "mckees不存在于词向量中\n",
      "gabriels不存在于词向量中\n",
      "catwalk不存在于词向量中\n",
      "producerdirector不存在于词向量中\n",
      "mahers不存在于词向量中\n",
      "prez不存在于词向量中\n",
      "undecipherable不存在于词向量中\n",
      "coixet不存在于词向量中\n",
      "aarons不存在于词向量中\n",
      "nathans不存在于词向量中\n",
      "spradling不存在于词向量中\n",
      "ferraris不存在于词向量中\n",
      "bannings不存在于词向量中\n",
      "ashleys不存在于词向量中\n",
      "workand不存在于词向量中\n",
      "kagan不存在于词向量中\n",
      "margarete不存在于词向量中\n",
      "bethere不存在于词向量中\n",
      "dumroo不存在于词向量中\n",
      "bucktown不存在于词向量中\n",
      "likethe不存在于词向量中\n",
      "anotherthis不存在于词向量中\n",
      "schepisis不存在于词向量中\n",
      "notif不存在于词向量中\n",
      "amputee不存在于词向量中\n",
      "tarrs不存在于词向量中\n",
      "stirbas不存在于词向量中\n",
      "thunderball不存在于词向量中\n",
      "zimmers不存在于词向量中\n",
      "presidente不存在于词向量中\n",
      "etcso不存在于词向量中\n",
      "mid-30s不存在于词向量中\n",
      "plotthis不存在于词向量中\n",
      "mallepa不存在于词向量中\n",
      "reflexes不存在于词向量中\n",
      "preacherman不存在于词向量中\n",
      "werners不存在于词向量中\n",
      "againin不存在于词向量中\n",
      "tommys不存在于词向量中\n",
      "roachs不存在于词向量中\n",
      "eikenberry不存在于词向量中\n",
      "wowi不存在于词向量中\n",
      "itwith不存在于词向量中\n",
      "cuarón不存在于词向量中\n",
      "natch不存在于词向量中\n",
      "suo不存在于词向量中\n",
      "bigs不存在于词向量中\n",
      "matterthe不存在于词向量中\n",
      "nadji不存在于词向量中\n",
      "meretricious不存在于词向量中\n",
      "hawns不存在于词向量中\n",
      "actordirector不存在于词向量中\n",
      "olivers不存在于词向量中\n",
      "glickenhaus不存在于词向量中\n",
      "enoughthe不存在于词向量中\n",
      "pelletier不存在于词向量中\n",
      "rolethis不存在于词向量中\n",
      "directorproducer不存在于词向量中\n",
      "meif不存在于词向量中\n",
      "collettes不存在于词向量中\n",
      "thingthis不存在于词向量中\n",
      "tuileries不存在于词向量中\n",
      "hadass不存在于词向量中\n",
      "alzheimers不存在于词向量中\n",
      "etzel不存在于词向量中\n",
      "timesand不存在于词向量中\n",
      "ulees不存在于词向量中\n",
      "ruge不存在于词向量中\n",
      "kine不存在于词向量中\n",
      "waya不存在于词向量中\n",
      "winslets不存在于词向量中\n",
      "newell不存在于词向量中\n",
      "frollo不存在于词向量中\n",
      "earlierthe不存在于词向量中\n",
      "non-existant不存在于词向量中\n",
      "bauraki不存在于词向量中\n",
      "goldbergs不存在于词向量中\n",
      "film!!!!不存在于词向量中\n",
      "danis不存在于词向量中\n",
      "etcit不存在于词向量中\n",
      "showroom不存在于词向量中\n",
      "belas不存在于词向量中\n",
      "cosimo不存在于词向量中\n",
      "f-不存在于词向量中\n",
      "dramathe不存在于词向量中\n",
      "itone不存在于词向量中\n",
      "melchior不存在于词向量中\n",
      "sinclairs不存在于词向量中\n",
      "barbarella不存在于词向量中\n",
      "boen不存在于词向量中\n",
      "acorn不存在于词向量中\n",
      "coincidently不存在于词向量中\n",
      "watchi不存在于词向量中\n",
      "beethovens不存在于词向量中\n",
      "antonius不存在于词向量中\n",
      "godfathers不存在于词向量中\n",
      "mongolia不存在于词向量中\n",
      "okeefe不存在于词向量中\n",
      "ramallo不存在于词向量中\n",
      "doa不存在于词向量中\n",
      "konvitz不存在于词向量中\n",
      "ge999不存在于词向量中\n",
      "pertwees不存在于词向量中\n",
      "rivera不存在于词向量中\n",
      ":-d不存在于词向量中\n",
      "bernhards不存在于词向量中\n",
      "gymnasium不存在于词向量中\n",
      "chancethe不存在于词向量中\n",
      "worldas不存在于词向量中\n",
      "audiencei不存在于词向量中\n",
      "domergues不存在于词向量中\n",
      "windman不存在于词向量中\n",
      "ithowever不存在于词向量中\n",
      "overhyped不存在于词向量中\n",
      "piedras不存在于词向量中\n",
      "dillons不存在于词向量中\n",
      "murdersthe不存在于词向量中\n",
      "yore不存在于词向量中\n",
      "heflins不存在于词向量中\n",
      "outin不存在于词向量中\n",
      "spiritualism不存在于词向量中\n",
      "pryors不存在于词向量中\n",
      "fani不存在于词向量中\n",
      "scenesthis不存在于词向量中\n",
      "dothis不存在于词向量中\n",
      "gornick不存在于词向量中\n",
      "seriouslythe不存在于词向量中\n",
      "tourneurs不存在于词向量中\n",
      "bullhorn不存在于词向量中\n",
      "cuckold不存在于词向量中\n",
      "kolchak:不存在于词向量中\n",
      "dreama不存在于词向量中\n",
      "soll不存在于词向量中\n",
      "matts不存在于词向量中\n",
      "bushs不存在于词向量中\n",
      "*may不存在于词向量中\n",
      "joshs不存在于词向量中\n",
      "phoenixs不存在于词向量中\n",
      "alsoi不存在于词向量中\n",
      "endingthis不存在于词向量中\n",
      "insp不存在于词向量中\n",
      "burgundians不存在于词向量中\n",
      "ak-47不存在于词向量中\n",
      "raffin不存在于词向量中\n",
      "tutors不存在于词向量中\n",
      "barek不存在于词向量中\n",
      "wella不存在于词向量中\n",
      "$250不存在于词向量中\n",
      "riedelsheimer不存在于词向量中\n",
      "gummo不存在于词向量中\n",
      "consummated不存在于词向量中\n",
      "motherdaughter不存在于词向量中\n",
      "girlthe不存在于词向量中\n",
      "mindi不存在于词向量中\n",
      "directioni不存在于词向量中\n",
      "doers不存在于词向量中\n",
      "rheostatics不存在于词向量中\n",
      "noahs不存在于词向量中\n",
      "mean!不存在于词向量中\n",
      "filmthat不存在于词向量中\n",
      "justins不存在于词向量中\n",
      "quintana不存在于词向量中\n",
      "muchand不存在于词向量中\n",
      "filmoh不存在于词向量中\n",
      "fancier不存在于词向量中\n",
      "extinguishers不存在于词向量中\n",
      "meh不存在于词向量中\n",
      "brendon不存在于词向量中\n",
      "charactersas不存在于词向量中\n",
      "eitherits不存在于词向量中\n",
      "thatso不存在于词向量中\n",
      "dkd不存在于词向量中\n",
      "trenchcoat不存在于词向量中\n",
      "newcombes不存在于词向量中\n",
      "top-quality不存在于词向量中\n",
      "chewie不存在于词向量中\n",
      "adieu不存在于词向量中\n",
      "storyits不存在于词向量中\n",
      "kroko不存在于词向量中\n",
      "welchs不存在于词向量中\n",
      "klute不存在于词向量中\n",
      "show-off不存在于词向量中\n",
      "titanics不存在于词向量中\n",
      "citythe不存在于词向量中\n",
      "turaqistan不存在于词向量中\n",
      "yoshida不存在于词向量中\n",
      "bards不存在于词向量中\n",
      "williamss不存在于词向量中\n",
      "yakitate!不存在于词向量中\n",
      "deans不存在于词向量中\n",
      "yodas不存在于词向量中\n",
      "fp1不存在于词向量中\n",
      "iton不存在于词向量中\n",
      "briny不存在于词向量中\n",
      "barneys不存在于词向量中\n",
      "withi不存在于词向量中\n",
      "rebar不存在于词向量中\n",
      "loulou不存在于词向量中\n",
      "callan不存在于词向量中\n",
      "beenthe不存在于词向量中\n",
      "havethe不存在于词向量中\n",
      "non-event不存在于词向量中\n",
      "floyds不存在于词向量中\n",
      "yolande不存在于词向量中\n",
      "emotionthe不存在于词向量中\n",
      "himherself不存在于词向量中\n",
      "mensonges不存在于词向量中\n",
      "tingler不存在于词向量中\n",
      "uranus不存在于词向量中\n",
      "homefront不存在于词向量中\n",
      "happenedi不存在于词向量中\n",
      "delias不存在于词向量中\n",
      "hypercube不存在于词向量中\n",
      "watchedthe不存在于词向量中\n",
      "beforethis不存在于词向量中\n",
      "jesters不存在于词向量中\n",
      "housethe不存在于词向量中\n",
      "actinga不存在于词向量中\n",
      "scarman不存在于词向量中\n",
      "yeari不存在于词向量中\n",
      "blachere不存在于词向量中\n",
      "timeso不存在于词向量中\n",
      "lifebut不存在于词向量中\n",
      "aro不存在于词向量中\n",
      "meand不存在于词向量中\n",
      "kol不存在于词向量中\n",
      "interestthe不存在于词向量中\n",
      "thatin不存在于词向量中\n",
      "brideshead不存在于词向量中\n",
      "usas不存在于词向量中\n",
      "deannas不存在于词向量中\n",
      "genn不存在于词向量中\n",
      "malamud不存在于词向量中\n",
      "dcom不存在于词向量中\n",
      "allif不存在于词向量中\n",
      "cassies不存在于词向量中\n",
      "levene不存在于词向量中\n",
      "solino不存在于词向量中\n",
      "cabals不存在于词向量中\n",
      "republics不存在于词向量中\n",
      "lams不存在于词向量中\n",
      "budgetthe不存在于词向量中\n",
      "salo不存在于词向量中\n",
      "juliana不存在于词向量中\n",
      "southampton不存在于词向量中\n",
      "lamia不存在于词向量中\n",
      "dibley不存在于词向量中\n",
      "englebert不存在于词向量中\n",
      "movieyes不存在于词向量中\n",
      "durantes不存在于词向量中\n",
      "conchata不存在于词向量中\n",
      "interestingi不存在于词向量中\n",
      "duchovnys不存在于词向量中\n",
      "dentistry不存在于词向量中\n",
      "opinioni不存在于词向量中\n",
      "goodtimes不存在于词向量中\n",
      "madeand不存在于词向量中\n",
      "barrys不存在于词向量中\n",
      "write-off不存在于词向量中\n",
      "missi不存在于词向量中\n",
      "curtizs不存在于词向量中\n",
      "videoi不存在于词向量中\n",
      "playoffs不存在于词向量中\n",
      "scripti不存在于词向量中\n",
      "itwho不存在于词向量中\n",
      "don-del-oro不存在于词向量中\n",
      "randle不存在于词向量中\n",
      "loessers不存在于词向量中\n",
      "hipness不存在于词向量中\n",
      "isotopes不存在于词向量中\n",
      "bacalls不存在于词向量中\n",
      "pavarottis不存在于词向量中\n",
      "madethis不存在于词向量中\n",
      "gaillardia不存在于词向量中\n",
      "sergius不存在于词向量中\n",
      "fedja不存在于词向量中\n",
      "wiseman不存在于词向量中\n",
      "kandice不存在于词向量中\n",
      "thrush不存在于词向量中\n",
      "hupperts不存在于词向量中\n",
      "itsome不存在于词向量中\n",
      "pointthis不存在于词向量中\n",
      "raimy不存在于词向量中\n",
      "seenthis不存在于词向量中\n",
      "deth不存在于词向量中\n",
      "itno不存在于词向量中\n",
      "bulldozers不存在于词向量中\n",
      "yanos不存在于词向量中\n",
      "dangerfields不存在于词向量中\n",
      "spiers不存在于词向量中\n",
      "chomet不存在于词向量中\n",
      "watchingi不存在于词向量中\n",
      "chriss不存在于词向量中\n",
      "slugs:不存在于词向量中\n",
      "culkins不存在于词向量中\n",
      "chehs不存在于词向量中\n",
      "mcnicol不存在于词向量中\n",
      "ermine不存在于词向量中\n",
      "filmwith不存在于词向量中\n",
      "zealanders不存在于词向量中\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**spoilers**不存在于词向量中\n",
      "aboriginals不存在于词向量中\n",
      "alexandres不存在于词向量中\n",
      "emigrant不存在于词向量中\n",
      "paramilitaries不存在于词向量中\n",
      "itof不存在于词向量中\n",
      "snore-fest不存在于词向量中\n",
      "lettieri不存在于词向量中\n",
      "gulfax不存在于词向量中\n",
      "blackwhite不存在于词向量中\n",
      "werewolfs不存在于词向量中\n",
      "filmto不存在于词向量中\n",
      "marjories不存在于词向量中\n",
      "hugsy不存在于词向量中\n",
      "zuckerman不存在于词向量中\n",
      "coote不存在于词向量中\n",
      "watchtower不存在于词向量中\n",
      "hereafter不存在于词向量中\n",
      "hyser不存在于词向量中\n",
      "tysons不存在于词向量中\n",
      "site!不存在于词向量中\n",
      "perspectivethe不存在于词向量中\n",
      "thunderbolt不存在于词向量中\n",
      "potentialthe不存在于词向量中\n",
      "everythingthe不存在于词向量中\n",
      "abstractions不存在于词向量中\n",
      "rainbeaux不存在于词向量中\n",
      "heorot不存在于词向量中\n",
      "wtf!不存在于词向量中\n",
      "monteith不存在于词向量中\n",
      "lukass不存在于词向量中\n",
      "lancasters不存在于词向量中\n",
      "4-bad不存在于词向量中\n",
      "richies不存在于词向量中\n",
      "rothschild不存在于词向量中\n",
      "lengles不存在于词向量中\n",
      "wangs不存在于词向量中\n",
      "noland不存在于词向量中\n",
      "mechs不存在于词向量中\n",
      "crouses不存在于词向量中\n",
      "så不存在于词向量中\n",
      "inhe不存在于词向量中\n",
      "videothe不存在于词向量中\n",
      "friendless不存在于词向量中\n",
      "antonella不存在于词向量中\n",
      "hammerhead:不存在于词向量中\n",
      "writerthe不存在于词向量中\n",
      "acs不存在于词向量中\n",
      "fixit不存在于词向量中\n",
      "directionthe不存在于词向量中\n",
      "`mr不存在于词向量中\n",
      "pvt不存在于词向量中\n",
      "tgif不存在于词向量中\n",
      "valcos不存在于词向量中\n",
      "holms不存在于词向量中\n",
      "buddys不存在于词向量中\n",
      "twos不存在于词向量中\n",
      "domini不存在于词向量中\n",
      "trombone不存在于词向量中\n",
      "showand不存在于词向量中\n",
      "demonstrative不存在于词向量中\n",
      "theni不存在于词向量中\n",
      "filmby不存在于词向量中\n",
      "thesethe不存在于词向量中\n",
      "fl不存在于词向量中\n",
      "greatand不存在于词向量中\n",
      "richardsons不存在于词向量中\n",
      "cassavettess不存在于词向量中\n",
      "obrian不存在于词向量中\n",
      "storyfor不存在于词向量中\n",
      "bohlen不存在于词向量中\n",
      "ambassadors不存在于词向量中\n",
      "huntingdon不存在于词向量中\n",
      "pasadena不存在于词向量中\n",
      "vampiros不存在于词向量中\n",
      "ladykillers不存在于词向量中\n",
      "sadies不存在于词向量中\n",
      "genoa不存在于词向量中\n",
      "molinas不存在于词向量中\n",
      "gr不存在于词向量中\n",
      "matchthe不存在于词向量中\n",
      "careys不存在于词向量中\n",
      "lifeone不存在于词向量中\n",
      "fn不存在于词向量中\n",
      "arisen不存在于词向量中\n",
      "heroheroine不存在于词向量中\n",
      "bregana不存在于词向量中\n",
      "hateable不存在于词向量中\n",
      "watership不存在于词向量中\n",
      "forrests不存在于词向量中\n",
      "talosians不存在于词向量中\n",
      "digicorps不存在于词向量中\n",
      "othellos不存在于词向量中\n",
      "luminescence不存在于词向量中\n",
      "gouald不存在于词向量中\n",
      "onin不存在于词向量中\n",
      "penry-jones不存在于词向量中\n",
      "hadleyville不存在于词向量中\n",
      "martys不存在于词向量中\n",
      "directorthis不存在于词向量中\n",
      "gonethe不存在于词向量中\n",
      "db不存在于词向量中\n",
      "hereit不存在于词向量中\n",
      "onesthe不存在于词向量中\n",
      "catalan不存在于词向量中\n",
      "hauntings不存在于词向量中\n",
      "brilliantthe不存在于词向量中\n",
      "darkheart不存在于词向量中\n",
      "alois不存在于词向量中\n",
      "roebuck不存在于词向量中\n",
      "roomthe不存在于词向量中\n",
      "langlaise不存在于词向量中\n",
      "finishthe不存在于词向量中\n",
      "summersisle不存在于词向量中\n",
      "joyces不存在于词向量中\n",
      "starck不存在于词向量中\n",
      "mitchells不存在于词向量中\n",
      "hereand不存在于词向量中\n",
      "mrlincoln不存在于词向量中\n",
      "levelthe不存在于词向量中\n",
      "dammit不存在于词向量中\n",
      "burrs不存在于词向量中\n",
      "torrances不存在于词向量中\n",
      "borge不存在于词向量中\n",
      "sculptures不存在于词向量中\n",
      "groovin不存在于词向量中\n",
      "bilious不存在于词向量中\n",
      "victoires不存在于词向量中\n",
      "klaws不存在于词向量中\n",
      "otooles不存在于词向量中\n",
      "fois不存在于词向量中\n",
      "carradines不存在于词向量中\n",
      "tippi不存在于词向量中\n",
      "artthe不存在于词向量中\n",
      "anands不存在于词向量中\n",
      "hassie不存在于词向量中\n",
      "yoshinaga不存在于词向量中\n",
      "666:不存在于词向量中\n",
      "linfield不存在于词向量中\n",
      "periodi不存在于词向量中\n",
      "timedirector不存在于词向量中\n",
      "zenias不存在于词向量中\n",
      "livin不存在于词向量中\n",
      "hackmans不存在于词向量中\n",
      "masons不存在于词向量中\n",
      "07不存在于词向量中\n",
      "christophers不存在于词向量中\n",
      "peptides不存在于词向量中\n",
      "filmvery不存在于词向量中\n",
      "wayif不存在于词向量中\n",
      "fenwick不存在于词向量中\n",
      "migraines不存在于词向量中\n",
      "repairman不存在于词向量中\n",
      "worsei不存在于词向量中\n",
      "childthis不存在于词向量中\n",
      "trelkovskys不存在于词向量中\n",
      "dresslers不存在于词向量中\n",
      "reveres不存在于词向量中\n",
      "bernsens不存在于词向量中\n",
      "poms不存在于词向量中\n",
      "curlys不存在于词向量中\n",
      "facei不存在于词向量中\n",
      "moviesso不存在于词向量中\n",
      "showthis不存在于词向量中\n",
      "proog不存在于词向量中\n",
      "homoeroticism不存在于词向量中\n",
      "moviehowever不存在于词向量中\n",
      "es不存在于词向量中\n",
      "eustaches不存在于词向量中\n",
      "chabon不存在于词向量中\n",
      "gokbakar不存在于词向量中\n",
      "cubas不存在于词向量中\n",
      "mistakethe不存在于词向量中\n",
      "leonoras不存在于词向量中\n",
      "gérald不存在于词向量中\n",
      "cullen不存在于词向量中\n",
      "allim不存在于词向量中\n",
      "scenei不存在于词向量中\n",
      "sequencethe不存在于词向量中\n",
      "himso不存在于词向量中\n",
      "enemys不存在于词向量中\n",
      "owni不存在于词向量中\n",
      "storyas不存在于词向量中\n",
      "!!!!!!!!不存在于词向量中\n",
      "neatness不存在于词向量中\n",
      "musicthis不存在于词向量中\n",
      "prestons不存在于词向量中\n",
      "voyagers不存在于词向量中\n",
      "upand不存在于词向量中\n",
      "$12000000不存在于词向量中\n",
      "dornwinkle不存在于词向量中\n",
      "eitherthis不存在于词向量中\n",
      "wellyou不存在于词向量中\n",
      "inas不存在于词向量中\n",
      "somethingthe不存在于词向量中\n",
      "gp不存在于词向量中\n",
      "cothk不存在于词向量中\n",
      "razzle-dazzle不存在于词向量中\n",
      "personi不存在于词向量中\n",
      "rivendell不存在于词向量中\n",
      "casei不存在于词向量中\n",
      "lorettas不存在于词向量中\n",
      "itid不存在于词向量中\n",
      "alberta不存在于词向量中\n",
      "who-dunnit不存在于词向量中\n",
      "gainey不存在于词向量中\n",
      "evelyns不存在于词向量中\n",
      "bolero不存在于词向量中\n",
      "disappointedthis不存在于词向量中\n",
      "bluths不存在于词向量中\n",
      "shortthe不存在于词向量中\n",
      "scenebut不存在于词向量中\n",
      "cuoco不存在于词向量中\n",
      "mir不存在于词向量中\n",
      "lighti不存在于词向量中\n",
      "crandall不存在于词向量中\n",
      "mats不存在于词向量中\n",
      "tereza不存在于词向量中\n",
      "launius不存在于词向量中\n",
      "hepton不存在于词向量中\n",
      "1s不存在于词向量中\n",
      "vidors不存在于词向量中\n",
      "interestingand不存在于词向量中\n",
      "nixons不存在于词向量中\n",
      "chastened不存在于词向量中\n",
      "10the不存在于词向量中\n",
      "mcgavins不存在于词向量中\n",
      "mauricio不存在于词向量中\n",
      "wargames:不存在于词向量中\n",
      "amen不存在于词向量中\n",
      "ariauna不存在于词向量中\n",
      "s2t不存在于词向量中\n",
      "raina不存在于词向量中\n",
      "hensons不存在于词向量中\n",
      "charactersthis不存在于词向量中\n",
      "castros不存在于词向量中\n",
      "amazingand不存在于词向量中\n",
      "pratfall不存在于词向量中\n",
      "thatthere不存在于词向量中\n",
      "geisel不存在于词向量中\n",
      "boththe不存在于词向量中\n",
      "shae不存在于词向量中\n",
      "ebsen不存在于词向量中\n",
      "shahids不存在于词向量中\n",
      "kennicut不存在于词向量中\n",
      "molla不存在于词向量中\n",
      "flawsthe不存在于词向量中\n",
      "herethere不存在于词向量中\n",
      "aroundi不存在于词向量中\n",
      "amenabars不存在于词向量中\n",
      "descendents不存在于词向量中\n",
      "townsends不存在于词向量中\n",
      "erathe不存在于词向量中\n",
      "asimovs不存在于词向量中\n",
      "okeeffe不存在于词向量中\n",
      "père-lachaise不存在于词向量中\n",
      "tvthe不存在于词向量中\n",
      "ferrells不存在于词向量中\n",
      "monasterys不存在于词向量中\n",
      "themwhat不存在于词向量中\n",
      "petiots不存在于词向量中\n",
      "entanglement不存在于词向量中\n",
      "facilitator不存在于词向量中\n",
      "grandpas不存在于词向量中\n",
      "schepisi不存在于词向量中\n",
      "mostels不存在于词向量中\n",
      "craigs不存在于词向量中\n",
      "steeles不存在于词向量中\n",
      "woah不存在于词向量中\n",
      "rodneys不存在于词向量中\n",
      "donofrio不存在于词向量中\n",
      "vadas不存在于词向量中\n",
      "tasuiev不存在于词向量中\n",
      "whoopee不存在于词向量中\n",
      "moviewith不存在于词向量中\n",
      "isand不存在于词向量中\n",
      "knowthis不存在于词向量中\n",
      "calvins不存在于词向量中\n",
      "saskatchewan不存在于词向量中\n",
      "amati不存在于词向量中\n",
      "anodyne不存在于词向量中\n",
      "thefts不存在于词向量中\n",
      "saboteur不存在于词向量中\n",
      "js不存在于词向量中\n",
      "gingers不存在于词向量中\n",
      "1010!不存在于词向量中\n",
      "habitats不存在于词向量中\n",
      "70si不存在于词向量中\n",
      "worsethis不存在于词向量中\n",
      "beforei不存在于词向量中\n",
      "witherspoons不存在于词向量中\n",
      "barnett不存在于词向量中\n",
      "nolans不存在于词向量中\n",
      "herand不存在于词向量中\n",
      "mjs不存在于词向量中\n",
      "jobi不存在于词向量中\n",
      "lifeas不存在于词向量中\n",
      "renoirs不存在于词向量中\n",
      "jesuss不存在于词向量中\n",
      "roxbury不存在于词向量中\n",
      "amis不存在于词向量中\n",
      "disappointedi不存在于词向量中\n",
      "cardos不存在于词向量中\n",
      "wendts不存在于词向量中\n",
      "torturei不存在于词向量中\n",
      "tentitle不存在于词向量中\n",
      "pias不存在于词向量中\n",
      "hickey不存在于词向量中\n",
      "goin不存在于词向量中\n",
      "rozs不存在于词向量中\n",
      "ginas不存在于词向量中\n",
      "didt不存在于词向量中\n",
      "loti不存在于词向量中\n",
      "vette不存在于词向量中\n",
      "siv不存在于词向量中\n",
      "eg:不存在于词向量中\n",
      "aragami不存在于词向量中\n",
      "jehovahs不存在于词向量中\n",
      "tristans不存在于词向量中\n",
      "thatafter不存在于词向量中\n",
      "alonethe不存在于词向量中\n",
      "tel-aviv不存在于词向量中\n",
      "cpt不存在于词向量中\n",
      "rainmaker不存在于词向量中\n",
      "irans不存在于词向量中\n",
      "darc不存在于词向量中\n",
      "muchif不存在于词向量中\n",
      "talentthe不存在于词向量中\n",
      "pazus不存在于词向量中\n",
      "scotlands不存在于词向量中\n",
      "asgard不存在于词向量中\n",
      "1894不存在于词向量中\n",
      "grandsons不存在于词向量中\n",
      "veils不存在于词向量中\n",
      "radiofreccia不存在于词向量中\n",
      "endbut不存在于词向量中\n",
      "catscratch不存在于词向量中\n",
      "word!不存在于词向量中\n",
      "suplexes不存在于词向量中\n",
      "sin-eater不存在于词向量中\n",
      "beaute不存在于词向量中\n",
      "bambis不存在于词向量中\n",
      "donlan不存在于词向量中\n",
      "cervera不存在于词向量中\n",
      "lessthe不存在于词向量中\n",
      "bangladesh不存在于词向量中\n",
      "hanlon不存在于词向量中\n",
      "testa不存在于词向量中\n",
      "capras不存在于词向量中\n",
      "amandas不存在于词向量中\n"
     ]
    }
   ],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.labelList = []\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        \n",
    "        if self.config.numClasses == 1:\n",
    "            labels = df[\"sentiment\"].tolist()\n",
    "        elif self.config.numClasses > 1:\n",
    "            labels = df[\"rate\"].tolist()\n",
    "            \n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "    \n",
    "    def _labelToIndex(self, labels, label2idx):\n",
    "        \"\"\"\n",
    "        将标签转换成索引表示\n",
    "        \"\"\"\n",
    "        labelIds = [label2idx[label] for label in labels]\n",
    "        return labelIds\n",
    "    \n",
    "    def _wordToIndex(self, reviews, word2idx):\n",
    "        \"\"\"\n",
    "        将词转换成索引\n",
    "        \"\"\"\n",
    "        reviewIds = [[word2idx.get(item, word2idx[\"UNK\"]) for item in review] for review in reviews]\n",
    "        return reviewIds\n",
    "        \n",
    "    def _genTrainEvalData(self, x, y, word2idx, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        reviews = []\n",
    "        for review in x:\n",
    "            if len(review) >= self._sequenceLength:\n",
    "                reviews.append(review[:self._sequenceLength])\n",
    "            else:\n",
    "                reviews.append(review + [word2idx[\"PAD\"]] * (self._sequenceLength - len(review)))\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(y[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(y[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews, labels):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        word2idx = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        \n",
    "        uniqueLabel = list(set(labels))\n",
    "        label2idx = dict(zip(uniqueLabel, list(range(len(uniqueLabel)))))\n",
    "        self.labelList = list(range(len(uniqueLabel)))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/word2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(word2idx, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/label2idx.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(label2idx, f)\n",
    "        \n",
    "        return word2idx, label2idx\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"PAD\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        word2idx, label2idx = self._genVocabulary(reviews, labels)\n",
    "        \n",
    "        # 将标签和句子数值化\n",
    "        labelIds = self._labelToIndex(labels, label2idx)\n",
    "        reviewIds = self._wordToIndex(reviews, word2idx)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviewIds, labelIds, word2idx, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000,)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class BiLSTM(object):\n",
    "    \"\"\"\n",
    "    Bi-LSTM 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.int32, [None], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2]\n",
    "                    self.embeddedWords = tf.concat(outputs, 2)\n",
    "        \n",
    "        # 去除最后时间步的输出作为全连接的输入\n",
    "        finalOutput = self.embeddedWords[:, -1, :]\n",
    "        \n",
    "        outputSize = config.model.hiddenSizes[-1] * 2  # 因为是双向LSTM，最终的输出值是fw和bw的拼接，因此要乘以2\n",
    "        output = tf.reshape(finalOutput, [-1, outputSize])  # reshape成全连接层的输入维度\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, config.numClasses],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[config.numClasses]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.logits = tf.nn.xw_plus_b(output, outputW, outputB, name=\"logits\")\n",
    "            if config.numClasses == 1:\n",
    "                self.predictions = tf.cast(tf.greater_equal(self.logits, 0.0), tf.float32, name=\"predictions\")\n",
    "            elif config.numClasses > 1:\n",
    "                self.predictions = tf.argmax(self.logits, axis=-1, name=\"predictions\")\n",
    "            print(self.predictions)\n",
    "            \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=tf.cast(tf.reshape(self.inputY, [-1, 1]), \n",
    "                                                                                                    dtype=tf.float32))\n",
    "            elif config.numClasses > 1:\n",
    "                losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.inputY)\n",
    "                \n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "定义各类性能指标\n",
    "\"\"\"\n",
    "\n",
    "def mean(item: list) -> float:\n",
    "    \"\"\"\n",
    "    计算列表中元素的平均值\n",
    "    :param item: 列表对象\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    res = sum(item) / len(item) if len(item) > 0 else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def accuracy(pred_y, true_y):\n",
    "    \"\"\"\n",
    "    计算二类和多类的准确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "    corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == true_y[i]:\n",
    "            corr += 1\n",
    "    acc = corr / len(pred_y) if len(pred_y) > 0 else 0\n",
    "    return acc\n",
    "\n",
    "\n",
    "def binary_precision(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的精确率计算\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    pred_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == positive:\n",
    "            pred_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    prec = corr / pred_corr if pred_corr > 0 else 0\n",
    "    return prec\n",
    "\n",
    "\n",
    "def binary_recall(pred_y, true_y, positive=1):\n",
    "    \"\"\"\n",
    "    二类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    corr = 0\n",
    "    true_corr = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if true_y[i] == positive:\n",
    "            true_corr += 1\n",
    "            if pred_y[i] == true_y[i]:\n",
    "                corr += 1\n",
    "\n",
    "    rec = corr / true_corr if true_corr > 0 else 0\n",
    "    return rec\n",
    "\n",
    "\n",
    "def binary_f_beta(pred_y, true_y, beta=1.0, positive=1):\n",
    "    \"\"\"\n",
    "    二类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param beta: beta值\n",
    "    :param positive: 正例的索引表示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    precision = binary_precision(pred_y, true_y, positive)\n",
    "    recall = binary_recall(pred_y, true_y, positive)\n",
    "    try:\n",
    "        f_b = (1 + beta * beta) * precision * recall / (beta * beta * precision + recall)\n",
    "    except:\n",
    "        f_b = 0\n",
    "    return f_b\n",
    "\n",
    "\n",
    "def multi_precision(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的精确率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    precisions = [binary_precision(pred_y, true_y, label) for label in labels]\n",
    "    prec = mean(precisions)\n",
    "    return prec\n",
    "\n",
    "\n",
    "def multi_recall(pred_y, true_y, labels):\n",
    "    \"\"\"\n",
    "    多类的召回率\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    recalls = [binary_recall(pred_y, true_y, label) for label in labels]\n",
    "    rec = mean(recalls)\n",
    "    return rec\n",
    "\n",
    "\n",
    "def multi_f_beta(pred_y, true_y, labels, beta=1.0):\n",
    "    \"\"\"\n",
    "    多类的f beta值\n",
    "    :param pred_y: 预测结果\n",
    "    :param true_y: 真实结果\n",
    "    :param labels: 标签列表\n",
    "    :param beta: beta值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if isinstance(pred_y[0], list):\n",
    "        pred_y = [item[0] for item in pred_y]\n",
    "\n",
    "    f_betas = [binary_f_beta(pred_y, true_y, beta, label) for label in labels]\n",
    "    f_beta = mean(f_betas)\n",
    "    return f_beta\n",
    "\n",
    "\n",
    "def get_binary_metrics(pred_y, true_y, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到二分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = binary_recall(pred_y, true_y)\n",
    "    precision = binary_precision(pred_y, true_y)\n",
    "    f_beta = binary_f_beta(pred_y, true_y, f_beta)\n",
    "    return acc, recall, precision, f_beta\n",
    "\n",
    "\n",
    "def get_multi_metrics(pred_y, true_y, labels, f_beta=1.0):\n",
    "    \"\"\"\n",
    "    得到多分类的性能指标\n",
    "    :param pred_y:\n",
    "    :param true_y:\n",
    "    :param labels:\n",
    "    :param f_beta:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    acc = accuracy(pred_y, true_y)\n",
    "    recall = multi_recall(pred_y, true_y, labels)\n",
    "    precision = multi_precision(pred_y, true_y, labels)\n",
    "    f_beta = multi_f_beta(pred_y, true_y, labels, f_beta)\n",
    "    return acc, recall, precision, f_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0726 14:25:57.413639 140504067012352 deprecation.py:323] From <ipython-input-6-4ca767c2c1b3>:31: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0726 14:25:57.420497 140504067012352 deprecation.py:323] From <ipython-input-6-4ca767c2c1b3>:43: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "W0726 14:25:57.421681 140504067012352 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn.py:464: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0726 14:25:57.501900 140504067012352 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0726 14:25:57.513026 140504067012352 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:961: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0726 14:25:59.164296 140504067012352 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0726 14:25:59.188365 140504067012352 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"output/predictions:0\", shape=(?, 1), dtype=float32)\n",
      "Writing to /home/jqxx/jupyternotebook/textClassifier-master/Bi-LSTM/summarys\n",
      "\n",
      "start training model\n",
      "train: step: 1, loss: 0.691848635673523, acc: 0.5234375, recall: 0.9523809523809523, precision: 0.5084745762711864, f_beta: 0.6629834254143646\n",
      "train: step: 2, loss: 0.7310537099838257, acc: 0.453125, recall: 1.0, precision: 0.44881889763779526, f_beta: 0.6195652173913043\n",
      "train: step: 3, loss: 0.8086796998977661, acc: 0.53125, recall: 0.4225352112676056, precision: 0.6122448979591837, f_beta: 0.4999999999999999\n",
      "train: step: 4, loss: 0.7384079098701477, acc: 0.4609375, recall: 0.42424242424242425, precision: 0.4745762711864407, f_beta: 0.448\n",
      "train: step: 5, loss: 0.7022448182106018, acc: 0.5546875, recall: 0.9855072463768116, precision: 0.5483870967741935, f_beta: 0.7046632124352332\n",
      "train: step: 6, loss: 0.7710385322570801, acc: 0.484375, recall: 1.0, precision: 0.48031496062992124, f_beta: 0.648936170212766\n",
      "train: step: 7, loss: 0.6992055177688599, acc: 0.515625, recall: 1.0, precision: 0.5118110236220472, f_beta: 0.6770833333333334\n",
      "train: step: 8, loss: 0.700127124786377, acc: 0.4921875, recall: 0.9508196721311475, precision: 0.48333333333333334, f_beta: 0.6408839779005524\n",
      "train: step: 9, loss: 0.6911571025848389, acc: 0.546875, recall: 0.7384615384615385, precision: 0.5393258426966292, f_beta: 0.6233766233766234\n",
      "train: step: 10, loss: 0.6918832063674927, acc: 0.515625, recall: 0.6470588235294118, precision: 0.5365853658536586, f_beta: 0.5866666666666668\n",
      "train: step: 11, loss: 0.6932427883148193, acc: 0.5078125, recall: 0.6440677966101694, precision: 0.475, f_beta: 0.5467625899280575\n",
      "train: step: 12, loss: 0.6901153326034546, acc: 0.5703125, recall: 0.6056338028169014, precision: 0.6142857142857143, f_beta: 0.6099290780141844\n",
      "train: step: 13, loss: 0.6925752758979797, acc: 0.53125, recall: 0.639344262295082, precision: 0.5064935064935064, f_beta: 0.5652173913043477\n",
      "train: step: 14, loss: 0.7151957750320435, acc: 0.4609375, recall: 0.48333333333333334, precision: 0.43283582089552236, f_beta: 0.45669291338582674\n",
      "train: step: 15, loss: 0.6876934766769409, acc: 0.546875, recall: 0.6451612903225806, precision: 0.5263157894736842, f_beta: 0.5797101449275363\n",
      "train: step: 16, loss: 0.6883262395858765, acc: 0.5625, recall: 0.640625, precision: 0.5540540540540541, f_beta: 0.5942028985507247\n",
      "train: step: 17, loss: 0.6853835582733154, acc: 0.5390625, recall: 0.8703703703703703, precision: 0.47474747474747475, f_beta: 0.6143790849673203\n",
      "train: step: 18, loss: 0.697029173374176, acc: 0.4921875, recall: 0.6129032258064516, precision: 0.4810126582278481, f_beta: 0.5390070921985816\n",
      "train: step: 19, loss: 0.6955094337463379, acc: 0.5078125, recall: 0.6984126984126984, precision: 0.5, f_beta: 0.5827814569536424\n",
      "train: step: 20, loss: 0.6954706907272339, acc: 0.484375, recall: 0.6557377049180327, precision: 0.47058823529411764, f_beta: 0.547945205479452\n",
      "train: step: 21, loss: 0.6928020715713501, acc: 0.515625, recall: 0.734375, precision: 0.5108695652173914, f_beta: 0.6025641025641026\n",
      "train: step: 22, loss: 0.6946430802345276, acc: 0.5234375, recall: 0.8181818181818182, precision: 0.5242718446601942, f_beta: 0.6390532544378699\n",
      "train: step: 23, loss: 0.6870516538619995, acc: 0.53125, recall: 0.765625, precision: 0.5212765957446809, f_beta: 0.620253164556962\n",
      "train: step: 24, loss: 0.6904032230377197, acc: 0.484375, recall: 0.7424242424242424, precision: 0.5, f_beta: 0.5975609756097561\n",
      "train: step: 25, loss: 0.7005080580711365, acc: 0.4609375, recall: 0.6875, precision: 0.4731182795698925, f_beta: 0.5605095541401275\n",
      "train: step: 26, loss: 0.6863225102424622, acc: 0.5390625, recall: 0.7391304347826086, precision: 0.5543478260869565, f_beta: 0.6335403726708074\n",
      "train: step: 27, loss: 0.6968237161636353, acc: 0.53125, recall: 0.7878787878787878, precision: 0.5306122448979592, f_beta: 0.6341463414634148\n",
      "train: step: 28, loss: 0.6894385814666748, acc: 0.46875, recall: 0.6129032258064516, precision: 0.4634146341463415, f_beta: 0.5277777777777778\n",
      "train: step: 29, loss: 0.6857638359069824, acc: 0.546875, recall: 0.8153846153846154, precision: 0.5353535353535354, f_beta: 0.6463414634146342\n",
      "train: step: 30, loss: 0.6847936511039734, acc: 0.53125, recall: 0.6351351351351351, precision: 0.5875, f_beta: 0.6103896103896105\n",
      "train: step: 31, loss: 0.7053803205490112, acc: 0.5625, recall: 0.7878787878787878, precision: 0.5531914893617021, f_beta: 0.65\n",
      "train: step: 32, loss: 0.6973302364349365, acc: 0.5390625, recall: 0.7123287671232876, precision: 0.5777777777777777, f_beta: 0.6380368098159509\n",
      "train: step: 33, loss: 0.6897827386856079, acc: 0.53125, recall: 0.72, precision: 0.5806451612903226, f_beta: 0.6428571428571428\n",
      "train: step: 34, loss: 0.7159941792488098, acc: 0.5078125, recall: 0.8225806451612904, precision: 0.49514563106796117, f_beta: 0.6181818181818183\n",
      "train: step: 35, loss: 0.6852786540985107, acc: 0.5546875, recall: 0.84375, precision: 0.5346534653465347, f_beta: 0.6545454545454545\n",
      "train: step: 36, loss: 0.7033217549324036, acc: 0.4609375, recall: 0.7407407407407407, precision: 0.42105263157894735, f_beta: 0.5369127516778524\n",
      "train: step: 37, loss: 0.6945071220397949, acc: 0.4765625, recall: 0.5094339622641509, precision: 0.39705882352941174, f_beta: 0.44628099173553715\n",
      "train: step: 38, loss: 0.7155840992927551, acc: 0.5078125, recall: 0.4857142857142857, precision: 0.5573770491803278, f_beta: 0.5190839694656487\n",
      "train: step: 39, loss: 0.6977059841156006, acc: 0.5, recall: 0.4, precision: 0.5098039215686274, f_beta: 0.4482758620689655\n",
      "train: step: 40, loss: 0.7145073413848877, acc: 0.515625, recall: 0.3269230769230769, precision: 0.38636363636363635, f_beta: 0.35416666666666663\n",
      "train: step: 41, loss: 0.705267071723938, acc: 0.53125, recall: 0.2878787878787879, precision: 0.59375, f_beta: 0.38775510204081637\n",
      "train: step: 42, loss: 0.7027515172958374, acc: 0.515625, recall: 0.3064516129032258, precision: 0.5, f_beta: 0.38\n",
      "train: step: 43, loss: 0.7035608887672424, acc: 0.421875, recall: 0.288135593220339, precision: 0.3469387755102041, f_beta: 0.31481481481481477\n",
      "train: step: 44, loss: 0.702897310256958, acc: 0.4921875, recall: 0.3968253968253968, precision: 0.4807692307692308, f_beta: 0.43478260869565216\n",
      "train: step: 45, loss: 0.6950392723083496, acc: 0.5390625, recall: 0.4838709677419355, precision: 0.5263157894736842, f_beta: 0.504201680672269\n",
      "train: step: 46, loss: 0.6917400360107422, acc: 0.5546875, recall: 0.46551724137931033, precision: 0.5094339622641509, f_beta: 0.48648648648648646\n",
      "train: step: 47, loss: 0.7016305923461914, acc: 0.453125, recall: 0.27692307692307694, precision: 0.43902439024390244, f_beta: 0.33962264150943394\n",
      "train: step: 48, loss: 0.7008162140846252, acc: 0.484375, recall: 0.18461538461538463, precision: 0.48, f_beta: 0.2666666666666667\n",
      "train: step: 49, loss: 0.7124060392379761, acc: 0.3828125, recall: 0.14473684210526316, precision: 0.44, f_beta: 0.2178217821782178\n",
      "train: step: 50, loss: 0.7062340974807739, acc: 0.421875, recall: 0.15492957746478872, precision: 0.44, f_beta: 0.22916666666666669\n",
      "train: step: 51, loss: 0.6925709247589111, acc: 0.5, recall: 0.4626865671641791, precision: 0.5254237288135594, f_beta: 0.4920634920634921\n",
      "train: step: 52, loss: 0.6955112218856812, acc: 0.4765625, recall: 0.5833333333333334, precision: 0.45454545454545453, f_beta: 0.5109489051094891\n",
      "train: step: 53, loss: 0.6942791938781738, acc: 0.5, recall: 0.7164179104477612, precision: 0.5161290322580645, f_beta: 0.6\n",
      "train: step: 54, loss: 0.6970669031143188, acc: 0.5234375, recall: 0.8615384615384616, precision: 0.5185185185185185, f_beta: 0.6473988439306357\n",
      "train: step: 55, loss: 0.6924517154693604, acc: 0.515625, recall: 0.8181818181818182, precision: 0.5192307692307693, f_beta: 0.6352941176470589\n",
      "train: step: 56, loss: 0.6900672316551208, acc: 0.5078125, recall: 0.8870967741935484, precision: 0.4954954954954955, f_beta: 0.6358381502890174\n",
      "train: step: 57, loss: 0.6873420476913452, acc: 0.5390625, recall: 0.835820895522388, precision: 0.5384615384615384, f_beta: 0.6549707602339181\n",
      "train: step: 58, loss: 0.7173526287078857, acc: 0.4140625, recall: 0.8679245283018868, precision: 0.40350877192982454, f_beta: 0.5508982035928144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 59, loss: 0.6935716867446899, acc: 0.515625, recall: 0.8307692307692308, precision: 0.5142857142857142, f_beta: 0.6352941176470588\n",
      "train: step: 60, loss: 0.6963160634040833, acc: 0.5078125, recall: 0.734375, precision: 0.5053763440860215, f_beta: 0.5987261146496815\n",
      "train: step: 61, loss: 0.7015841007232666, acc: 0.515625, recall: 0.6133333333333333, precision: 0.5822784810126582, f_beta: 0.5974025974025974\n",
      "train: step: 62, loss: 0.69252610206604, acc: 0.46875, recall: 0.7894736842105263, precision: 0.44554455445544555, f_beta: 0.569620253164557\n",
      "train: step: 63, loss: 0.7003044486045837, acc: 0.4453125, recall: 0.6515151515151515, precision: 0.4725274725274725, f_beta: 0.5477707006369427\n",
      "train: step: 64, loss: 0.6912063956260681, acc: 0.5390625, recall: 0.7941176470588235, precision: 0.5454545454545454, f_beta: 0.6467065868263473\n",
      "train: step: 65, loss: 0.7022831439971924, acc: 0.4609375, recall: 0.7868852459016393, precision: 0.46153846153846156, f_beta: 0.5818181818181818\n",
      "train: step: 66, loss: 0.6944853067398071, acc: 0.5625, recall: 0.8405797101449275, precision: 0.5631067961165048, f_beta: 0.6744186046511628\n",
      "train: step: 67, loss: 0.6992270350456238, acc: 0.4609375, recall: 0.7796610169491526, precision: 0.45098039215686275, f_beta: 0.5714285714285714\n",
      "train: step: 68, loss: 0.6912661194801331, acc: 0.4921875, recall: 0.7536231884057971, precision: 0.52, f_beta: 0.6153846153846154\n",
      "train: step: 69, loss: 0.6835639476776123, acc: 0.5546875, recall: 0.7571428571428571, precision: 0.5698924731182796, f_beta: 0.6503067484662578\n",
      "train: step: 70, loss: 0.6810169219970703, acc: 0.53125, recall: 0.7571428571428571, precision: 0.5520833333333334, f_beta: 0.6385542168674698\n",
      "train: step: 71, loss: 0.6910101175308228, acc: 0.4765625, recall: 0.782608695652174, precision: 0.5094339622641509, f_beta: 0.6171428571428571\n",
      "train: step: 72, loss: 0.6991705894470215, acc: 0.4609375, recall: 0.875, precision: 0.44144144144144143, f_beta: 0.5868263473053893\n",
      "train: step: 73, loss: 0.6855195760726929, acc: 0.5078125, recall: 0.746268656716418, precision: 0.5208333333333334, f_beta: 0.6134969325153375\n",
      "train: step: 74, loss: 0.6907258033752441, acc: 0.515625, recall: 0.75, precision: 0.5106382978723404, f_beta: 0.6075949367088608\n",
      "train: step: 75, loss: 0.6900577545166016, acc: 0.546875, recall: 0.6417910447761194, precision: 0.5584415584415584, f_beta: 0.5972222222222222\n",
      "train: step: 76, loss: 0.689041256904602, acc: 0.515625, recall: 0.5692307692307692, precision: 0.5211267605633803, f_beta: 0.5441176470588235\n",
      "train: step: 77, loss: 0.685928225517273, acc: 0.5625, recall: 0.4262295081967213, precision: 0.5531914893617021, f_beta: 0.48148148148148145\n",
      "train: step: 78, loss: 0.6914573907852173, acc: 0.5390625, recall: 0.3387096774193548, precision: 0.5384615384615384, f_beta: 0.41584158415841577\n",
      "train: step: 79, loss: 0.6893315315246582, acc: 0.484375, recall: 0.31666666666666665, precision: 0.4318181818181818, f_beta: 0.36538461538461536\n",
      "train: step: 80, loss: 0.6762559413909912, acc: 0.578125, recall: 0.14545454545454545, precision: 0.5333333333333333, f_beta: 0.2285714285714286\n",
      "train: step: 81, loss: 0.6822795867919922, acc: 0.53125, recall: 0.04838709677419355, precision: 0.75, f_beta: 0.0909090909090909\n",
      "train: step: 82, loss: 0.6881248950958252, acc: 0.5234375, recall: 0.016666666666666666, precision: 0.3333333333333333, f_beta: 0.031746031746031744\n",
      "train: step: 83, loss: 0.6918591856956482, acc: 0.5, recall: 0.046153846153846156, precision: 0.6, f_beta: 0.08571428571428573\n",
      "train: step: 84, loss: 0.6980180740356445, acc: 0.515625, recall: 0.10606060606060606, precision: 0.7, f_beta: 0.18421052631578946\n",
      "train: step: 85, loss: 0.6871322393417358, acc: 0.5546875, recall: 0.1896551724137931, precision: 0.5238095238095238, f_beta: 0.2784810126582279\n",
      "train: step: 86, loss: 0.6938867568969727, acc: 0.5390625, recall: 0.35135135135135137, precision: 0.7027027027027027, f_beta: 0.46846846846846846\n",
      "train: step: 87, loss: 0.7095136642456055, acc: 0.5, recall: 0.32727272727272727, precision: 0.4, f_beta: 0.36000000000000004\n",
      "train: step: 88, loss: 0.7047784328460693, acc: 0.46875, recall: 0.38461538461538464, precision: 0.4716981132075472, f_beta: 0.423728813559322\n",
      "train: step: 89, loss: 0.71925288438797, acc: 0.484375, recall: 0.423728813559322, precision: 0.43859649122807015, f_beta: 0.43103448275862066\n",
      "train: step: 90, loss: 0.705082893371582, acc: 0.5, recall: 0.375, precision: 0.42, f_beta: 0.39622641509433965\n",
      "train: step: 91, loss: 0.701264500617981, acc: 0.484375, recall: 0.2711864406779661, precision: 0.41025641025641024, f_beta: 0.32653061224489793\n",
      "train: step: 92, loss: 0.6808196306228638, acc: 0.5703125, recall: 0.1746031746031746, precision: 0.7857142857142857, f_beta: 0.2857142857142857\n",
      "train: step: 93, loss: 0.7211850881576538, acc: 0.484375, recall: 0.0625, precision: 0.4, f_beta: 0.10810810810810811\n",
      "train: step: 94, loss: 0.6855109333992004, acc: 0.5859375, recall: 0.05660377358490566, precision: 0.5, f_beta: 0.1016949152542373\n",
      "train: step: 95, loss: 0.6790899038314819, acc: 0.515625, recall: 0.0, precision: 0.0, f_beta: 0\n",
      "train: step: 96, loss: 0.6958793997764587, acc: 0.484375, recall: 0.04411764705882353, precision: 0.75, f_beta: 0.08333333333333334\n",
      "train: step: 97, loss: 0.6984421610832214, acc: 0.484375, recall: 0.08571428571428572, precision: 0.75, f_beta: 0.15384615384615383\n",
      "train: step: 98, loss: 0.7008258104324341, acc: 0.4609375, recall: 0.12857142857142856, precision: 0.5294117647058824, f_beta: 0.20689655172413793\n",
      "train: step: 99, loss: 0.6857945919036865, acc: 0.5, recall: 0.24242424242424243, precision: 0.5333333333333333, f_beta: 0.3333333333333333\n",
      "train: step: 100, loss: 0.6749156713485718, acc: 0.6015625, recall: 0.7377049180327869, precision: 0.5625, f_beta: 0.6382978723404255\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T14:29:57.434324, step: 100, loss: 0.6859108622257526, acc: 0.5194310897435898,precision: 0.9566312717865986, recall: 0.5132518520433277, f_beta: 0.6666712800875961\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-100\n",
      "\n",
      "train: step: 101, loss: 0.6810342073440552, acc: 0.5390625, recall: 0.8260869565217391, precision: 0.5480769230769231, f_beta: 0.6589595375722545\n",
      "train: step: 102, loss: 0.6868460178375244, acc: 0.46875, recall: 0.9642857142857143, precision: 0.45, f_beta: 0.6136363636363636\n",
      "train: step: 103, loss: 0.7183710336685181, acc: 0.4375, recall: 0.9272727272727272, precision: 0.42857142857142855, f_beta: 0.5862068965517241\n",
      "train: step: 104, loss: 0.7007530331611633, acc: 0.5, recall: 0.9047619047619048, precision: 0.4956521739130435, f_beta: 0.6404494382022472\n",
      "train: step: 105, loss: 0.6798869371414185, acc: 0.546875, recall: 0.8970588235294118, precision: 0.5446428571428571, f_beta: 0.6777777777777778\n",
      "train: step: 106, loss: 0.698805570602417, acc: 0.4609375, recall: 0.7586206896551724, precision: 0.4444444444444444, f_beta: 0.5605095541401274\n",
      "train: step: 107, loss: 0.6766812801361084, acc: 0.546875, recall: 0.6857142857142857, precision: 0.5714285714285714, f_beta: 0.6233766233766234\n",
      "train: step: 108, loss: 0.683121919631958, acc: 0.5703125, recall: 0.6031746031746031, precision: 0.5588235294117647, f_beta: 0.5801526717557252\n",
      "train: step: 109, loss: 0.6802210807800293, acc: 0.546875, recall: 0.4032258064516129, precision: 0.5434782608695652, f_beta: 0.4629629629629629\n",
      "train: step: 110, loss: 0.6926480531692505, acc: 0.53125, recall: 0.26865671641791045, precision: 0.6206896551724138, f_beta: 0.37500000000000006\n",
      "train: step: 111, loss: 0.6727344989776611, acc: 0.625, recall: 0.36363636363636365, precision: 0.6060606060606061, f_beta: 0.45454545454545453\n",
      "train: step: 112, loss: 0.6687403917312622, acc: 0.5703125, recall: 0.1, precision: 0.8571428571428571, f_beta: 0.17910447761194032\n",
      "train: step: 113, loss: 0.6769936084747314, acc: 0.53125, recall: 0.13846153846153847, precision: 0.6923076923076923, f_beta: 0.23076923076923078\n",
      "train: step: 114, loss: 0.6974261403083801, acc: 0.5234375, recall: 0.14925373134328357, precision: 0.7142857142857143, f_beta: 0.24691358024691357\n",
      "train: step: 115, loss: 0.6593438386917114, acc: 0.625, recall: 0.3384615384615385, precision: 0.8148148148148148, f_beta: 0.4782608695652174\n",
      "train: step: 116, loss: 0.6700160503387451, acc: 0.6171875, recall: 0.6164383561643836, precision: 0.6818181818181818, f_beta: 0.6474820143884891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 117, loss: 0.6259950399398804, acc: 0.6640625, recall: 0.9393939393939394, precision: 0.6138613861386139, f_beta: 0.7425149700598802\n",
      "train: step: 118, loss: 0.5430752635002136, acc: 0.796875, recall: 0.7285714285714285, precision: 0.8793103448275862, f_beta: 0.796875\n",
      "train: step: 119, loss: 0.486763060092926, acc: 0.8203125, recall: 0.8888888888888888, precision: 0.810126582278481, f_beta: 0.847682119205298\n",
      "train: step: 120, loss: 0.6891999244689941, acc: 0.671875, recall: 0.7538461538461538, precision: 0.6533333333333333, f_beta: 0.7\n",
      "train: step: 121, loss: 0.553303599357605, acc: 0.75, recall: 0.5357142857142857, precision: 0.8333333333333334, f_beta: 0.6521739130434783\n",
      "train: step: 122, loss: 0.664178729057312, acc: 0.625, recall: 0.24193548387096775, precision: 0.9375, f_beta: 0.3846153846153846\n",
      "train: step: 123, loss: 0.6045092344284058, acc: 0.65625, recall: 0.390625, precision: 0.8333333333333334, f_beta: 0.5319148936170213\n",
      "train: step: 124, loss: 0.627227783203125, acc: 0.5625, recall: 0.6065573770491803, precision: 0.5362318840579711, f_beta: 0.5692307692307692\n",
      "train: step: 125, loss: 0.5904281139373779, acc: 0.6328125, recall: 0.9014084507042254, precision: 0.6153846153846154, f_beta: 0.7314285714285714\n",
      "train: step: 126, loss: 0.98443204164505, acc: 0.5078125, recall: 1.0, precision: 0.475, f_beta: 0.6440677966101694\n",
      "train: step: 127, loss: 0.7927674055099487, acc: 0.5390625, recall: 1.0, precision: 0.5317460317460317, f_beta: 0.694300518134715\n",
      "train: step: 128, loss: 0.7370861768722534, acc: 0.5, recall: 0.5217391304347826, precision: 0.5373134328358209, f_beta: 0.5294117647058824\n",
      "train: step: 129, loss: 0.7069644927978516, acc: 0.453125, recall: 0.4666666666666667, precision: 0.42424242424242425, f_beta: 0.4444444444444445\n",
      "train: step: 130, loss: 0.6837132573127747, acc: 0.546875, recall: 0.7014925373134329, precision: 0.5529411764705883, f_beta: 0.618421052631579\n",
      "train: step: 131, loss: 0.6857038736343384, acc: 0.5546875, recall: 0.7538461538461538, precision: 0.5444444444444444, f_beta: 0.6322580645161291\n",
      "train: step: 132, loss: 0.6968611478805542, acc: 0.5, recall: 0.7454545454545455, precision: 0.45054945054945056, f_beta: 0.5616438356164384\n",
      "train: step: 133, loss: 0.6598753929138184, acc: 0.5625, recall: 0.5263157894736842, precision: 0.5084745762711864, f_beta: 0.5172413793103449\n",
      "train: step: 134, loss: 0.7068232297897339, acc: 0.5078125, recall: 0.4225352112676056, precision: 0.5769230769230769, f_beta: 0.4878048780487805\n",
      "train: step: 135, loss: 0.6869432926177979, acc: 0.5546875, recall: 0.39344262295081966, precision: 0.5454545454545454, f_beta: 0.45714285714285713\n",
      "train: step: 136, loss: 0.6758406162261963, acc: 0.5234375, recall: 0.39285714285714285, precision: 0.4489795918367347, f_beta: 0.419047619047619\n",
      "train: step: 137, loss: 0.6874344348907471, acc: 0.484375, recall: 0.3064516129032258, precision: 0.4523809523809524, f_beta: 0.36538461538461536\n",
      "train: step: 138, loss: 0.6754213571548462, acc: 0.5234375, recall: 0.32727272727272727, precision: 0.42857142857142855, f_beta: 0.3711340206185567\n",
      "train: step: 139, loss: 0.6861310005187988, acc: 0.5390625, recall: 0.46153846153846156, precision: 0.5555555555555556, f_beta: 0.504201680672269\n",
      "train: step: 140, loss: 0.6703999042510986, acc: 0.578125, recall: 0.546875, precision: 0.5833333333333334, f_beta: 0.564516129032258\n",
      "train: step: 141, loss: 0.6519002914428711, acc: 0.578125, recall: 0.6231884057971014, precision: 0.6056338028169014, f_beta: 0.6142857142857142\n",
      "train: step: 142, loss: 0.6836397647857666, acc: 0.578125, recall: 0.6451612903225806, precision: 0.5555555555555556, f_beta: 0.5970149253731343\n",
      "train: step: 143, loss: 0.6731778383255005, acc: 0.5546875, recall: 0.765625, precision: 0.5384615384615384, f_beta: 0.6322580645161291\n",
      "train: step: 144, loss: 0.638093113899231, acc: 0.59375, recall: 0.7966101694915254, precision: 0.5402298850574713, f_beta: 0.6438356164383562\n",
      "train: step: 145, loss: 0.6558637619018555, acc: 0.625, recall: 0.8571428571428571, precision: 0.6122448979591837, f_beta: 0.7142857142857143\n",
      "train: step: 146, loss: 0.6639895439147949, acc: 0.5859375, recall: 0.8253968253968254, precision: 0.5531914893617021, f_beta: 0.6624203821656052\n",
      "train: step: 147, loss: 0.5979444980621338, acc: 0.7109375, recall: 0.7384615384615385, precision: 0.7058823529411765, f_beta: 0.7218045112781954\n",
      "train: step: 148, loss: 0.5990018844604492, acc: 0.71875, recall: 0.7671232876712328, precision: 0.7466666666666667, f_beta: 0.7567567567567568\n",
      "train: step: 149, loss: 0.5172579288482666, acc: 0.7890625, recall: 0.8245614035087719, precision: 0.734375, f_beta: 0.7768595041322315\n",
      "train: step: 150, loss: 0.5087320804595947, acc: 0.796875, recall: 0.8260869565217391, precision: 0.8028169014084507, f_beta: 0.8142857142857144\n",
      "train: step: 151, loss: 0.4937625229358673, acc: 0.8125, recall: 0.7794117647058824, precision: 0.8548387096774194, f_beta: 0.8153846153846154\n",
      "train: step: 152, loss: 0.5482008457183838, acc: 0.75, recall: 0.819672131147541, precision: 0.704225352112676, f_beta: 0.7575757575757576\n",
      "train: step: 153, loss: 0.5602419376373291, acc: 0.765625, recall: 0.7543859649122807, precision: 0.7288135593220338, f_beta: 0.7413793103448276\n",
      "train: step: 154, loss: 0.44952720403671265, acc: 0.8203125, recall: 0.8507462686567164, precision: 0.8142857142857143, f_beta: 0.832116788321168\n",
      "train: step: 155, loss: 0.6298488974571228, acc: 0.71875, recall: 0.7058823529411765, precision: 0.75, f_beta: 0.7272727272727272\n",
      "train: step: 156, loss: 0.5282696485519409, acc: 0.7890625, recall: 0.8412698412698413, precision: 0.7571428571428571, f_beta: 0.7969924812030075\n",
      "start training model\n",
      "train: step: 157, loss: 0.54579758644104, acc: 0.78125, recall: 0.875, precision: 0.7, f_beta: 0.7777777777777777\n",
      "train: step: 158, loss: 0.4522649645805359, acc: 0.8359375, recall: 0.8805970149253731, precision: 0.8194444444444444, f_beta: 0.8489208633093526\n",
      "train: step: 159, loss: 0.4906104803085327, acc: 0.78125, recall: 0.7741935483870968, precision: 0.7741935483870968, f_beta: 0.7741935483870968\n",
      "train: step: 160, loss: 0.5692566633224487, acc: 0.765625, recall: 0.6527777777777778, precision: 0.9038461538461539, f_beta: 0.7580645161290323\n",
      "train: step: 161, loss: 0.6000068783760071, acc: 0.734375, recall: 0.5166666666666667, precision: 0.8611111111111112, f_beta: 0.6458333333333335\n",
      "train: step: 162, loss: 0.49849382042884827, acc: 0.7734375, recall: 0.660377358490566, precision: 0.7608695652173914, f_beta: 0.7070707070707071\n",
      "train: step: 163, loss: 0.5072177648544312, acc: 0.78125, recall: 0.704225352112676, precision: 0.8771929824561403, f_beta: 0.7812499999999999\n",
      "train: step: 164, loss: 0.5151925683021545, acc: 0.7734375, recall: 0.5737704918032787, precision: 0.9210526315789473, f_beta: 0.7070707070707071\n",
      "train: step: 165, loss: 0.5281103849411011, acc: 0.7734375, recall: 0.6212121212121212, precision: 0.9111111111111111, f_beta: 0.7387387387387386\n",
      "train: step: 166, loss: 0.5488137006759644, acc: 0.7734375, recall: 0.6557377049180327, precision: 0.8333333333333334, f_beta: 0.7339449541284403\n",
      "train: step: 167, loss: 0.48675045371055603, acc: 0.8046875, recall: 0.7936507936507936, precision: 0.8064516129032258, f_beta: 0.7999999999999999\n",
      "train: step: 168, loss: 0.4540509581565857, acc: 0.8359375, recall: 0.8450704225352113, precision: 0.8571428571428571, f_beta: 0.851063829787234\n",
      "train: step: 169, loss: 0.5354893207550049, acc: 0.7890625, recall: 0.7808219178082192, precision: 0.8382352941176471, f_beta: 0.8085106382978724\n",
      "train: step: 170, loss: 0.5469394326210022, acc: 0.78125, recall: 0.8703703703703703, precision: 0.6911764705882353, f_beta: 0.7704918032786884\n",
      "train: step: 171, loss: 0.6188785433769226, acc: 0.6953125, recall: 0.5294117647058824, precision: 0.8372093023255814, f_beta: 0.6486486486486487\n",
      "train: step: 172, loss: 0.6293923854827881, acc: 0.6875, recall: 0.3620689655172414, precision: 0.875, f_beta: 0.5121951219512195\n",
      "train: step: 173, loss: 0.7905091047286987, acc: 0.5703125, recall: 0.19117647058823528, precision: 1.0, f_beta: 0.32098765432098764\n",
      "train: step: 174, loss: 0.8028188943862915, acc: 0.53125, recall: 0.09090909090909091, precision: 1.0, f_beta: 0.16666666666666669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 175, loss: 0.7383542656898499, acc: 0.5, recall: 0.03225806451612903, precision: 0.3333333333333333, f_beta: 0.0588235294117647\n",
      "train: step: 176, loss: 0.6970446109771729, acc: 0.5390625, recall: 0.2923076923076923, precision: 0.59375, f_beta: 0.3917525773195876\n",
      "train: step: 177, loss: 0.7181582450866699, acc: 0.5234375, recall: 0.6545454545454545, precision: 0.46153846153846156, f_beta: 0.5413533834586467\n",
      "train: step: 178, loss: 0.6923552751541138, acc: 0.5390625, recall: 0.75, precision: 0.5274725274725275, f_beta: 0.6193548387096776\n",
      "train: step: 179, loss: 0.7477456331253052, acc: 0.46875, recall: 0.6612903225806451, precision: 0.4659090909090909, f_beta: 0.5466666666666665\n",
      "train: step: 180, loss: 0.7157608270645142, acc: 0.515625, recall: 0.7777777777777778, precision: 0.5051546391752577, f_beta: 0.6125\n",
      "train: step: 181, loss: 0.7165135145187378, acc: 0.4609375, recall: 0.8, precision: 0.43137254901960786, f_beta: 0.5605095541401275\n",
      "train: step: 182, loss: 0.6704245805740356, acc: 0.5546875, recall: 0.7534246575342466, precision: 0.5851063829787234, f_beta: 0.658682634730539\n",
      "train: step: 183, loss: 0.6740903854370117, acc: 0.53125, recall: 0.5161290322580645, precision: 0.5161290322580645, f_beta: 0.5161290322580645\n",
      "train: step: 184, loss: 0.6760289669036865, acc: 0.6015625, recall: 0.41304347826086957, precision: 0.4418604651162791, f_beta: 0.4269662921348315\n",
      "train: step: 185, loss: 0.6962470412254333, acc: 0.5234375, recall: 0.27941176470588236, precision: 0.6129032258064516, f_beta: 0.38383838383838387\n",
      "train: step: 186, loss: 0.715769350528717, acc: 0.5078125, recall: 0.15384615384615385, precision: 0.5555555555555556, f_beta: 0.24096385542168675\n",
      "train: step: 187, loss: 0.6912604570388794, acc: 0.5546875, recall: 0.2857142857142857, precision: 0.7407407407407407, f_beta: 0.41237113402061853\n",
      "train: step: 188, loss: 0.6826565265655518, acc: 0.59375, recall: 0.3050847457627119, precision: 0.6206896551724138, f_beta: 0.40909090909090917\n",
      "train: step: 189, loss: 0.688220202922821, acc: 0.5625, recall: 0.36486486486486486, precision: 0.75, f_beta: 0.49090909090909085\n",
      "train: step: 190, loss: 0.6673060655593872, acc: 0.5859375, recall: 0.4090909090909091, precision: 0.6585365853658537, f_beta: 0.5046728971962617\n",
      "train: step: 191, loss: 0.629473090171814, acc: 0.6015625, recall: 0.6206896551724138, precision: 0.5538461538461539, f_beta: 0.5853658536585366\n",
      "train: step: 192, loss: 0.6606309413909912, acc: 0.625, recall: 0.6338028169014085, precision: 0.6716417910447762, f_beta: 0.6521739130434784\n",
      "train: step: 193, loss: 0.6628518104553223, acc: 0.59375, recall: 0.746031746031746, precision: 0.5662650602409639, f_beta: 0.6438356164383563\n",
      "train: step: 194, loss: 0.6262452602386475, acc: 0.5546875, recall: 0.7605633802816901, precision: 0.574468085106383, f_beta: 0.6545454545454545\n",
      "train: step: 195, loss: 0.6289457082748413, acc: 0.609375, recall: 0.881578947368421, precision: 0.6203703703703703, f_beta: 0.7282608695652174\n",
      "train: step: 196, loss: 0.6667860746383667, acc: 0.546875, recall: 0.9830508474576272, precision: 0.5043478260869565, f_beta: 0.6666666666666667\n",
      "train: step: 197, loss: 0.617803692817688, acc: 0.6328125, recall: 0.9857142857142858, precision: 0.6, f_beta: 0.7459459459459459\n",
      "train: step: 198, loss: 0.5579992532730103, acc: 0.640625, recall: 0.96875, precision: 0.5849056603773585, f_beta: 0.7294117647058823\n",
      "train: step: 199, loss: 0.5184533596038818, acc: 0.7734375, recall: 0.8840579710144928, precision: 0.7439024390243902, f_beta: 0.8079470198675496\n",
      "train: step: 200, loss: 0.5136901140213013, acc: 0.828125, recall: 0.8412698412698413, precision: 0.8153846153846154, f_beta: 0.8281250000000001\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T14:33:48.965040, step: 200, loss: 0.5403921122734363, acc: 0.7606169871794872,precision: 0.6475877809094163, recall: 0.8410243194980782, f_beta: 0.7303409292670165\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-200\n",
      "\n",
      "train: step: 201, loss: 0.5487193465232849, acc: 0.7578125, recall: 0.7121212121212122, precision: 0.7966101694915254, f_beta: 0.7520000000000001\n",
      "train: step: 202, loss: 0.549053966999054, acc: 0.7578125, recall: 0.6290322580645161, precision: 0.8297872340425532, f_beta: 0.7155963302752293\n",
      "train: step: 203, loss: 0.48104920983314514, acc: 0.7890625, recall: 0.6857142857142857, precision: 0.9056603773584906, f_beta: 0.7804878048780488\n",
      "train: step: 204, loss: 0.56168532371521, acc: 0.7109375, recall: 0.5633802816901409, precision: 0.8695652173913043, f_beta: 0.6837606837606838\n",
      "train: step: 205, loss: 0.5441250205039978, acc: 0.7578125, recall: 0.65625, precision: 0.8235294117647058, f_beta: 0.7304347826086957\n",
      "train: step: 206, loss: 0.5792970657348633, acc: 0.765625, recall: 0.7543859649122807, precision: 0.7288135593220338, f_beta: 0.7413793103448276\n",
      "train: step: 207, loss: 0.5172549486160278, acc: 0.78125, recall: 0.7777777777777778, precision: 0.7777777777777778, f_beta: 0.7777777777777778\n",
      "train: step: 208, loss: 0.6515861749649048, acc: 0.7109375, recall: 0.6909090909090909, precision: 0.6551724137931034, f_beta: 0.6725663716814159\n",
      "train: step: 209, loss: 0.5516665577888489, acc: 0.7421875, recall: 0.7096774193548387, precision: 0.7457627118644068, f_beta: 0.7272727272727273\n",
      "train: step: 210, loss: 0.4963563084602356, acc: 0.796875, recall: 0.7213114754098361, precision: 0.8301886792452831, f_beta: 0.7719298245614035\n",
      "train: step: 211, loss: 0.5481472015380859, acc: 0.765625, recall: 0.6883116883116883, precision: 0.8983050847457628, f_beta: 0.7794117647058822\n",
      "train: step: 212, loss: 0.6256967782974243, acc: 0.7265625, recall: 0.6268656716417911, precision: 0.8076923076923077, f_beta: 0.7058823529411765\n",
      "train: step: 213, loss: 0.49605417251586914, acc: 0.8046875, recall: 0.7142857142857143, precision: 0.9090909090909091, f_beta: 0.8\n",
      "train: step: 214, loss: 0.4934884309768677, acc: 0.8046875, recall: 0.78125, precision: 0.819672131147541, f_beta: 0.8\n",
      "train: step: 215, loss: 0.46713969111442566, acc: 0.828125, recall: 0.8269230769230769, precision: 0.7678571428571429, f_beta: 0.7962962962962962\n",
      "train: step: 216, loss: 0.5180450677871704, acc: 0.796875, recall: 0.6935483870967742, precision: 0.86, f_beta: 0.7678571428571428\n",
      "train: step: 217, loss: 0.45782721042633057, acc: 0.828125, recall: 0.7575757575757576, precision: 0.8928571428571429, f_beta: 0.819672131147541\n",
      "train: step: 218, loss: 0.5618933439254761, acc: 0.734375, recall: 0.6440677966101694, precision: 0.7450980392156863, f_beta: 0.6909090909090908\n",
      "train: step: 219, loss: 0.4995890259742737, acc: 0.828125, recall: 0.782608695652174, precision: 0.8852459016393442, f_beta: 0.8307692307692308\n",
      "train: step: 220, loss: 0.5580359101295471, acc: 0.78125, recall: 0.7833333333333333, precision: 0.7580645161290323, f_beta: 0.7704918032786884\n",
      "train: step: 221, loss: 0.5585033893585205, acc: 0.734375, recall: 0.7796610169491526, precision: 0.6865671641791045, f_beta: 0.7301587301587301\n",
      "train: step: 222, loss: 0.47675588726997375, acc: 0.8203125, recall: 0.88, precision: 0.825, f_beta: 0.8516129032258064\n",
      "train: step: 223, loss: 0.46122342348098755, acc: 0.84375, recall: 0.9552238805970149, precision: 0.7901234567901234, f_beta: 0.8648648648648647\n",
      "train: step: 224, loss: 0.47482192516326904, acc: 0.84375, recall: 0.9154929577464789, precision: 0.8227848101265823, f_beta: 0.8666666666666667\n",
      "train: step: 225, loss: 0.5306364297866821, acc: 0.7578125, recall: 0.7719298245614035, precision: 0.7096774193548387, f_beta: 0.7394957983193277\n",
      "train: step: 226, loss: 0.5428237915039062, acc: 0.7578125, recall: 0.6, precision: 0.8372093023255814, f_beta: 0.6990291262135923\n",
      "train: step: 227, loss: 0.5808681845664978, acc: 0.7265625, recall: 0.53125, precision: 0.8717948717948718, f_beta: 0.6601941747572816\n",
      "train: step: 228, loss: 0.5768965482711792, acc: 0.7265625, recall: 0.46774193548387094, precision: 0.9354838709677419, f_beta: 0.6236559139784946\n",
      "train: step: 229, loss: 0.7031976580619812, acc: 0.65625, recall: 0.373134328358209, precision: 0.9259259259259259, f_beta: 0.5319148936170213\n",
      "train: step: 230, loss: 0.6398710012435913, acc: 0.671875, recall: 0.36507936507936506, precision: 0.92, f_beta: 0.5227272727272727\n",
      "train: step: 231, loss: 0.5875650644302368, acc: 0.6953125, recall: 0.42424242424242425, precision: 0.9655172413793104, f_beta: 0.5894736842105264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 232, loss: 0.6113072633743286, acc: 0.6875, recall: 0.3559322033898305, precision: 0.9130434782608695, f_beta: 0.5121951219512195\n",
      "train: step: 233, loss: 0.656326413154602, acc: 0.6171875, recall: 0.36486486486486486, precision: 0.9310344827586207, f_beta: 0.5242718446601942\n",
      "train: step: 234, loss: 0.5225404500961304, acc: 0.7734375, recall: 0.5423728813559322, precision: 0.9411764705882353, f_beta: 0.6881720430107526\n",
      "train: step: 235, loss: 0.598054051399231, acc: 0.6328125, recall: 0.35714285714285715, precision: 0.9259259259259259, f_beta: 0.5154639175257733\n",
      "train: step: 236, loss: 0.5421816110610962, acc: 0.7421875, recall: 0.47368421052631576, precision: 0.9, f_beta: 0.6206896551724138\n",
      "train: step: 237, loss: 0.4926144480705261, acc: 0.7421875, recall: 0.5797101449275363, precision: 0.9090909090909091, f_beta: 0.7079646017699115\n",
      "train: step: 238, loss: 0.5517706871032715, acc: 0.7578125, recall: 0.5925925925925926, precision: 0.7804878048780488, f_beta: 0.6736842105263158\n",
      "train: step: 239, loss: 0.5440772771835327, acc: 0.75, recall: 0.5806451612903226, precision: 0.8571428571428571, f_beta: 0.6923076923076923\n",
      "train: step: 240, loss: 0.5700759887695312, acc: 0.703125, recall: 0.6818181818181818, precision: 0.7258064516129032, f_beta: 0.703125\n",
      "train: step: 241, loss: 0.5135307312011719, acc: 0.7734375, recall: 0.7162162162162162, precision: 0.8688524590163934, f_beta: 0.7851851851851852\n",
      "train: step: 242, loss: 0.5240889191627502, acc: 0.765625, recall: 0.6229508196721312, precision: 0.8444444444444444, f_beta: 0.7169811320754719\n",
      "train: step: 243, loss: 0.48131030797958374, acc: 0.8359375, recall: 0.7758620689655172, precision: 0.8490566037735849, f_beta: 0.8108108108108107\n",
      "train: step: 244, loss: 0.4922316074371338, acc: 0.7734375, recall: 0.65, precision: 0.8297872340425532, f_beta: 0.7289719626168223\n",
      "train: step: 245, loss: 0.5247753858566284, acc: 0.765625, recall: 0.6060606060606061, precision: 0.9090909090909091, f_beta: 0.7272727272727273\n",
      "train: step: 246, loss: 0.5659531354904175, acc: 0.71875, recall: 0.44642857142857145, precision: 0.8333333333333334, f_beta: 0.5813953488372092\n",
      "train: step: 247, loss: 0.620936930179596, acc: 0.6171875, recall: 0.35714285714285715, precision: 0.8620689655172413, f_beta: 0.5050505050505051\n",
      "train: step: 248, loss: 0.6619528532028198, acc: 0.5703125, recall: 0.1875, precision: 0.8, f_beta: 0.3037974683544304\n",
      "train: step: 249, loss: 0.6388178467750549, acc: 0.59375, recall: 0.22388059701492538, precision: 1.0, f_beta: 0.36585365853658536\n",
      "train: step: 250, loss: 0.671028733253479, acc: 0.5625, recall: 0.23943661971830985, precision: 0.8947368421052632, f_beta: 0.37777777777777777\n",
      "train: step: 251, loss: 0.643053412437439, acc: 0.609375, recall: 0.2898550724637681, precision: 0.9523809523809523, f_beta: 0.4444444444444445\n",
      "train: step: 252, loss: 0.6211593151092529, acc: 0.640625, recall: 0.2786885245901639, precision: 0.8947368421052632, f_beta: 0.425\n",
      "train: step: 253, loss: 0.6178869605064392, acc: 0.6796875, recall: 0.2857142857142857, precision: 0.9411764705882353, f_beta: 0.4383561643835616\n",
      "train: step: 254, loss: 0.6005878448486328, acc: 0.640625, recall: 0.3888888888888889, precision: 0.9333333333333333, f_beta: 0.5490196078431373\n",
      "train: step: 255, loss: 0.6338400840759277, acc: 0.625, recall: 0.3333333333333333, precision: 0.8461538461538461, f_beta: 0.47826086956521735\n",
      "train: step: 256, loss: 0.5781528949737549, acc: 0.703125, recall: 0.5, precision: 0.7894736842105263, f_beta: 0.6122448979591837\n",
      "train: step: 257, loss: 0.6965656280517578, acc: 0.578125, recall: 0.3333333333333333, precision: 0.6363636363636364, f_beta: 0.43749999999999994\n",
      "train: step: 258, loss: 0.5421390533447266, acc: 0.7109375, recall: 0.5538461538461539, precision: 0.8181818181818182, f_beta: 0.6605504587155965\n",
      "train: step: 259, loss: 0.6202743053436279, acc: 0.625, recall: 0.43333333333333335, precision: 0.65, f_beta: 0.5199999999999999\n",
      "train: step: 260, loss: 0.6000461578369141, acc: 0.6640625, recall: 0.5384615384615384, precision: 0.7291666666666666, f_beta: 0.6194690265486725\n",
      "train: step: 261, loss: 0.5775251388549805, acc: 0.6796875, recall: 0.5737704918032787, precision: 0.7, f_beta: 0.6306306306306306\n",
      "train: step: 262, loss: 0.5843974947929382, acc: 0.6796875, recall: 0.4444444444444444, precision: 0.6857142857142857, f_beta: 0.5393258426966292\n",
      "train: step: 263, loss: 0.565232515335083, acc: 0.6875, recall: 0.5333333333333333, precision: 0.7272727272727273, f_beta: 0.6153846153846153\n",
      "train: step: 264, loss: 0.5889360308647156, acc: 0.6484375, recall: 0.44776119402985076, precision: 0.7894736842105263, f_beta: 0.5714285714285714\n",
      "train: step: 265, loss: 0.6268051266670227, acc: 0.6484375, recall: 0.4090909090909091, precision: 0.8181818181818182, f_beta: 0.5454545454545455\n",
      "train: step: 266, loss: 0.5981988310813904, acc: 0.609375, recall: 0.3333333333333333, precision: 0.7857142857142857, f_beta: 0.4680851063829786\n",
      "train: step: 267, loss: 0.6184082627296448, acc: 0.6015625, recall: 0.30434782608695654, precision: 0.875, f_beta: 0.4516129032258065\n",
      "train: step: 268, loss: 0.5789728164672852, acc: 0.7265625, recall: 0.42592592592592593, precision: 0.8518518518518519, f_beta: 0.5679012345679013\n",
      "train: step: 269, loss: 0.576348066329956, acc: 0.6953125, recall: 0.4375, precision: 0.9032258064516129, f_beta: 0.5894736842105263\n",
      "train: step: 270, loss: 0.5812861919403076, acc: 0.6953125, recall: 0.41379310344827586, precision: 0.8275862068965517, f_beta: 0.5517241379310345\n",
      "train: step: 271, loss: 0.5116302967071533, acc: 0.71875, recall: 0.5081967213114754, precision: 0.8378378378378378, f_beta: 0.6326530612244898\n",
      "train: step: 272, loss: 0.6189088821411133, acc: 0.703125, recall: 0.4423076923076923, precision: 0.71875, f_beta: 0.5476190476190477\n",
      "train: step: 273, loss: 0.46720176935195923, acc: 0.765625, recall: 0.5882352941176471, precision: 0.9523809523809523, f_beta: 0.7272727272727274\n",
      "train: step: 274, loss: 0.5116008520126343, acc: 0.75, recall: 0.609375, precision: 0.8478260869565217, f_beta: 0.709090909090909\n",
      "train: step: 275, loss: 0.4929327964782715, acc: 0.71875, recall: 0.5074626865671642, precision: 0.918918918918919, f_beta: 0.6538461538461539\n",
      "train: step: 276, loss: 0.5304443836212158, acc: 0.78125, recall: 0.6612903225806451, precision: 0.8541666666666666, f_beta: 0.7454545454545455\n",
      "train: step: 277, loss: 0.5469841957092285, acc: 0.75, recall: 0.639344262295082, precision: 0.7959183673469388, f_beta: 0.7090909090909091\n",
      "train: step: 278, loss: 0.43313243985176086, acc: 0.7890625, recall: 0.6610169491525424, precision: 0.8478260869565217, f_beta: 0.7428571428571429\n",
      "train: step: 279, loss: 0.4268428087234497, acc: 0.8125, recall: 0.6363636363636364, precision: 0.8974358974358975, f_beta: 0.7446808510638298\n",
      "train: step: 280, loss: 0.5052900314331055, acc: 0.734375, recall: 0.6086956521739131, precision: 0.8571428571428571, f_beta: 0.7118644067796609\n",
      "train: step: 281, loss: 0.562708854675293, acc: 0.703125, recall: 0.5483870967741935, precision: 0.7727272727272727, f_beta: 0.6415094339622641\n",
      "train: step: 282, loss: 0.49433502554893494, acc: 0.84375, recall: 0.75, precision: 0.8478260869565217, f_beta: 0.7959183673469389\n",
      "train: step: 283, loss: 0.4542692005634308, acc: 0.8125, recall: 0.7254901960784313, precision: 0.7872340425531915, f_beta: 0.7551020408163265\n",
      "train: step: 284, loss: 0.5718469619750977, acc: 0.7890625, recall: 0.7846153846153846, precision: 0.796875, f_beta: 0.7906976744186046\n",
      "train: step: 285, loss: 0.6097702980041504, acc: 0.7734375, recall: 0.7619047619047619, precision: 0.7741935483870968, f_beta: 0.768\n",
      "train: step: 286, loss: 0.5637505650520325, acc: 0.7734375, recall: 0.7761194029850746, precision: 0.7878787878787878, f_beta: 0.7819548872180451\n",
      "train: step: 287, loss: 0.5109691023826599, acc: 0.8046875, recall: 0.8769230769230769, precision: 0.7702702702702703, f_beta: 0.8201438848920863\n",
      "train: step: 288, loss: 0.5352522134780884, acc: 0.7890625, recall: 0.8356164383561644, precision: 0.8026315789473685, f_beta: 0.8187919463087249\n",
      "train: step: 289, loss: 0.49144312739372253, acc: 0.796875, recall: 0.896551724137931, precision: 0.7222222222222222, f_beta: 0.7999999999999999\n",
      "train: step: 290, loss: 0.5913205146789551, acc: 0.6875, recall: 0.9180327868852459, precision: 0.6153846153846154, f_beta: 0.7368421052631579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 291, loss: 0.6093322038650513, acc: 0.6640625, recall: 0.92, precision: 0.6509433962264151, f_beta: 0.7624309392265193\n",
      "train: step: 292, loss: 0.6308245658874512, acc: 0.6328125, recall: 0.6885245901639344, precision: 0.6, f_beta: 0.6412213740458015\n",
      "train: step: 293, loss: 0.5876933336257935, acc: 0.6328125, recall: 0.8181818181818182, precision: 0.5487804878048781, f_beta: 0.656934306569343\n",
      "train: step: 294, loss: 0.6421159505844116, acc: 0.5703125, recall: 0.7580645161290323, precision: 0.5402298850574713, f_beta: 0.6308724832214766\n",
      "train: step: 295, loss: 0.5596605539321899, acc: 0.609375, recall: 0.7936507936507936, precision: 0.5747126436781609, f_beta: 0.6666666666666666\n",
      "train: step: 296, loss: 0.6162463426589966, acc: 0.5859375, recall: 0.559322033898305, precision: 0.55, f_beta: 0.5546218487394957\n",
      "train: step: 297, loss: 0.657448410987854, acc: 0.5703125, recall: 0.35294117647058826, precision: 0.6857142857142857, f_beta: 0.46601941747572817\n",
      "train: step: 298, loss: 0.5666986107826233, acc: 0.6484375, recall: 0.6307692307692307, precision: 0.6612903225806451, f_beta: 0.6456692913385826\n",
      "train: step: 299, loss: 0.5719020962715149, acc: 0.7265625, recall: 0.8305084745762712, precision: 0.6621621621621622, f_beta: 0.7368421052631579\n",
      "train: step: 300, loss: 0.5591070055961609, acc: 0.78125, recall: 0.7428571428571429, precision: 0.8387096774193549, f_beta: 0.787878787878788\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T14:37:40.603561, step: 300, loss: 0.5333368266240145, acc: 0.8104967948717948,precision: 0.7822770228599908, recall: 0.833125991851252, f_beta: 0.8061588544401241\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-300\n",
      "\n",
      "train: step: 301, loss: 0.526543140411377, acc: 0.828125, recall: 0.765625, precision: 0.875, f_beta: 0.8166666666666667\n",
      "train: step: 302, loss: 0.4322523772716522, acc: 0.890625, recall: 0.8656716417910447, precision: 0.9206349206349206, f_beta: 0.8923076923076922\n",
      "train: step: 303, loss: 0.591469407081604, acc: 0.8046875, recall: 0.7272727272727273, precision: 0.9333333333333333, f_beta: 0.8175182481751825\n",
      "train: step: 304, loss: 0.5514442920684814, acc: 0.7734375, recall: 0.71875, precision: 0.8070175438596491, f_beta: 0.7603305785123967\n",
      "train: step: 305, loss: 0.3834877014160156, acc: 0.8671875, recall: 0.96875, precision: 0.8051948051948052, f_beta: 0.8794326241134752\n",
      "train: step: 306, loss: 0.5282366871833801, acc: 0.7578125, recall: 0.9436619718309859, precision: 0.7127659574468085, f_beta: 0.8121212121212121\n",
      "train: step: 307, loss: 0.6039469242095947, acc: 0.7265625, recall: 0.9861111111111112, precision: 0.6761904761904762, f_beta: 0.8022598870056498\n",
      "train: step: 308, loss: 0.7938382625579834, acc: 0.5859375, recall: 1.0, precision: 0.5391304347826087, f_beta: 0.7005649717514124\n",
      "train: step: 309, loss: 0.8455207347869873, acc: 0.5859375, recall: 1.0, precision: 0.5470085470085471, f_beta: 0.707182320441989\n",
      "train: step: 310, loss: 0.8933559060096741, acc: 0.4921875, recall: 0.9830508474576272, precision: 0.47540983606557374, f_beta: 0.6408839779005525\n",
      "train: step: 311, loss: 0.7506920695304871, acc: 0.5703125, recall: 1.0, precision: 0.5491803278688525, f_beta: 0.7089947089947091\n",
      "train: step: 312, loss: 0.7484740018844604, acc: 0.546875, recall: 0.9848484848484849, precision: 0.5327868852459017, f_beta: 0.6914893617021277\n",
      "start training model\n",
      "train: step: 313, loss: 0.7055167555809021, acc: 0.578125, recall: 0.9864864864864865, precision: 0.5793650793650794, f_beta: 0.7300000000000001\n",
      "train: step: 314, loss: 0.7120009660720825, acc: 0.4921875, recall: 0.9666666666666667, precision: 0.4793388429752066, f_beta: 0.6408839779005524\n",
      "train: step: 315, loss: 0.7099522352218628, acc: 0.4453125, recall: 0.6911764705882353, precision: 0.4845360824742268, f_beta: 0.5696969696969696\n",
      "train: step: 316, loss: 0.6937255859375, acc: 0.484375, recall: 0.5084745762711864, precision: 0.44776119402985076, f_beta: 0.4761904761904762\n",
      "train: step: 317, loss: 0.6937515735626221, acc: 0.4765625, recall: 0.25, precision: 0.45714285714285713, f_beta: 0.32323232323232326\n",
      "train: step: 318, loss: 0.6834740042686462, acc: 0.578125, recall: 0.38333333333333336, precision: 0.575, f_beta: 0.4600000000000001\n",
      "train: step: 319, loss: 0.6860707998275757, acc: 0.578125, recall: 0.16666666666666666, precision: 0.5, f_beta: 0.25\n",
      "train: step: 320, loss: 0.7326574921607971, acc: 0.5, recall: 0.11940298507462686, precision: 0.6153846153846154, f_beta: 0.19999999999999998\n",
      "train: step: 321, loss: 0.6781148910522461, acc: 0.5703125, recall: 0.19672131147540983, precision: 0.6666666666666666, f_beta: 0.30379746835443033\n",
      "train: step: 322, loss: 0.7330187559127808, acc: 0.4453125, recall: 0.09210526315789473, precision: 0.7777777777777778, f_beta: 0.16470588235294117\n",
      "train: step: 323, loss: 0.6769542694091797, acc: 0.5390625, recall: 0.1076923076923077, precision: 0.875, f_beta: 0.1917808219178082\n",
      "train: step: 324, loss: 0.708358883857727, acc: 0.5390625, recall: 0.17647058823529413, precision: 0.8, f_beta: 0.28915662650602414\n",
      "train: step: 325, loss: 0.7019287943840027, acc: 0.46875, recall: 0.15384615384615385, precision: 0.43478260869565216, f_beta: 0.22727272727272724\n",
      "train: step: 326, loss: 0.7014973163604736, acc: 0.5234375, recall: 0.2857142857142857, precision: 0.5294117647058824, f_beta: 0.37113402061855666\n",
      "train: step: 327, loss: 0.6848747134208679, acc: 0.546875, recall: 0.6379310344827587, precision: 0.5, f_beta: 0.5606060606060607\n",
      "train: step: 328, loss: 0.6931059956550598, acc: 0.484375, recall: 0.5441176470588235, precision: 0.5138888888888888, f_beta: 0.5285714285714286\n",
      "train: step: 329, loss: 0.6949896216392517, acc: 0.4453125, recall: 0.6612903225806451, precision: 0.45054945054945056, f_beta: 0.5359477124183006\n",
      "train: step: 330, loss: 0.6784003376960754, acc: 0.5234375, recall: 0.8484848484848485, precision: 0.5233644859813084, f_beta: 0.6473988439306357\n",
      "train: step: 331, loss: 0.6690859794616699, acc: 0.6328125, recall: 0.8571428571428571, precision: 0.6470588235294118, f_beta: 0.7374301675977654\n",
      "train: step: 332, loss: 0.7077531218528748, acc: 0.484375, recall: 0.9122807017543859, precision: 0.46017699115044247, f_beta: 0.6117647058823529\n",
      "train: step: 333, loss: 0.6899508237838745, acc: 0.546875, recall: 0.9571428571428572, precision: 0.5491803278688525, f_beta: 0.6979166666666667\n",
      "train: step: 334, loss: 0.6561120748519897, acc: 0.65625, recall: 0.948051948051948, precision: 0.6460176991150443, f_beta: 0.768421052631579\n",
      "train: step: 335, loss: 0.7063353061676025, acc: 0.5, recall: 0.9655172413793104, precision: 0.4745762711864407, f_beta: 0.6363636363636365\n",
      "train: step: 336, loss: 0.6987943649291992, acc: 0.5078125, recall: 0.9827586206896551, precision: 0.4789915966386555, f_beta: 0.6440677966101696\n",
      "train: step: 337, loss: 0.6677963733673096, acc: 0.6015625, recall: 0.9571428571428572, precision: 0.5826086956521739, f_beta: 0.7243243243243244\n",
      "train: step: 338, loss: 0.6758522987365723, acc: 0.5859375, recall: 0.9420289855072463, precision: 0.5701754385964912, f_beta: 0.7103825136612022\n",
      "train: step: 339, loss: 0.6872397661209106, acc: 0.53125, recall: 0.9516129032258065, precision: 0.5086206896551724, f_beta: 0.6629213483146067\n",
      "train: step: 340, loss: 0.6923891305923462, acc: 0.5234375, recall: 0.9206349206349206, precision: 0.5087719298245614, f_beta: 0.655367231638418\n",
      "train: step: 341, loss: 0.6878144145011902, acc: 0.515625, recall: 0.8392857142857143, precision: 0.47, f_beta: 0.6025641025641026\n",
      "train: step: 342, loss: 0.6900675892829895, acc: 0.5, recall: 0.7213114754098361, precision: 0.4835164835164835, f_beta: 0.5789473684210528\n",
      "train: step: 343, loss: 0.6748588681221008, acc: 0.5625, recall: 0.6865671641791045, precision: 0.5679012345679012, f_beta: 0.6216216216216215\n",
      "train: step: 344, loss: 0.6867573261260986, acc: 0.5234375, recall: 0.5942028985507246, precision: 0.5540540540540541, f_beta: 0.5734265734265734\n",
      "train: step: 345, loss: 0.6828581094741821, acc: 0.4765625, recall: 0.40298507462686567, precision: 0.5, f_beta: 0.44628099173553715\n",
      "train: step: 346, loss: 0.6782279014587402, acc: 0.5703125, recall: 0.5517241379310345, precision: 0.5245901639344263, f_beta: 0.5378151260504203\n",
      "train: step: 347, loss: 0.6932657957077026, acc: 0.546875, recall: 0.43661971830985913, precision: 0.6326530612244898, f_beta: 0.5166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 348, loss: 0.6799864172935486, acc: 0.5390625, recall: 0.40540540540540543, precision: 0.6666666666666666, f_beta: 0.504201680672269\n",
      "train: step: 349, loss: 0.6553460359573364, acc: 0.6328125, recall: 0.5161290322580645, precision: 0.6530612244897959, f_beta: 0.5765765765765767\n",
      "train: step: 350, loss: 0.6900237202644348, acc: 0.5703125, recall: 0.4794520547945205, precision: 0.6730769230769231, f_beta: 0.56\n",
      "train: step: 351, loss: 0.6649883985519409, acc: 0.609375, recall: 0.5303030303030303, precision: 0.6481481481481481, f_beta: 0.5833333333333333\n",
      "train: step: 352, loss: 0.6761205196380615, acc: 0.5234375, recall: 0.5588235294117647, precision: 0.5507246376811594, f_beta: 0.5547445255474452\n",
      "train: step: 353, loss: 0.6783623695373535, acc: 0.53125, recall: 0.671875, precision: 0.524390243902439, f_beta: 0.589041095890411\n",
      "train: step: 354, loss: 0.6782728433609009, acc: 0.4921875, recall: 0.6896551724137931, precision: 0.45977011494252873, f_beta: 0.5517241379310345\n",
      "train: step: 355, loss: 0.6564414501190186, acc: 0.609375, recall: 0.7586206896551724, precision: 0.55, f_beta: 0.6376811594202898\n",
      "train: step: 356, loss: 0.6535587310791016, acc: 0.6015625, recall: 0.7763157894736842, precision: 0.6344086021505376, f_beta: 0.6982248520710058\n",
      "train: step: 357, loss: 0.6746162176132202, acc: 0.578125, recall: 0.6730769230769231, precision: 0.4861111111111111, f_beta: 0.564516129032258\n",
      "train: step: 358, loss: 0.6657880544662476, acc: 0.5546875, recall: 0.6515151515151515, precision: 0.5584415584415584, f_beta: 0.6013986013986015\n",
      "train: step: 359, loss: 0.6676778793334961, acc: 0.5859375, recall: 0.7540983606557377, precision: 0.5476190476190477, f_beta: 0.6344827586206897\n",
      "train: step: 360, loss: 0.6250195503234863, acc: 0.6953125, recall: 0.6885245901639344, precision: 0.6774193548387096, f_beta: 0.6829268292682927\n",
      "train: step: 361, loss: 0.6498046517372131, acc: 0.6875, recall: 0.734375, precision: 0.6714285714285714, f_beta: 0.7014925373134329\n",
      "train: step: 362, loss: 0.654918909072876, acc: 0.5546875, recall: 0.484375, precision: 0.5636363636363636, f_beta: 0.5210084033613446\n",
      "train: step: 363, loss: 0.629576563835144, acc: 0.671875, recall: 0.62, precision: 0.5740740740740741, f_beta: 0.5961538461538461\n",
      "train: step: 364, loss: 0.6401053071022034, acc: 0.6171875, recall: 0.59375, precision: 0.6229508196721312, f_beta: 0.608\n",
      "train: step: 365, loss: 0.6539875864982605, acc: 0.6015625, recall: 0.5471698113207547, precision: 0.5178571428571429, f_beta: 0.5321100917431193\n",
      "train: step: 366, loss: 0.6510778069496155, acc: 0.5859375, recall: 0.631578947368421, precision: 0.5294117647058824, f_beta: 0.576\n",
      "train: step: 367, loss: 0.6568036079406738, acc: 0.5625, recall: 0.4925373134328358, precision: 0.6, f_beta: 0.5409836065573771\n",
      "train: step: 368, loss: 0.630448579788208, acc: 0.640625, recall: 0.5441176470588235, precision: 0.7115384615384616, f_beta: 0.6166666666666666\n",
      "train: step: 369, loss: 0.6239489316940308, acc: 0.6953125, recall: 0.7083333333333334, precision: 0.7391304347826086, f_beta: 0.723404255319149\n",
      "train: step: 370, loss: 0.615659773349762, acc: 0.71875, recall: 0.8484848484848485, precision: 0.6829268292682927, f_beta: 0.7567567567567567\n",
      "train: step: 371, loss: 0.5811949968338013, acc: 0.7265625, recall: 0.8947368421052632, precision: 0.7157894736842105, f_beta: 0.7953216374269005\n",
      "train: step: 372, loss: 0.5824502110481262, acc: 0.7421875, recall: 0.8923076923076924, precision: 0.6904761904761905, f_beta: 0.7785234899328859\n",
      "train: step: 373, loss: 0.5805352330207825, acc: 0.734375, recall: 0.6363636363636364, precision: 0.8076923076923077, f_beta: 0.7118644067796609\n",
      "train: step: 374, loss: 0.6161381602287292, acc: 0.65625, recall: 0.9701492537313433, precision: 0.6074766355140186, f_beta: 0.7471264367816092\n",
      "train: step: 375, loss: 0.6753337383270264, acc: 0.578125, recall: 1.0, precision: 0.5344827586206896, f_beta: 0.6966292134831461\n",
      "train: step: 376, loss: 0.5399852991104126, acc: 0.7734375, recall: 0.9032258064516129, precision: 0.7088607594936709, f_beta: 0.7943262411347518\n",
      "train: step: 377, loss: 0.4973754286766052, acc: 0.7734375, recall: 0.5849056603773585, precision: 0.8157894736842105, f_beta: 0.6813186813186812\n",
      "train: step: 378, loss: 0.6366134285926819, acc: 0.625, recall: 0.3333333333333333, precision: 0.8461538461538461, f_beta: 0.47826086956521735\n",
      "train: step: 379, loss: 0.5494927167892456, acc: 0.7890625, recall: 0.6, precision: 0.9230769230769231, f_beta: 0.7272727272727274\n",
      "train: step: 380, loss: 0.4998328685760498, acc: 0.796875, recall: 0.6619718309859155, precision: 0.9591836734693877, f_beta: 0.7833333333333333\n",
      "train: step: 381, loss: 0.5044022798538208, acc: 0.78125, recall: 0.8095238095238095, precision: 0.7611940298507462, f_beta: 0.7846153846153846\n",
      "train: step: 382, loss: 0.6021183133125305, acc: 0.6953125, recall: 0.421875, precision: 0.9310344827586207, f_beta: 0.5806451612903226\n",
      "train: step: 383, loss: 0.593625545501709, acc: 0.6875, recall: 0.47058823529411764, precision: 0.8888888888888888, f_beta: 0.6153846153846153\n",
      "train: step: 384, loss: 0.5404376983642578, acc: 0.7734375, recall: 0.7538461538461538, precision: 0.7903225806451613, f_beta: 0.7716535433070866\n",
      "train: step: 385, loss: 0.681228518486023, acc: 0.6171875, recall: 0.7, precision: 0.5753424657534246, f_beta: 0.6315789473684211\n",
      "train: step: 386, loss: 0.5470012426376343, acc: 0.71875, recall: 0.8571428571428571, precision: 0.6976744186046512, f_beta: 0.7692307692307693\n",
      "train: step: 387, loss: 0.41849154233932495, acc: 0.8515625, recall: 0.7719298245614035, precision: 0.88, f_beta: 0.822429906542056\n",
      "train: step: 388, loss: 0.514214813709259, acc: 0.7734375, recall: 0.676923076923077, precision: 0.8461538461538461, f_beta: 0.7521367521367521\n",
      "train: step: 389, loss: 0.5616880655288696, acc: 0.71875, recall: 0.5217391304347826, precision: 0.9230769230769231, f_beta: 0.6666666666666667\n",
      "train: step: 390, loss: 0.4511711597442627, acc: 0.8359375, recall: 0.8076923076923077, precision: 0.7924528301886793, f_beta: 0.7999999999999999\n",
      "train: step: 391, loss: 0.6152746677398682, acc: 0.7421875, recall: 0.9661016949152542, precision: 0.6477272727272727, f_beta: 0.7755102040816327\n",
      "train: step: 392, loss: 0.8137720823287964, acc: 0.609375, recall: 0.9629629629629629, precision: 0.52, f_beta: 0.6753246753246753\n",
      "train: step: 393, loss: 0.7352897524833679, acc: 0.6328125, recall: 0.9824561403508771, precision: 0.5490196078431373, f_beta: 0.7044025157232705\n",
      "train: step: 394, loss: 0.7270809412002563, acc: 0.578125, recall: 0.9636363636363636, precision: 0.5047619047619047, f_beta: 0.6625\n",
      "train: step: 395, loss: 0.5513922572135925, acc: 0.6640625, recall: 0.9852941176470589, precision: 0.6146788990825688, f_beta: 0.7570621468926553\n",
      "train: step: 396, loss: 0.6478447914123535, acc: 0.59375, recall: 0.7230769230769231, precision: 0.5802469135802469, f_beta: 0.6438356164383562\n",
      "train: step: 397, loss: 0.6138706207275391, acc: 0.6328125, recall: 0.5, precision: 0.7021276595744681, f_beta: 0.584070796460177\n",
      "train: step: 398, loss: 0.6757742762565613, acc: 0.578125, recall: 0.3333333333333333, precision: 0.6363636363636364, f_beta: 0.43749999999999994\n",
      "train: step: 399, loss: 0.6668238639831543, acc: 0.625, recall: 0.20689655172413793, precision: 0.8571428571428571, f_beta: 0.33333333333333326\n",
      "train: step: 400, loss: 0.8254690170288086, acc: 0.5078125, recall: 0.07575757575757576, precision: 0.7142857142857143, f_beta: 0.136986301369863\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T14:41:29.747904, step: 400, loss: 0.7266492278147967, acc: 0.5516826923076923,precision: 0.14601469059995628, recall: 0.8145814515545405, f_beta: 0.2444927434693357\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-400\n",
      "\n",
      "train: step: 401, loss: 0.7155684232711792, acc: 0.59375, recall: 0.234375, precision: 0.8333333333333334, f_beta: 0.3658536585365853\n",
      "train: step: 402, loss: 0.7937618494033813, acc: 0.4921875, recall: 0.1527777777777778, precision: 0.7333333333333333, f_beta: 0.25287356321839083\n",
      "train: step: 403, loss: 0.6712996959686279, acc: 0.59375, recall: 0.15517241379310345, precision: 0.75, f_beta: 0.2571428571428572\n",
      "train: step: 404, loss: 0.7076668739318848, acc: 0.6328125, recall: 0.4117647058823529, precision: 0.8, f_beta: 0.5436893203883495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 405, loss: 0.5974327921867371, acc: 0.6953125, recall: 0.45, precision: 0.8181818181818182, f_beta: 0.5806451612903226\n",
      "train: step: 406, loss: 0.6187204122543335, acc: 0.6953125, recall: 0.5540540540540541, precision: 0.8723404255319149, f_beta: 0.6776859504132232\n",
      "train: step: 407, loss: 0.627832293510437, acc: 0.6875, recall: 0.5172413793103449, precision: 0.7142857142857143, f_beta: 0.6000000000000001\n",
      "train: step: 408, loss: 0.5751530528068542, acc: 0.7578125, recall: 0.6229508196721312, precision: 0.8260869565217391, f_beta: 0.7102803738317758\n",
      "train: step: 409, loss: 0.6118990182876587, acc: 0.703125, recall: 0.6111111111111112, precision: 0.66, f_beta: 0.6346153846153846\n",
      "train: step: 410, loss: 0.6328854560852051, acc: 0.6640625, recall: 0.6557377049180327, precision: 0.6451612903225806, f_beta: 0.6504065040650406\n",
      "train: step: 411, loss: 0.5944898128509521, acc: 0.703125, recall: 0.676056338028169, precision: 0.7619047619047619, f_beta: 0.7164179104477612\n",
      "train: step: 412, loss: 0.6392657160758972, acc: 0.6171875, recall: 0.4, precision: 0.6486486486486487, f_beta: 0.49484536082474234\n",
      "train: step: 413, loss: 0.6468960046768188, acc: 0.578125, recall: 0.4098360655737705, precision: 0.5813953488372093, f_beta: 0.4807692307692308\n",
      "train: step: 414, loss: 0.657584547996521, acc: 0.609375, recall: 0.3404255319148936, precision: 0.45714285714285713, f_beta: 0.39024390243902435\n",
      "train: step: 415, loss: 0.7170984745025635, acc: 0.4609375, recall: 0.2923076923076923, precision: 0.4523809523809524, f_beta: 0.35514018691588783\n",
      "train: step: 416, loss: 0.7070822715759277, acc: 0.4765625, recall: 0.22807017543859648, precision: 0.3611111111111111, f_beta: 0.27956989247311825\n",
      "train: step: 417, loss: 0.7046822905540466, acc: 0.4921875, recall: 0.3125, precision: 0.4878048780487805, f_beta: 0.38095238095238093\n",
      "train: step: 418, loss: 0.7206436395645142, acc: 0.5078125, recall: 0.3484848484848485, precision: 0.5348837209302325, f_beta: 0.4220183486238532\n",
      "train: step: 419, loss: 0.6956555843353271, acc: 0.5, recall: 0.40789473684210525, precision: 0.62, f_beta: 0.49206349206349215\n",
      "train: step: 420, loss: 0.6911684274673462, acc: 0.5234375, recall: 0.5454545454545454, precision: 0.5373134328358209, f_beta: 0.5413533834586466\n",
      "train: step: 421, loss: 0.7226485013961792, acc: 0.46875, recall: 0.5357142857142857, precision: 0.4166666666666667, f_beta: 0.46875000000000006\n",
      "train: step: 422, loss: 0.6632752418518066, acc: 0.5625, recall: 0.6515151515151515, precision: 0.5657894736842105, f_beta: 0.6056338028169014\n",
      "train: step: 423, loss: 0.6898666620254517, acc: 0.53125, recall: 0.704225352112676, precision: 0.5617977528089888, f_beta: 0.625\n",
      "train: step: 424, loss: 0.6880797147750854, acc: 0.53125, recall: 0.75, precision: 0.5625, f_beta: 0.6428571428571429\n",
      "train: step: 425, loss: 0.6999724507331848, acc: 0.5703125, recall: 0.7297297297297297, precision: 0.6067415730337079, f_beta: 0.6625766871165645\n",
      "train: step: 426, loss: 0.6875805854797363, acc: 0.5625, recall: 0.8421052631578947, precision: 0.5052631578947369, f_beta: 0.631578947368421\n",
      "train: step: 427, loss: 0.7480275630950928, acc: 0.4609375, recall: 0.7678571428571429, precision: 0.43434343434343436, f_beta: 0.5548387096774193\n",
      "train: step: 428, loss: 0.6748722791671753, acc: 0.5703125, recall: 0.7397260273972602, precision: 0.6, f_beta: 0.6625766871165644\n",
      "train: step: 429, loss: 0.7058351039886475, acc: 0.4765625, recall: 0.6986301369863014, precision: 0.53125, f_beta: 0.6035502958579881\n",
      "train: step: 430, loss: 0.6654336452484131, acc: 0.5703125, recall: 0.7910447761194029, precision: 0.5638297872340425, f_beta: 0.6583850931677019\n",
      "train: step: 431, loss: 0.7049307823181152, acc: 0.515625, recall: 0.7368421052631579, precision: 0.47191011235955055, f_beta: 0.5753424657534246\n",
      "train: step: 432, loss: 0.6680012941360474, acc: 0.546875, recall: 0.609375, precision: 0.5416666666666666, f_beta: 0.573529411764706\n",
      "train: step: 433, loss: 0.6964108943939209, acc: 0.5546875, recall: 0.6166666666666667, precision: 0.5211267605633803, f_beta: 0.564885496183206\n",
      "train: step: 434, loss: 0.6995483040809631, acc: 0.453125, recall: 0.5925925925925926, precision: 0.4, f_beta: 0.4776119402985075\n",
      "train: step: 435, loss: 0.7032648324966431, acc: 0.46875, recall: 0.34210526315789475, precision: 0.5909090909090909, f_beta: 0.43333333333333335\n",
      "train: step: 436, loss: 0.6665763854980469, acc: 0.609375, recall: 0.41935483870967744, precision: 0.65, f_beta: 0.5098039215686274\n",
      "train: step: 437, loss: 0.6970701813697815, acc: 0.515625, recall: 0.3559322033898305, precision: 0.4666666666666667, f_beta: 0.4038461538461538\n",
      "train: step: 438, loss: 0.6912289261817932, acc: 0.53125, recall: 0.3125, precision: 0.5555555555555556, f_beta: 0.39999999999999997\n",
      "train: step: 439, loss: 0.6772654056549072, acc: 0.578125, recall: 0.4153846153846154, precision: 0.627906976744186, f_beta: 0.5\n",
      "train: step: 440, loss: 0.6967986822128296, acc: 0.546875, recall: 0.38095238095238093, precision: 0.5581395348837209, f_beta: 0.45283018867924524\n",
      "train: step: 441, loss: 0.7151688933372498, acc: 0.46875, recall: 0.36363636363636365, precision: 0.5957446808510638, f_beta: 0.4516129032258065\n",
      "train: step: 442, loss: 0.6564639806747437, acc: 0.625, recall: 0.5774647887323944, precision: 0.6949152542372882, f_beta: 0.6307692307692307\n",
      "train: step: 443, loss: 0.6727695465087891, acc: 0.578125, recall: 0.6153846153846154, precision: 0.5797101449275363, f_beta: 0.5970149253731344\n",
      "train: step: 444, loss: 0.6774167418479919, acc: 0.5703125, recall: 0.625, precision: 0.5633802816901409, f_beta: 0.5925925925925926\n",
      "train: step: 445, loss: 0.6597179174423218, acc: 0.5546875, recall: 0.7164179104477612, precision: 0.5581395348837209, f_beta: 0.6274509803921569\n",
      "train: step: 446, loss: 0.6845389604568481, acc: 0.578125, recall: 0.75, precision: 0.5357142857142857, f_beta: 0.6250000000000001\n",
      "train: step: 447, loss: 0.6654281616210938, acc: 0.5703125, recall: 0.8615384615384616, precision: 0.5490196078431373, f_beta: 0.6706586826347305\n",
      "train: step: 448, loss: 0.7103074789047241, acc: 0.4921875, recall: 0.8275862068965517, precision: 0.46601941747572817, f_beta: 0.5962732919254659\n",
      "train: step: 449, loss: 0.6550835371017456, acc: 0.6171875, recall: 0.8695652173913043, precision: 0.6, f_beta: 0.7100591715976331\n",
      "train: step: 450, loss: 0.6435196399688721, acc: 0.640625, recall: 0.8235294117647058, precision: 0.6222222222222222, f_beta: 0.7088607594936709\n",
      "train: step: 451, loss: 0.6979207992553711, acc: 0.46875, recall: 0.5892857142857143, precision: 0.4230769230769231, f_beta: 0.49253731343283585\n",
      "train: step: 452, loss: 0.6757073402404785, acc: 0.5234375, recall: 0.49230769230769234, precision: 0.5333333333333333, f_beta: 0.512\n",
      "train: step: 453, loss: 0.6447688341140747, acc: 0.6796875, recall: 0.6842105263157895, precision: 0.6290322580645161, f_beta: 0.6554621848739496\n",
      "train: step: 454, loss: 0.631376326084137, acc: 0.6796875, recall: 0.6875, precision: 0.676923076923077, f_beta: 0.6821705426356589\n",
      "train: step: 455, loss: 0.6587417125701904, acc: 0.578125, recall: 0.6206896551724138, precision: 0.5294117647058824, f_beta: 0.5714285714285715\n",
      "train: step: 456, loss: 0.6225993633270264, acc: 0.6328125, recall: 0.5555555555555556, precision: 0.6481481481481481, f_beta: 0.5982905982905983\n",
      "train: step: 457, loss: 0.6536239385604858, acc: 0.625, recall: 0.31666666666666665, precision: 0.7307692307692307, f_beta: 0.44186046511627913\n",
      "train: step: 458, loss: 0.6730454564094543, acc: 0.5546875, recall: 0.234375, precision: 0.6521739130434783, f_beta: 0.3448275862068965\n",
      "train: step: 459, loss: 0.6877071857452393, acc: 0.5703125, recall: 0.2033898305084746, precision: 0.6, f_beta: 0.3037974683544304\n",
      "train: step: 460, loss: 0.6937093734741211, acc: 0.4765625, recall: 0.14516129032258066, precision: 0.391304347826087, f_beta: 0.21176470588235294\n",
      "train: step: 461, loss: 0.6898802518844604, acc: 0.53125, recall: 0.3064516129032258, precision: 0.5277777777777778, f_beta: 0.3877551020408163\n",
      "train: step: 462, loss: 0.6626765727996826, acc: 0.59375, recall: 0.4, precision: 0.6, f_beta: 0.48\n",
      "train: step: 463, loss: 0.673825740814209, acc: 0.5625, recall: 0.3448275862068966, precision: 0.5263157894736842, f_beta: 0.4166666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 464, loss: 0.6419829726219177, acc: 0.578125, recall: 0.39436619718309857, precision: 0.717948717948718, f_beta: 0.509090909090909\n",
      "train: step: 465, loss: 0.6368380784988403, acc: 0.6484375, recall: 0.676923076923077, precision: 0.6470588235294118, f_beta: 0.6616541353383458\n",
      "train: step: 466, loss: 0.6788122653961182, acc: 0.5625, recall: 0.6901408450704225, precision: 0.5903614457831325, f_beta: 0.6363636363636364\n",
      "train: step: 467, loss: 0.6409952640533447, acc: 0.609375, recall: 0.8275862068965517, precision: 0.5454545454545454, f_beta: 0.6575342465753425\n",
      "train: step: 468, loss: 0.6894078254699707, acc: 0.515625, recall: 0.7833333333333333, precision: 0.4895833333333333, f_beta: 0.6025641025641025\n",
      "start training model\n",
      "train: step: 469, loss: 0.6240158081054688, acc: 0.65625, recall: 0.835820895522388, precision: 0.6292134831460674, f_beta: 0.717948717948718\n",
      "train: step: 470, loss: 0.5983593463897705, acc: 0.6484375, recall: 0.7538461538461538, precision: 0.6282051282051282, f_beta: 0.6853146853146853\n",
      "train: step: 471, loss: 0.6434730887413025, acc: 0.640625, recall: 0.75, precision: 0.6153846153846154, f_beta: 0.676056338028169\n",
      "train: step: 472, loss: 0.6243523359298706, acc: 0.625, recall: 0.5945945945945946, precision: 0.7096774193548387, f_beta: 0.6470588235294118\n",
      "train: step: 473, loss: 0.6038776636123657, acc: 0.6328125, recall: 0.7464788732394366, precision: 0.6463414634146342, f_beta: 0.69281045751634\n",
      "train: step: 474, loss: 0.6118943691253662, acc: 0.625, recall: 0.8703703703703703, precision: 0.5340909090909091, f_beta: 0.6619718309859154\n",
      "train: step: 475, loss: 0.639282763004303, acc: 0.625, recall: 0.819672131147541, precision: 0.5747126436781609, f_beta: 0.6756756756756757\n",
      "train: step: 476, loss: 0.6032410860061646, acc: 0.6953125, recall: 0.7945205479452054, precision: 0.7073170731707317, f_beta: 0.7483870967741935\n",
      "train: step: 477, loss: 0.6353247165679932, acc: 0.640625, recall: 0.8307692307692308, precision: 0.6067415730337079, f_beta: 0.7012987012987013\n",
      "train: step: 478, loss: 0.5386751294136047, acc: 0.71875, recall: 0.8166666666666667, precision: 0.6621621621621622, f_beta: 0.7313432835820896\n",
      "train: step: 479, loss: 0.5785796642303467, acc: 0.7109375, recall: 0.9152542372881356, precision: 0.627906976744186, f_beta: 0.7448275862068966\n",
      "train: step: 480, loss: 0.4909977912902832, acc: 0.8203125, recall: 0.8524590163934426, precision: 0.7878787878787878, f_beta: 0.8188976377952757\n",
      "train: step: 481, loss: 0.503261923789978, acc: 0.8125, recall: 0.7727272727272727, precision: 0.85, f_beta: 0.8095238095238095\n",
      "train: step: 482, loss: 0.4168001413345337, acc: 0.8828125, recall: 0.8771929824561403, precision: 0.8620689655172413, f_beta: 0.8695652173913043\n",
      "train: step: 483, loss: 0.44462352991104126, acc: 0.84375, recall: 0.8028169014084507, precision: 0.9047619047619048, f_beta: 0.8507462686567164\n",
      "train: step: 484, loss: 0.484477162361145, acc: 0.8046875, recall: 0.7230769230769231, precision: 0.8703703703703703, f_beta: 0.7899159663865546\n",
      "train: step: 485, loss: 0.4491933286190033, acc: 0.8359375, recall: 0.8, precision: 0.8421052631578947, f_beta: 0.8205128205128205\n",
      "train: step: 486, loss: 0.36283111572265625, acc: 0.8828125, recall: 0.8450704225352113, precision: 0.9375, f_beta: 0.8888888888888888\n",
      "train: step: 487, loss: 0.4660445749759674, acc: 0.8203125, recall: 0.7903225806451613, precision: 0.8305084745762712, f_beta: 0.8099173553719008\n",
      "train: step: 488, loss: 0.47025546431541443, acc: 0.8125, recall: 0.6612903225806451, precision: 0.9318181818181818, f_beta: 0.7735849056603773\n",
      "train: step: 489, loss: 0.36127954721450806, acc: 0.875, recall: 0.8253968253968254, precision: 0.9122807017543859, f_beta: 0.8666666666666667\n",
      "train: step: 490, loss: 0.39496538043022156, acc: 0.8515625, recall: 0.85, precision: 0.8360655737704918, f_beta: 0.8429752066115703\n",
      "train: step: 491, loss: 0.35429954528808594, acc: 0.8515625, recall: 0.8412698412698413, precision: 0.8548387096774194, f_beta: 0.848\n",
      "train: step: 492, loss: 0.38722532987594604, acc: 0.8359375, recall: 0.7857142857142857, precision: 0.9016393442622951, f_beta: 0.8396946564885497\n",
      "train: step: 493, loss: 0.39215087890625, acc: 0.8125, recall: 0.8, precision: 0.7719298245614035, f_beta: 0.7857142857142858\n",
      "train: step: 494, loss: 0.47657081484794617, acc: 0.765625, recall: 0.5645161290322581, precision: 0.9210526315789473, f_beta: 0.7000000000000001\n",
      "train: step: 495, loss: 0.34303033351898193, acc: 0.8515625, recall: 0.8714285714285714, precision: 0.8591549295774648, f_beta: 0.8652482269503546\n",
      "train: step: 496, loss: 0.5449002981185913, acc: 0.71875, recall: 0.9454545454545454, precision: 0.611764705882353, f_beta: 0.742857142857143\n",
      "train: step: 497, loss: 0.442098468542099, acc: 0.8125, recall: 0.8405797101449275, precision: 0.8169014084507042, f_beta: 0.8285714285714286\n",
      "train: step: 498, loss: 0.43082159757614136, acc: 0.8203125, recall: 0.7903225806451613, precision: 0.8305084745762712, f_beta: 0.8099173553719008\n",
      "train: step: 499, loss: 0.5208193063735962, acc: 0.734375, recall: 0.5151515151515151, precision: 0.9444444444444444, f_beta: 0.6666666666666666\n",
      "train: step: 500, loss: 0.5174685716629028, acc: 0.734375, recall: 0.5625, precision: 0.8571428571428571, f_beta: 0.6792452830188678\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T14:45:21.424688, step: 500, loss: 0.44731508844938034, acc: 0.8056891025641025,precision: 0.8193815974496503, recall: 0.8015079401752799, f_beta: 0.8091133589681839\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-500\n",
      "\n",
      "train: step: 501, loss: 0.4343501925468445, acc: 0.8046875, recall: 0.8125, precision: 0.8, f_beta: 0.8062015503875969\n",
      "train: step: 502, loss: 0.3749394118785858, acc: 0.8515625, recall: 0.8823529411764706, precision: 0.8450704225352113, f_beta: 0.8633093525179857\n",
      "train: step: 503, loss: 0.45887434482574463, acc: 0.8046875, recall: 0.8059701492537313, precision: 0.8181818181818182, f_beta: 0.8120300751879699\n",
      "train: step: 504, loss: 0.3702557682991028, acc: 0.8671875, recall: 0.819672131147541, precision: 0.8928571428571429, f_beta: 0.8547008547008548\n",
      "train: step: 505, loss: 0.3445783257484436, acc: 0.875, recall: 0.8412698412698413, precision: 0.8983050847457628, f_beta: 0.8688524590163935\n",
      "train: step: 506, loss: 0.40227803587913513, acc: 0.828125, recall: 0.8035714285714286, precision: 0.8035714285714286, f_beta: 0.8035714285714286\n",
      "train: step: 507, loss: 0.4656653106212616, acc: 0.796875, recall: 0.8805970149253731, precision: 0.7662337662337663, f_beta: 0.8194444444444445\n",
      "train: step: 508, loss: 0.3598518967628479, acc: 0.875, recall: 0.9310344827586207, precision: 0.8181818181818182, f_beta: 0.8709677419354839\n",
      "train: step: 509, loss: 0.3429616093635559, acc: 0.890625, recall: 0.875, precision: 0.9032258064516129, f_beta: 0.8888888888888888\n",
      "train: step: 510, loss: 0.43652939796447754, acc: 0.8203125, recall: 0.8813559322033898, precision: 0.7647058823529411, f_beta: 0.8188976377952756\n",
      "train: step: 511, loss: 0.3786476254463196, acc: 0.859375, recall: 0.7627118644067796, precision: 0.9183673469387755, f_beta: 0.8333333333333333\n",
      "train: step: 512, loss: 0.43776434659957886, acc: 0.828125, recall: 0.6607142857142857, precision: 0.925, f_beta: 0.7708333333333333\n",
      "train: step: 513, loss: 0.634924054145813, acc: 0.734375, recall: 0.41379310344827586, precision: 1.0, f_beta: 0.5853658536585366\n",
      "train: step: 514, loss: 0.6443272829055786, acc: 0.7109375, recall: 0.453125, precision: 0.9354838709677419, f_beta: 0.6105263157894737\n",
      "train: step: 515, loss: 0.5934122204780579, acc: 0.7265625, recall: 0.5757575757575758, precision: 0.8444444444444444, f_beta: 0.6846846846846847\n",
      "train: step: 516, loss: 0.44193172454833984, acc: 0.8359375, recall: 0.8395061728395061, precision: 0.8947368421052632, f_beta: 0.8662420382165605\n",
      "train: step: 517, loss: 0.44667893648147583, acc: 0.796875, recall: 0.9264705882352942, precision: 0.75, f_beta: 0.8289473684210525\n",
      "train: step: 518, loss: 0.704471230506897, acc: 0.6796875, recall: 1.0, precision: 0.616822429906542, f_beta: 0.7630057803468209\n",
      "train: step: 519, loss: 0.884803295135498, acc: 0.6796875, recall: 1.0, precision: 0.6434782608695652, f_beta: 0.7830687830687831\n",
      "train: step: 520, loss: 0.9321357607841492, acc: 0.6171875, recall: 1.0, precision: 0.5847457627118644, f_beta: 0.7379679144385026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 521, loss: 0.8465974926948547, acc: 0.578125, recall: 1.0, precision: 0.5462184873949579, f_beta: 0.7065217391304348\n",
      "train: step: 522, loss: 0.722926139831543, acc: 0.640625, recall: 0.9846153846153847, precision: 0.5871559633027523, f_beta: 0.735632183908046\n",
      "train: step: 523, loss: 0.6761631965637207, acc: 0.65625, recall: 0.8769230769230769, precision: 0.6129032258064516, f_beta: 0.7215189873417722\n",
      "train: step: 524, loss: 0.5371111631393433, acc: 0.7421875, recall: 0.8571428571428571, precision: 0.7228915662650602, f_beta: 0.7843137254901961\n",
      "train: step: 525, loss: 0.5782390832901001, acc: 0.6953125, recall: 0.6779661016949152, precision: 0.6666666666666666, f_beta: 0.6722689075630253\n",
      "train: step: 526, loss: 0.6095215678215027, acc: 0.640625, recall: 0.47619047619047616, precision: 0.6976744186046512, f_beta: 0.5660377358490566\n",
      "train: step: 527, loss: 0.5090015530586243, acc: 0.6875, recall: 0.4852941176470588, precision: 0.868421052631579, f_beta: 0.6226415094339622\n",
      "train: step: 528, loss: 0.5011864900588989, acc: 0.75, recall: 0.47368421052631576, precision: 0.9310344827586207, f_beta: 0.627906976744186\n",
      "train: step: 529, loss: 0.5298348665237427, acc: 0.75, recall: 0.5573770491803278, precision: 0.8717948717948718, f_beta: 0.6799999999999999\n",
      "train: step: 530, loss: 0.5291637778282166, acc: 0.7578125, recall: 0.5964912280701754, precision: 0.8095238095238095, f_beta: 0.6868686868686869\n",
      "train: step: 531, loss: 0.48256635665893555, acc: 0.78125, recall: 0.6935483870967742, precision: 0.8269230769230769, f_beta: 0.7543859649122807\n",
      "train: step: 532, loss: 0.46834197640419006, acc: 0.8203125, recall: 0.7704918032786885, precision: 0.8392857142857143, f_beta: 0.8034188034188035\n",
      "train: step: 533, loss: 0.4621242582798004, acc: 0.8125, recall: 0.7536231884057971, precision: 0.8813559322033898, f_beta: 0.8124999999999999\n",
      "train: step: 534, loss: 0.468343585729599, acc: 0.796875, recall: 0.8548387096774194, precision: 0.7571428571428571, f_beta: 0.803030303030303\n",
      "train: step: 535, loss: 0.49844902753829956, acc: 0.765625, recall: 0.8253968253968254, precision: 0.7323943661971831, f_beta: 0.7761194029850745\n",
      "train: step: 536, loss: 0.4355516731739044, acc: 0.8359375, recall: 0.9180327868852459, precision: 0.7777777777777778, f_beta: 0.8421052631578947\n",
      "train: step: 537, loss: 0.49302399158477783, acc: 0.7734375, recall: 0.8888888888888888, precision: 0.676056338028169, f_beta: 0.7679999999999999\n",
      "train: step: 538, loss: 0.4359510540962219, acc: 0.8125, recall: 0.90625, precision: 0.7631578947368421, f_beta: 0.8285714285714286\n",
      "train: step: 539, loss: 0.4518493711948395, acc: 0.828125, recall: 0.847457627118644, precision: 0.7936507936507936, f_beta: 0.819672131147541\n",
      "train: step: 540, loss: 0.4337794780731201, acc: 0.8125, recall: 0.8367346938775511, precision: 0.7192982456140351, f_beta: 0.7735849056603773\n",
      "train: step: 541, loss: 0.5249545574188232, acc: 0.7421875, recall: 0.765625, precision: 0.7313432835820896, f_beta: 0.748091603053435\n",
      "train: step: 542, loss: 0.5420247316360474, acc: 0.7734375, recall: 0.6933333333333334, precision: 0.896551724137931, f_beta: 0.7819548872180451\n",
      "train: step: 543, loss: 0.42594534158706665, acc: 0.8515625, recall: 0.7377049180327869, precision: 0.9375, f_beta: 0.8256880733944955\n",
      "train: step: 544, loss: 0.5095701217651367, acc: 0.7734375, recall: 0.6835443037974683, precision: 0.9310344827586207, f_beta: 0.7883211678832116\n",
      "train: step: 545, loss: 0.44562119245529175, acc: 0.828125, recall: 0.7741935483870968, precision: 0.8571428571428571, f_beta: 0.8135593220338982\n",
      "train: step: 546, loss: 0.32379430532455444, acc: 0.859375, recall: 0.8333333333333334, precision: 0.8620689655172413, f_beta: 0.847457627118644\n",
      "train: step: 547, loss: 0.34104055166244507, acc: 0.859375, recall: 0.8870967741935484, precision: 0.8333333333333334, f_beta: 0.859375\n",
      "train: step: 548, loss: 0.4339926838874817, acc: 0.859375, recall: 0.864406779661017, precision: 0.8360655737704918, f_beta: 0.85\n",
      "train: step: 549, loss: 0.4726875126361847, acc: 0.8203125, recall: 0.8656716417910447, precision: 0.8055555555555556, f_beta: 0.8345323741007193\n",
      "train: step: 550, loss: 0.5222160816192627, acc: 0.765625, recall: 0.9137931034482759, precision: 0.6794871794871795, f_beta: 0.7794117647058824\n",
      "train: step: 551, loss: 0.39283525943756104, acc: 0.875, recall: 0.9285714285714286, precision: 0.8125, f_beta: 0.8666666666666666\n",
      "train: step: 552, loss: 0.33809491991996765, acc: 0.8828125, recall: 0.9344262295081968, precision: 0.8382352941176471, f_beta: 0.8837209302325582\n",
      "train: step: 553, loss: 0.3311890661716461, acc: 0.890625, recall: 0.9142857142857143, precision: 0.8888888888888888, f_beta: 0.9014084507042254\n",
      "train: step: 554, loss: 0.4448877274990082, acc: 0.828125, recall: 0.746031746031746, precision: 0.8867924528301887, f_beta: 0.8103448275862069\n",
      "train: step: 555, loss: 0.47636914253234863, acc: 0.796875, recall: 0.6190476190476191, precision: 0.9512195121951219, f_beta: 0.7500000000000001\n",
      "train: step: 556, loss: 0.3670412302017212, acc: 0.8671875, recall: 0.75, precision: 0.9795918367346939, f_beta: 0.8495575221238937\n",
      "train: step: 557, loss: 0.48560625314712524, acc: 0.796875, recall: 0.6911764705882353, precision: 0.9038461538461539, f_beta: 0.7833333333333333\n",
      "train: step: 558, loss: 0.49895650148391724, acc: 0.8046875, recall: 0.6349206349206349, precision: 0.9523809523809523, f_beta: 0.7619047619047619\n",
      "train: step: 559, loss: 0.4517720639705658, acc: 0.796875, recall: 0.6153846153846154, precision: 0.975609756097561, f_beta: 0.7547169811320755\n",
      "train: step: 560, loss: 0.48686277866363525, acc: 0.8046875, recall: 0.7236842105263158, precision: 0.9322033898305084, f_beta: 0.8148148148148149\n",
      "train: step: 561, loss: 0.37398484349250793, acc: 0.859375, recall: 0.8235294117647058, precision: 0.9032258064516129, f_beta: 0.8615384615384616\n",
      "train: step: 562, loss: 0.35992658138275146, acc: 0.8671875, recall: 0.8461538461538461, precision: 0.8870967741935484, f_beta: 0.8661417322834646\n",
      "train: step: 563, loss: 0.41560691595077515, acc: 0.8203125, recall: 0.8805970149253731, precision: 0.7972972972972973, f_beta: 0.8368794326241135\n",
      "train: step: 564, loss: 0.3783608078956604, acc: 0.828125, recall: 0.9322033898305084, precision: 0.7534246575342466, f_beta: 0.8333333333333333\n",
      "train: step: 565, loss: 0.5058286190032959, acc: 0.78125, recall: 0.8983050847457628, precision: 0.7066666666666667, f_beta: 0.7910447761194029\n",
      "train: step: 566, loss: 0.4056625962257385, acc: 0.828125, recall: 0.9206349206349206, precision: 0.7733333333333333, f_beta: 0.8405797101449275\n",
      "train: step: 567, loss: 0.45329469442367554, acc: 0.7890625, recall: 0.8852459016393442, precision: 0.7297297297297297, f_beta: 0.7999999999999999\n",
      "train: step: 568, loss: 0.4292806088924408, acc: 0.8515625, recall: 0.8688524590163934, precision: 0.828125, f_beta: 0.8480000000000001\n",
      "train: step: 569, loss: 0.41052767634391785, acc: 0.859375, recall: 0.8529411764705882, precision: 0.8787878787878788, f_beta: 0.8656716417910447\n",
      "train: step: 570, loss: 0.41347363591194153, acc: 0.8203125, recall: 0.8387096774193549, precision: 0.8, f_beta: 0.8188976377952757\n",
      "train: step: 571, loss: 0.38375845551490784, acc: 0.8203125, recall: 0.7205882352941176, precision: 0.9245283018867925, f_beta: 0.8099173553719008\n",
      "train: step: 572, loss: 0.3794940710067749, acc: 0.8515625, recall: 0.7903225806451613, precision: 0.8909090909090909, f_beta: 0.8376068376068375\n",
      "train: step: 573, loss: 0.3731045722961426, acc: 0.875, recall: 0.8, precision: 0.9454545454545454, f_beta: 0.8666666666666666\n",
      "train: step: 574, loss: 0.342113196849823, acc: 0.8671875, recall: 0.7887323943661971, precision: 0.9655172413793104, f_beta: 0.8682170542635659\n",
      "train: step: 575, loss: 0.3897936940193176, acc: 0.84375, recall: 0.7368421052631579, precision: 0.8936170212765957, f_beta: 0.8076923076923077\n",
      "train: step: 576, loss: 0.3568234443664551, acc: 0.8515625, recall: 0.84375, precision: 0.8571428571428571, f_beta: 0.8503937007874015\n",
      "train: step: 577, loss: 0.3894920349121094, acc: 0.8359375, recall: 0.8095238095238095, precision: 0.85, f_beta: 0.8292682926829269\n",
      "train: step: 578, loss: 0.3899904787540436, acc: 0.84375, recall: 0.8235294117647058, precision: 0.7924528301886793, f_beta: 0.8076923076923077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 579, loss: 0.3653629422187805, acc: 0.84375, recall: 0.8615384615384616, precision: 0.835820895522388, f_beta: 0.8484848484848485\n",
      "train: step: 580, loss: 0.35301730036735535, acc: 0.8671875, recall: 0.9154929577464789, precision: 0.8552631578947368, f_beta: 0.8843537414965986\n",
      "train: step: 581, loss: 0.28171080350875854, acc: 0.9140625, recall: 0.9538461538461539, precision: 0.8857142857142857, f_beta: 0.9185185185185185\n",
      "train: step: 582, loss: 0.2777208089828491, acc: 0.8984375, recall: 0.9193548387096774, precision: 0.8769230769230769, f_beta: 0.8976377952755904\n",
      "train: step: 583, loss: 0.4429500699043274, acc: 0.8125, recall: 0.9076923076923077, precision: 0.7662337662337663, f_beta: 0.8309859154929577\n",
      "train: step: 584, loss: 0.3903989791870117, acc: 0.8359375, recall: 0.8524590163934426, precision: 0.8125, f_beta: 0.8319999999999999\n",
      "train: step: 585, loss: 0.3943512439727783, acc: 0.8125, recall: 0.746031746031746, precision: 0.8545454545454545, f_beta: 0.7966101694915254\n",
      "train: step: 586, loss: 0.48789307475090027, acc: 0.7890625, recall: 0.8823529411764706, precision: 0.6818181818181818, f_beta: 0.7692307692307693\n",
      "train: step: 587, loss: 0.3657762408256531, acc: 0.859375, recall: 0.8028169014084507, precision: 0.9344262295081968, f_beta: 0.8636363636363635\n",
      "train: step: 588, loss: 0.31120821833610535, acc: 0.8828125, recall: 0.8923076923076924, precision: 0.8787878787878788, f_beta: 0.8854961832061069\n",
      "train: step: 589, loss: 0.4246988296508789, acc: 0.8046875, recall: 0.8135593220338984, precision: 0.7741935483870968, f_beta: 0.7933884297520662\n",
      "train: step: 590, loss: 0.43508338928222656, acc: 0.8046875, recall: 0.8524590163934426, precision: 0.7647058823529411, f_beta: 0.8062015503875968\n",
      "train: step: 591, loss: 0.38163983821868896, acc: 0.8671875, recall: 0.9016393442622951, precision: 0.8333333333333334, f_beta: 0.8661417322834646\n",
      "train: step: 592, loss: 0.361159086227417, acc: 0.8671875, recall: 0.9701492537313433, precision: 0.8125, f_beta: 0.8843537414965987\n",
      "train: step: 593, loss: 0.22652027010917664, acc: 0.9375, recall: 0.9821428571428571, precision: 0.8870967741935484, f_beta: 0.9322033898305085\n",
      "train: step: 594, loss: 0.3530913293361664, acc: 0.8671875, recall: 0.8181818181818182, precision: 0.9545454545454546, f_beta: 0.881118881118881\n",
      "train: step: 595, loss: 0.39316004514694214, acc: 0.8671875, recall: 0.9032258064516129, precision: 0.835820895522388, f_beta: 0.8682170542635659\n",
      "train: step: 596, loss: 0.3351006507873535, acc: 0.8671875, recall: 0.8939393939393939, precision: 0.855072463768116, f_beta: 0.8740740740740741\n",
      "train: step: 597, loss: 0.3274495005607605, acc: 0.8671875, recall: 0.9852941176470589, precision: 0.8072289156626506, f_beta: 0.8874172185430464\n",
      "train: step: 598, loss: 0.34225043654441833, acc: 0.8515625, recall: 0.921875, precision: 0.8082191780821918, f_beta: 0.8613138686131386\n",
      "train: step: 599, loss: 0.3408755660057068, acc: 0.8671875, recall: 0.9733333333333334, precision: 0.8295454545454546, f_beta: 0.8957055214723927\n",
      "train: step: 600, loss: 0.353823721408844, acc: 0.84375, recall: 0.9710144927536232, precision: 0.788235294117647, f_beta: 0.8701298701298701\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 14:49:13.292679 140504067012352 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-07-26T14:49:13.194827, step: 600, loss: 0.41734415674820924, acc: 0.8056891025641025,precision: 0.9170105933624288, recall: 0.7532126017505482, f_beta: 0.825739114121917\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-600\n",
      "\n",
      "train: step: 601, loss: 0.3334807753562927, acc: 0.875, recall: 0.9298245614035088, precision: 0.8153846153846154, f_beta: 0.8688524590163934\n",
      "train: step: 602, loss: 0.4000929594039917, acc: 0.8046875, recall: 0.95, precision: 0.7215189873417721, f_beta: 0.8201438848920861\n",
      "train: step: 603, loss: 0.3446348011493683, acc: 0.828125, recall: 0.8688524590163934, precision: 0.7910447761194029, f_beta: 0.828125\n",
      "train: step: 604, loss: 0.34852075576782227, acc: 0.84375, recall: 0.8840579710144928, precision: 0.8356164383561644, f_beta: 0.8591549295774648\n",
      "train: step: 605, loss: 0.2895595133304596, acc: 0.9140625, recall: 0.9032258064516129, precision: 0.9180327868852459, f_beta: 0.9105691056910569\n",
      "train: step: 606, loss: 0.2875668406486511, acc: 0.8828125, recall: 0.8571428571428571, precision: 0.9, f_beta: 0.8780487804878048\n",
      "train: step: 607, loss: 0.33047449588775635, acc: 0.8671875, recall: 0.8387096774193549, precision: 0.8813559322033898, f_beta: 0.859504132231405\n",
      "train: step: 608, loss: 0.3850441575050354, acc: 0.859375, recall: 0.8412698412698413, precision: 0.8688524590163934, f_beta: 0.8548387096774194\n",
      "train: step: 609, loss: 0.42522910237312317, acc: 0.8359375, recall: 0.7868852459016393, precision: 0.8571428571428571, f_beta: 0.8205128205128205\n",
      "train: step: 610, loss: 0.35786378383636475, acc: 0.8671875, recall: 0.8253968253968254, precision: 0.896551724137931, f_beta: 0.8595041322314049\n",
      "train: step: 611, loss: 0.2828484773635864, acc: 0.875, recall: 0.855072463768116, precision: 0.9076923076923077, f_beta: 0.8805970149253731\n",
      "train: step: 612, loss: 0.44208192825317383, acc: 0.8203125, recall: 0.859375, precision: 0.7971014492753623, f_beta: 0.8270676691729324\n",
      "train: step: 613, loss: 0.2819816768169403, acc: 0.8984375, recall: 0.8970588235294118, precision: 0.9104477611940298, f_beta: 0.9037037037037037\n",
      "train: step: 614, loss: 0.32292261719703674, acc: 0.875, recall: 0.8484848484848485, precision: 0.9032258064516129, f_beta: 0.875\n",
      "train: step: 615, loss: 0.31735557317733765, acc: 0.875, recall: 0.8767123287671232, precision: 0.9014084507042254, f_beta: 0.8888888888888888\n",
      "train: step: 616, loss: 0.43178945779800415, acc: 0.84375, recall: 0.7936507936507936, precision: 0.8771929824561403, f_beta: 0.8333333333333334\n",
      "train: step: 617, loss: 0.39687636494636536, acc: 0.8359375, recall: 0.8275862068965517, precision: 0.8135593220338984, f_beta: 0.8205128205128206\n",
      "train: step: 618, loss: 0.2839429974555969, acc: 0.890625, recall: 0.8333333333333334, precision: 0.9259259259259259, f_beta: 0.8771929824561403\n",
      "train: step: 619, loss: 0.23567184805870056, acc: 0.921875, recall: 0.8928571428571429, precision: 0.9259259259259259, f_beta: 0.9090909090909091\n",
      "train: step: 620, loss: 0.3594334125518799, acc: 0.875, recall: 0.7894736842105263, precision: 0.9183673469387755, f_beta: 0.8490566037735849\n",
      "train: step: 621, loss: 0.38818293809890747, acc: 0.828125, recall: 0.7808219178082192, precision: 0.9047619047619048, f_beta: 0.8382352941176471\n",
      "train: step: 622, loss: 0.3299019932746887, acc: 0.859375, recall: 0.7692307692307693, precision: 0.9433962264150944, f_beta: 0.847457627118644\n",
      "train: step: 623, loss: 0.486553817987442, acc: 0.78125, recall: 0.7272727272727273, precision: 0.8888888888888888, f_beta: 0.7999999999999999\n",
      "train: step: 624, loss: 0.32743844389915466, acc: 0.875, recall: 0.8548387096774194, precision: 0.8833333333333333, f_beta: 0.8688524590163934\n",
      "start training model\n",
      "train: step: 625, loss: 0.3630076050758362, acc: 0.84375, recall: 0.9259259259259259, precision: 0.7575757575757576, f_beta: 0.8333333333333334\n",
      "train: step: 626, loss: 0.26587197184562683, acc: 0.90625, recall: 1.0, precision: 0.8554216867469879, f_beta: 0.922077922077922\n",
      "train: step: 627, loss: 0.28995639085769653, acc: 0.8828125, recall: 0.95, precision: 0.8260869565217391, f_beta: 0.8837209302325583\n",
      "train: step: 628, loss: 0.39276641607284546, acc: 0.8125, recall: 0.9242424242424242, precision: 0.7625, f_beta: 0.8356164383561645\n",
      "train: step: 629, loss: 0.244994655251503, acc: 0.90625, recall: 0.921875, precision: 0.8939393939393939, f_beta: 0.9076923076923077\n",
      "train: step: 630, loss: 0.2539758086204529, acc: 0.9140625, recall: 0.9672131147540983, precision: 0.8676470588235294, f_beta: 0.9147286821705426\n",
      "train: step: 631, loss: 0.27305638790130615, acc: 0.9140625, recall: 0.9130434782608695, precision: 0.9264705882352942, f_beta: 0.9197080291970804\n",
      "train: step: 632, loss: 0.2938368618488312, acc: 0.890625, recall: 0.9324324324324325, precision: 0.8846153846153846, f_beta: 0.9078947368421053\n",
      "train: step: 633, loss: 0.29082754254341125, acc: 0.875, recall: 0.9642857142857143, precision: 0.7941176470588235, f_beta: 0.8709677419354839\n",
      "train: step: 634, loss: 0.38902056217193604, acc: 0.859375, recall: 0.9137931034482759, precision: 0.803030303030303, f_beta: 0.8548387096774194\n",
      "train: step: 635, loss: 0.3157613277435303, acc: 0.859375, recall: 0.9866666666666667, precision: 0.8131868131868132, f_beta: 0.8915662650602411\n",
      "train: step: 636, loss: 0.2848493158817291, acc: 0.90625, recall: 0.96875, precision: 0.8611111111111112, f_beta: 0.911764705882353\n",
      "train: step: 637, loss: 0.32435518503189087, acc: 0.8515625, recall: 0.9117647058823529, precision: 0.8266666666666667, f_beta: 0.8671328671328671\n",
      "train: step: 638, loss: 0.3391997218132019, acc: 0.890625, recall: 0.9436619718309859, precision: 0.8701298701298701, f_beta: 0.9054054054054054\n",
      "train: step: 639, loss: 0.35098183155059814, acc: 0.859375, recall: 0.921875, precision: 0.8194444444444444, f_beta: 0.8676470588235294\n",
      "train: step: 640, loss: 0.3219658136367798, acc: 0.8671875, recall: 0.8387096774193549, precision: 0.8813559322033898, f_beta: 0.859504132231405\n",
      "train: step: 641, loss: 0.3013998866081238, acc: 0.8828125, recall: 0.782608695652174, precision: 0.8780487804878049, f_beta: 0.8275862068965518\n",
      "train: step: 642, loss: 0.22095340490341187, acc: 0.9140625, recall: 0.9076923076923077, precision: 0.921875, f_beta: 0.9147286821705427\n",
      "train: step: 643, loss: 0.2922620177268982, acc: 0.8828125, recall: 0.8714285714285714, precision: 0.9104477611940298, f_beta: 0.8905109489051095\n",
      "train: step: 644, loss: 0.29037076234817505, acc: 0.890625, recall: 0.8333333333333334, precision: 0.9259259259259259, f_beta: 0.8771929824561403\n",
      "train: step: 645, loss: 0.24281489849090576, acc: 0.8984375, recall: 0.8484848484848485, precision: 0.9491525423728814, f_beta: 0.896\n",
      "train: step: 646, loss: 0.326282262802124, acc: 0.859375, recall: 0.8115942028985508, precision: 0.9180327868852459, f_beta: 0.8615384615384616\n",
      "train: step: 647, loss: 0.2598561644554138, acc: 0.90625, recall: 0.8732394366197183, precision: 0.9538461538461539, f_beta: 0.9117647058823529\n",
      "train: step: 648, loss: 0.28572866320610046, acc: 0.8984375, recall: 0.9393939393939394, precision: 0.8732394366197183, f_beta: 0.9051094890510948\n",
      "train: step: 649, loss: 0.3179137110710144, acc: 0.875, recall: 0.9230769230769231, precision: 0.8450704225352113, f_beta: 0.8823529411764706\n",
      "train: step: 650, loss: 0.1715482920408249, acc: 0.921875, recall: 0.9324324324324325, precision: 0.9324324324324325, f_beta: 0.9324324324324325\n",
      "train: step: 651, loss: 0.3389081060886383, acc: 0.859375, recall: 0.9272727272727272, precision: 0.7846153846153846, f_beta: 0.8500000000000001\n",
      "train: step: 652, loss: 0.2204994559288025, acc: 0.9296875, recall: 0.984375, precision: 0.8873239436619719, f_beta: 0.9333333333333333\n",
      "train: step: 653, loss: 0.24925781786441803, acc: 0.90625, recall: 0.9661016949152542, precision: 0.8507462686567164, f_beta: 0.9047619047619047\n",
      "train: step: 654, loss: 0.25747060775756836, acc: 0.9140625, recall: 0.8985507246376812, precision: 0.9393939393939394, f_beta: 0.9185185185185185\n",
      "train: step: 655, loss: 0.1466882973909378, acc: 0.953125, recall: 0.9464285714285714, precision: 0.9464285714285714, f_beta: 0.9464285714285714\n",
      "train: step: 656, loss: 0.3562719523906708, acc: 0.8828125, recall: 0.8208955223880597, precision: 0.9482758620689655, f_beta: 0.8799999999999999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 657, loss: 0.29766392707824707, acc: 0.90625, recall: 0.8615384615384616, precision: 0.9491525423728814, f_beta: 0.903225806451613\n",
      "train: step: 658, loss: 0.2764798402786255, acc: 0.8828125, recall: 0.8533333333333334, precision: 0.9411764705882353, f_beta: 0.8951048951048952\n",
      "train: step: 659, loss: 0.3433328866958618, acc: 0.8671875, recall: 0.8235294117647058, precision: 0.9180327868852459, f_beta: 0.868217054263566\n",
      "train: step: 660, loss: 0.2522072494029999, acc: 0.890625, recall: 0.859375, precision: 0.9166666666666666, f_beta: 0.8870967741935484\n",
      "train: step: 661, loss: 0.2563058137893677, acc: 0.9140625, recall: 0.9295774647887324, precision: 0.9166666666666666, f_beta: 0.9230769230769231\n",
      "train: step: 662, loss: 0.3160707950592041, acc: 0.875, recall: 0.9, precision: 0.84375, f_beta: 0.870967741935484\n",
      "train: step: 663, loss: 0.29577410221099854, acc: 0.875, recall: 0.9464285714285714, precision: 0.803030303030303, f_beta: 0.8688524590163934\n",
      "train: step: 664, loss: 0.2708815932273865, acc: 0.8984375, recall: 0.8985507246376812, precision: 0.9117647058823529, f_beta: 0.9051094890510949\n",
      "train: step: 665, loss: 0.3513076603412628, acc: 0.84375, recall: 0.9259259259259259, precision: 0.7575757575757576, f_beta: 0.8333333333333334\n",
      "train: step: 666, loss: 0.2946217656135559, acc: 0.8984375, recall: 0.953125, precision: 0.8591549295774648, f_beta: 0.9037037037037037\n",
      "train: step: 667, loss: 0.27757692337036133, acc: 0.890625, recall: 0.9090909090909091, precision: 0.847457627118644, f_beta: 0.8771929824561402\n",
      "train: step: 668, loss: 0.3770321011543274, acc: 0.84375, recall: 0.8615384615384616, precision: 0.835820895522388, f_beta: 0.8484848484848485\n",
      "train: step: 669, loss: 0.30427128076553345, acc: 0.890625, recall: 0.84375, precision: 0.9310344827586207, f_beta: 0.8852459016393444\n",
      "train: step: 670, loss: 0.2528620958328247, acc: 0.9140625, recall: 0.8545454545454545, precision: 0.94, f_beta: 0.8952380952380952\n",
      "train: step: 671, loss: 0.404323935508728, acc: 0.84375, recall: 0.8260869565217391, precision: 0.8769230769230769, f_beta: 0.8507462686567164\n",
      "train: step: 672, loss: 0.2192620187997818, acc: 0.9140625, recall: 0.8305084745762712, precision: 0.98, f_beta: 0.8990825688073395\n",
      "train: step: 673, loss: 0.2654205560684204, acc: 0.90625, recall: 0.8235294117647058, precision: 1.0, f_beta: 0.9032258064516129\n",
      "train: step: 674, loss: 0.28918084502220154, acc: 0.875, recall: 0.8208955223880597, precision: 0.9322033898305084, f_beta: 0.873015873015873\n",
      "train: step: 675, loss: 0.2754347324371338, acc: 0.890625, recall: 0.8181818181818182, precision: 0.9642857142857143, f_beta: 0.8852459016393442\n",
      "train: step: 676, loss: 0.3036777973175049, acc: 0.875, recall: 0.8688524590163934, precision: 0.8688524590163934, f_beta: 0.8688524590163934\n",
      "train: step: 677, loss: 0.2784348726272583, acc: 0.890625, recall: 0.8805970149253731, precision: 0.9076923076923077, f_beta: 0.8939393939393939\n",
      "train: step: 678, loss: 0.25796499848365784, acc: 0.9140625, recall: 0.967741935483871, precision: 0.8695652173913043, f_beta: 0.9160305343511451\n",
      "train: step: 679, loss: 0.2974613308906555, acc: 0.8671875, recall: 0.8484848484848485, precision: 0.8888888888888888, f_beta: 0.8682170542635659\n",
      "train: step: 680, loss: 0.22824141383171082, acc: 0.921875, recall: 0.8985507246376812, precision: 0.9538461538461539, f_beta: 0.9253731343283582\n",
      "train: step: 681, loss: 0.3270718455314636, acc: 0.8671875, recall: 0.8695652173913043, precision: 0.8823529411764706, f_beta: 0.8759124087591241\n",
      "train: step: 682, loss: 0.4006882309913635, acc: 0.8671875, recall: 0.95, precision: 0.8028169014084507, f_beta: 0.8702290076335878\n",
      "train: step: 683, loss: 0.2897629737854004, acc: 0.8515625, recall: 0.8615384615384616, precision: 0.8484848484848485, f_beta: 0.8549618320610687\n",
      "train: step: 684, loss: 0.2511429190635681, acc: 0.9140625, recall: 0.8985507246376812, precision: 0.9393939393939394, f_beta: 0.9185185185185185\n",
      "train: step: 685, loss: 0.2103557288646698, acc: 0.921875, recall: 0.8571428571428571, precision: 0.9818181818181818, f_beta: 0.9152542372881356\n",
      "train: step: 686, loss: 0.24642817676067352, acc: 0.8984375, recall: 0.8596491228070176, precision: 0.9074074074074074, f_beta: 0.8828828828828829\n",
      "train: step: 687, loss: 0.3284047842025757, acc: 0.890625, recall: 0.8823529411764706, precision: 0.9090909090909091, f_beta: 0.8955223880597014\n",
      "train: step: 688, loss: 0.36028051376342773, acc: 0.8515625, recall: 0.8082191780821918, precision: 0.921875, f_beta: 0.8613138686131386\n",
      "train: step: 689, loss: 0.3270810544490814, acc: 0.859375, recall: 0.9245283018867925, precision: 0.7777777777777778, f_beta: 0.8448275862068966\n",
      "train: step: 690, loss: 0.26851940155029297, acc: 0.8984375, recall: 0.8387096774193549, precision: 0.9454545454545454, f_beta: 0.8888888888888888\n",
      "train: step: 691, loss: 0.2821189761161804, acc: 0.8828125, recall: 0.9032258064516129, precision: 0.8615384615384616, f_beta: 0.8818897637795274\n",
      "train: step: 692, loss: 0.24570299685001373, acc: 0.8828125, recall: 0.8923076923076924, precision: 0.8787878787878788, f_beta: 0.8854961832061069\n",
      "train: step: 693, loss: 0.25124314427375793, acc: 0.90625, recall: 0.9629629629629629, precision: 0.8387096774193549, f_beta: 0.896551724137931\n",
      "train: step: 694, loss: 0.25804251432418823, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
      "train: step: 695, loss: 0.36758145689964294, acc: 0.8203125, recall: 0.8524590163934426, precision: 0.7878787878787878, f_beta: 0.8188976377952757\n",
      "train: step: 696, loss: 0.22871677577495575, acc: 0.90625, recall: 0.8923076923076924, precision: 0.9206349206349206, f_beta: 0.90625\n",
      "train: step: 697, loss: 0.27784818410873413, acc: 0.890625, recall: 0.9295774647887324, precision: 0.88, f_beta: 0.9041095890410958\n",
      "train: step: 698, loss: 0.3033817410469055, acc: 0.875, recall: 0.875, precision: 0.875, f_beta: 0.875\n",
      "train: step: 699, loss: 0.3999147415161133, acc: 0.8515625, recall: 0.9090909090909091, precision: 0.821917808219178, f_beta: 0.8633093525179857\n",
      "train: step: 700, loss: 0.2701305150985718, acc: 0.890625, recall: 0.8793103448275862, precision: 0.8793103448275862, f_beta: 0.8793103448275863\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T14:53:04.357671, step: 700, loss: 0.3698208737067687, acc: 0.8533653846153846,precision: 0.8632117517913238, recall: 0.8506816064662157, f_beta: 0.8560007900814818\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-700\n",
      "\n",
      "train: step: 701, loss: 0.28523778915405273, acc: 0.8671875, recall: 0.9701492537313433, precision: 0.8125, f_beta: 0.8843537414965987\n",
      "train: step: 702, loss: 0.23250815272331238, acc: 0.90625, recall: 0.9027777777777778, precision: 0.9285714285714286, f_beta: 0.9154929577464788\n",
      "train: step: 703, loss: 0.21972376108169556, acc: 0.9296875, recall: 0.9354838709677419, precision: 0.9206349206349206, f_beta: 0.9279999999999999\n",
      "train: step: 704, loss: 0.20145666599273682, acc: 0.9140625, recall: 0.85, precision: 0.9622641509433962, f_beta: 0.9026548672566371\n",
      "train: step: 705, loss: 0.31406867504119873, acc: 0.875, recall: 0.8709677419354839, precision: 0.8709677419354839, f_beta: 0.8709677419354839\n",
      "train: step: 706, loss: 0.25556108355522156, acc: 0.8828125, recall: 0.8507462686567164, precision: 0.9193548387096774, f_beta: 0.883720930232558\n",
      "train: step: 707, loss: 0.2990703582763672, acc: 0.890625, recall: 0.8676470588235294, precision: 0.921875, f_beta: 0.893939393939394\n",
      "train: step: 708, loss: 0.3279763460159302, acc: 0.8984375, recall: 0.8448275862068966, precision: 0.9245283018867925, f_beta: 0.8828828828828829\n",
      "train: step: 709, loss: 0.312601238489151, acc: 0.875, recall: 0.8392857142857143, precision: 0.8703703703703703, f_beta: 0.8545454545454546\n",
      "train: step: 710, loss: 0.28788748383522034, acc: 0.8984375, recall: 0.8428571428571429, precision: 0.9672131147540983, f_beta: 0.9007633587786259\n",
      "train: step: 711, loss: 0.28186070919036865, acc: 0.90625, recall: 0.8153846153846154, precision: 1.0, f_beta: 0.8983050847457628\n",
      "train: step: 712, loss: 0.2604488134384155, acc: 0.890625, recall: 0.8636363636363636, precision: 0.9193548387096774, f_beta: 0.890625\n",
      "train: step: 713, loss: 0.2153932750225067, acc: 0.9296875, recall: 0.9285714285714286, precision: 0.9420289855072463, f_beta: 0.935251798561151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 714, loss: 0.3613634407520294, acc: 0.8671875, recall: 0.855072463768116, precision: 0.8939393939393939, f_beta: 0.8740740740740741\n",
      "train: step: 715, loss: 0.33839845657348633, acc: 0.859375, recall: 0.8656716417910447, precision: 0.8656716417910447, f_beta: 0.8656716417910447\n",
      "train: step: 716, loss: 0.21463432908058167, acc: 0.921875, recall: 0.9710144927536232, precision: 0.8933333333333333, f_beta: 0.9305555555555556\n",
      "train: step: 717, loss: 0.2718316316604614, acc: 0.875, recall: 0.9473684210526315, precision: 0.8059701492537313, f_beta: 0.8709677419354839\n",
      "train: step: 718, loss: 0.25319036841392517, acc: 0.8828125, recall: 0.8363636363636363, precision: 0.8846153846153846, f_beta: 0.8598130841121494\n",
      "train: step: 719, loss: 0.27046656608581543, acc: 0.859375, recall: 0.9402985074626866, precision: 0.8181818181818182, f_beta: 0.8750000000000001\n",
      "train: step: 720, loss: 0.29566729068756104, acc: 0.875, recall: 0.9245283018867925, precision: 0.8032786885245902, f_beta: 0.8596491228070176\n",
      "train: step: 721, loss: 0.3230528235435486, acc: 0.8828125, recall: 0.9354838709677419, precision: 0.8405797101449275, f_beta: 0.8854961832061068\n",
      "train: step: 722, loss: 0.2939191162586212, acc: 0.8828125, recall: 0.875, precision: 0.8888888888888888, f_beta: 0.8818897637795274\n",
      "train: step: 723, loss: 0.2748766541481018, acc: 0.8984375, recall: 0.9206349206349206, precision: 0.8787878787878788, f_beta: 0.8992248062015504\n",
      "train: step: 724, loss: 0.2866925597190857, acc: 0.890625, recall: 0.8545454545454545, precision: 0.8867924528301887, f_beta: 0.8703703703703703\n",
      "train: step: 725, loss: 0.21602168679237366, acc: 0.9140625, recall: 0.8771929824561403, precision: 0.9259259259259259, f_beta: 0.9009009009009009\n",
      "train: step: 726, loss: 0.2744806408882141, acc: 0.9140625, recall: 0.875, precision: 0.9491525423728814, f_beta: 0.9105691056910569\n",
      "train: step: 727, loss: 0.2858157753944397, acc: 0.8515625, recall: 0.7666666666666667, precision: 0.9019607843137255, f_beta: 0.8288288288288289\n",
      "train: step: 728, loss: 0.23115117847919464, acc: 0.8984375, recall: 0.8461538461538461, precision: 0.9482758620689655, f_beta: 0.894308943089431\n",
      "train: step: 729, loss: 0.3813960552215576, acc: 0.84375, recall: 0.7972972972972973, precision: 0.921875, f_beta: 0.8550724637681159\n",
      "train: step: 730, loss: 0.3461029827594757, acc: 0.875, recall: 0.847457627118644, precision: 0.8771929824561403, f_beta: 0.8620689655172413\n",
      "train: step: 731, loss: 0.26065248250961304, acc: 0.8828125, recall: 0.8545454545454545, precision: 0.8703703703703703, f_beta: 0.8623853211009175\n",
      "train: step: 732, loss: 0.2933351397514343, acc: 0.8671875, recall: 0.8857142857142857, precision: 0.8732394366197183, f_beta: 0.8794326241134751\n",
      "train: step: 733, loss: 0.28796571493148804, acc: 0.8984375, recall: 1.0, precision: 0.803030303030303, f_beta: 0.8907563025210083\n",
      "train: step: 734, loss: 0.29252129793167114, acc: 0.8828125, recall: 0.967741935483871, precision: 0.821917808219178, f_beta: 0.8888888888888888\n",
      "train: step: 735, loss: 0.3564223051071167, acc: 0.828125, recall: 0.9444444444444444, precision: 0.7906976744186046, f_beta: 0.8607594936708861\n",
      "train: step: 736, loss: 0.1649342030286789, acc: 0.953125, recall: 0.971830985915493, precision: 0.9452054794520548, f_beta: 0.9583333333333334\n",
      "train: step: 737, loss: 0.26809802651405334, acc: 0.890625, recall: 0.9375, precision: 0.8571428571428571, f_beta: 0.8955223880597014\n",
      "train: step: 738, loss: 0.27128684520721436, acc: 0.875, recall: 0.9154929577464789, precision: 0.8666666666666667, f_beta: 0.8904109589041095\n",
      "train: step: 739, loss: 0.1583576202392578, acc: 0.9609375, recall: 0.9682539682539683, precision: 0.953125, f_beta: 0.9606299212598425\n",
      "train: step: 740, loss: 0.2677850127220154, acc: 0.8984375, recall: 0.9508196721311475, precision: 0.8529411764705882, f_beta: 0.8992248062015503\n",
      "train: step: 741, loss: 0.26702505350112915, acc: 0.8984375, recall: 0.9846153846153847, precision: 0.8421052631578947, f_beta: 0.9078014184397163\n",
      "train: step: 742, loss: 0.27065420150756836, acc: 0.890625, recall: 0.890625, precision: 0.890625, f_beta: 0.890625\n",
      "train: step: 743, loss: 0.30195415019989014, acc: 0.8671875, recall: 0.8307692307692308, precision: 0.9, f_beta: 0.8640000000000001\n",
      "train: step: 744, loss: 0.2742951512336731, acc: 0.8828125, recall: 0.8888888888888888, precision: 0.875, f_beta: 0.8818897637795274\n",
      "train: step: 745, loss: 0.33918651938438416, acc: 0.859375, recall: 0.7916666666666666, precision: 0.95, f_beta: 0.8636363636363635\n",
      "train: step: 746, loss: 0.24809998273849487, acc: 0.90625, recall: 0.9, precision: 0.9264705882352942, f_beta: 0.9130434782608695\n",
      "train: step: 747, loss: 0.2753232419490814, acc: 0.890625, recall: 0.8620689655172413, precision: 0.8928571428571429, f_beta: 0.8771929824561403\n",
      "train: step: 748, loss: 0.19825612008571625, acc: 0.9140625, recall: 0.958904109589041, precision: 0.8974358974358975, f_beta: 0.9271523178807948\n",
      "train: step: 749, loss: 0.23710250854492188, acc: 0.8984375, recall: 0.9661016949152542, precision: 0.8382352941176471, f_beta: 0.8976377952755905\n",
      "train: step: 750, loss: 0.19678637385368347, acc: 0.890625, recall: 0.9393939393939394, precision: 0.8611111111111112, f_beta: 0.8985507246376813\n",
      "train: step: 751, loss: 0.2799636423587799, acc: 0.8984375, recall: 0.9545454545454546, precision: 0.863013698630137, f_beta: 0.906474820143885\n",
      "train: step: 752, loss: 0.32940682768821716, acc: 0.8828125, recall: 0.9354838709677419, precision: 0.8405797101449275, f_beta: 0.8854961832061068\n",
      "train: step: 753, loss: 0.30870044231414795, acc: 0.8671875, recall: 0.9523809523809523, precision: 0.8108108108108109, f_beta: 0.8759124087591241\n",
      "train: step: 754, loss: 0.19160817563533783, acc: 0.9453125, recall: 0.96875, precision: 0.9253731343283582, f_beta: 0.9465648854961832\n",
      "train: step: 755, loss: 0.18525314331054688, acc: 0.9296875, recall: 0.9122807017543859, precision: 0.9285714285714286, f_beta: 0.9203539823008849\n",
      "train: step: 756, loss: 0.2578918933868408, acc: 0.8984375, recall: 0.875, precision: 0.9180327868852459, f_beta: 0.8959999999999999\n",
      "train: step: 757, loss: 0.35338321328163147, acc: 0.859375, recall: 0.803030303030303, precision: 0.9137931034482759, f_beta: 0.8548387096774194\n",
      "train: step: 758, loss: 0.23210054636001587, acc: 0.9375, recall: 0.8985507246376812, precision: 0.9841269841269841, f_beta: 0.9393939393939393\n",
      "train: step: 759, loss: 0.3165634870529175, acc: 0.8515625, recall: 0.7727272727272727, precision: 0.9272727272727272, f_beta: 0.8429752066115703\n",
      "train: step: 760, loss: 0.33219993114471436, acc: 0.875, recall: 0.7894736842105263, precision: 0.9183673469387755, f_beta: 0.8490566037735849\n",
      "train: step: 761, loss: 0.2693578004837036, acc: 0.90625, recall: 0.8909090909090909, precision: 0.8909090909090909, f_beta: 0.8909090909090909\n",
      "train: step: 762, loss: 0.16935521364212036, acc: 0.921875, recall: 0.8852459016393442, precision: 0.9473684210526315, f_beta: 0.9152542372881356\n",
      "train: step: 763, loss: 0.23109355568885803, acc: 0.921875, recall: 0.9365079365079365, precision: 0.9076923076923077, f_beta: 0.9218749999999999\n",
      "train: step: 764, loss: 0.24206069111824036, acc: 0.9140625, recall: 0.9090909090909091, precision: 0.9230769230769231, f_beta: 0.9160305343511451\n",
      "train: step: 765, loss: 0.26863551139831543, acc: 0.9140625, recall: 0.953125, precision: 0.8840579710144928, f_beta: 0.9172932330827068\n",
      "train: step: 766, loss: 0.3596110939979553, acc: 0.859375, recall: 0.890625, precision: 0.8382352941176471, f_beta: 0.8636363636363636\n",
      "train: step: 767, loss: 0.3079187572002411, acc: 0.8671875, recall: 0.9482758620689655, precision: 0.7971014492753623, f_beta: 0.8661417322834646\n",
      "train: step: 768, loss: 0.3706900179386139, acc: 0.8515625, recall: 0.9166666666666666, precision: 0.8354430379746836, f_beta: 0.8741721854304636\n",
      "train: step: 769, loss: 0.391653835773468, acc: 0.8359375, recall: 0.9696969696969697, precision: 0.7710843373493976, f_beta: 0.8590604026845637\n",
      "train: step: 770, loss: 0.27761727571487427, acc: 0.875, recall: 0.9384615384615385, precision: 0.8356164383561644, f_beta: 0.8840579710144928\n",
      "train: step: 771, loss: 0.2425766885280609, acc: 0.9140625, recall: 0.8955223880597015, precision: 0.9375, f_beta: 0.9160305343511451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 772, loss: 0.2170344889163971, acc: 0.890625, recall: 0.8548387096774194, precision: 0.9137931034482759, f_beta: 0.8833333333333333\n",
      "train: step: 773, loss: 0.19671756029129028, acc: 0.9296875, recall: 0.921875, precision: 0.9365079365079365, f_beta: 0.9291338582677166\n",
      "train: step: 774, loss: 0.24962416291236877, acc: 0.921875, recall: 0.875, precision: 0.9423076923076923, f_beta: 0.9074074074074073\n",
      "train: step: 775, loss: 0.3551269471645355, acc: 0.8359375, recall: 0.7941176470588235, precision: 0.8852459016393442, f_beta: 0.8372093023255813\n",
      "train: step: 776, loss: 0.259827196598053, acc: 0.8984375, recall: 0.86, precision: 0.8775510204081632, f_beta: 0.8686868686868686\n",
      "train: step: 777, loss: 0.2290181666612625, acc: 0.90625, recall: 0.8870967741935484, precision: 0.9166666666666666, f_beta: 0.9016393442622951\n",
      "train: step: 778, loss: 0.2568624019622803, acc: 0.890625, recall: 0.8235294117647058, precision: 0.8936170212765957, f_beta: 0.8571428571428571\n",
      "train: step: 779, loss: 0.2911612391471863, acc: 0.8828125, recall: 0.9014084507042254, precision: 0.8888888888888888, f_beta: 0.8951048951048951\n",
      "train: step: 780, loss: 0.3118970990180969, acc: 0.859375, recall: 0.8695652173913043, precision: 0.8695652173913043, f_beta: 0.8695652173913043\n",
      "start training model\n",
      "train: step: 781, loss: 0.2228865623474121, acc: 0.90625, recall: 0.9655172413793104, precision: 0.8484848484848485, f_beta: 0.9032258064516129\n",
      "train: step: 782, loss: 0.18453890085220337, acc: 0.9140625, recall: 0.9565217391304348, precision: 0.8918918918918919, f_beta: 0.9230769230769231\n",
      "train: step: 783, loss: 0.25245046615600586, acc: 0.875, recall: 0.9836065573770492, precision: 0.8, f_beta: 0.8823529411764706\n",
      "train: step: 784, loss: 0.2275450974702835, acc: 0.9140625, recall: 0.9863013698630136, precision: 0.8780487804878049, f_beta: 0.9290322580645162\n",
      "train: step: 785, loss: 0.25939276814460754, acc: 0.90625, recall: 0.9649122807017544, precision: 0.8461538461538461, f_beta: 0.9016393442622951\n",
      "train: step: 786, loss: 0.22258998453617096, acc: 0.9296875, recall: 0.9836065573770492, precision: 0.8823529411764706, f_beta: 0.9302325581395349\n",
      "train: step: 787, loss: 0.20345419645309448, acc: 0.8984375, recall: 0.8387096774193549, precision: 0.9454545454545454, f_beta: 0.8888888888888888\n",
      "train: step: 788, loss: 0.1449950933456421, acc: 0.9453125, recall: 0.9230769230769231, precision: 0.967741935483871, f_beta: 0.9448818897637796\n",
      "train: step: 789, loss: 0.31622210144996643, acc: 0.8828125, recall: 0.859375, precision: 0.9016393442622951, f_beta: 0.88\n",
      "train: step: 790, loss: 0.1936894655227661, acc: 0.9296875, recall: 0.9056603773584906, precision: 0.9230769230769231, f_beta: 0.9142857142857143\n",
      "train: step: 791, loss: 0.19775106012821198, acc: 0.9375, recall: 0.890625, precision: 0.9827586206896551, f_beta: 0.9344262295081966\n",
      "train: step: 792, loss: 0.1795673966407776, acc: 0.9375, recall: 0.9, precision: 0.984375, f_beta: 0.9402985074626866\n",
      "train: step: 793, loss: 0.20936548709869385, acc: 0.921875, recall: 0.8545454545454545, precision: 0.9591836734693877, f_beta: 0.9038461538461537\n",
      "train: step: 794, loss: 0.20410054922103882, acc: 0.9140625, recall: 0.9178082191780822, precision: 0.9305555555555556, f_beta: 0.9241379310344828\n",
      "train: step: 795, loss: 0.16550594568252563, acc: 0.9453125, recall: 0.9666666666666667, precision: 0.9206349206349206, f_beta: 0.943089430894309\n",
      "train: step: 796, loss: 0.12431144714355469, acc: 0.953125, recall: 0.9655172413793104, precision: 0.9333333333333333, f_beta: 0.9491525423728815\n",
      "train: step: 797, loss: 0.1751285046339035, acc: 0.9375, recall: 0.9642857142857143, precision: 0.9, f_beta: 0.9310344827586207\n",
      "train: step: 798, loss: 0.19104135036468506, acc: 0.9296875, recall: 0.9818181818181818, precision: 0.8709677419354839, f_beta: 0.923076923076923\n",
      "train: step: 799, loss: 0.21515068411827087, acc: 0.921875, recall: 0.9166666666666666, precision: 0.9166666666666666, f_beta: 0.9166666666666666\n",
      "train: step: 800, loss: 0.14993616938591003, acc: 0.9375, recall: 0.9642857142857143, precision: 0.9, f_beta: 0.9310344827586207\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T14:56:54.600499, step: 800, loss: 0.3732214936843285, acc: 0.8637820512820513,precision: 0.8818745362861404, recall: 0.8529322731423739, f_beta: 0.8662599318867867\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-800\n",
      "\n",
      "train: step: 801, loss: 0.1808452308177948, acc: 0.9296875, recall: 0.9285714285714286, precision: 0.9420289855072463, f_beta: 0.935251798561151\n",
      "train: step: 802, loss: 0.14583221077919006, acc: 0.9453125, recall: 0.9682539682539683, precision: 0.9242424242424242, f_beta: 0.9457364341085271\n",
      "train: step: 803, loss: 0.15797516703605652, acc: 0.9609375, recall: 0.9365079365079365, precision: 0.9833333333333333, f_beta: 0.9593495934959351\n",
      "train: step: 804, loss: 0.21528707444667816, acc: 0.9140625, recall: 0.8688524590163934, precision: 0.9464285714285714, f_beta: 0.9059829059829059\n",
      "train: step: 805, loss: 0.1910942792892456, acc: 0.9375, recall: 0.9180327868852459, precision: 0.9491525423728814, f_beta: 0.9333333333333333\n",
      "train: step: 806, loss: 0.15123602747917175, acc: 0.9453125, recall: 0.9436619718309859, precision: 0.9571428571428572, f_beta: 0.9503546099290779\n",
      "train: step: 807, loss: 0.1984173059463501, acc: 0.953125, recall: 0.896551724137931, precision: 1.0, f_beta: 0.9454545454545454\n",
      "train: step: 808, loss: 0.18932032585144043, acc: 0.9375, recall: 0.8888888888888888, precision: 0.96, f_beta: 0.923076923076923\n",
      "train: step: 809, loss: 0.1815340220928192, acc: 0.953125, recall: 0.9459459459459459, precision: 0.9722222222222222, f_beta: 0.9589041095890412\n",
      "train: step: 810, loss: 0.22010588645935059, acc: 0.8984375, recall: 0.9117647058823529, precision: 0.8985507246376812, f_beta: 0.9051094890510949\n",
      "train: step: 811, loss: 0.12158331274986267, acc: 0.9609375, recall: 0.9354838709677419, precision: 0.9830508474576272, f_beta: 0.9586776859504132\n",
      "train: step: 812, loss: 0.16303981840610504, acc: 0.9453125, recall: 0.9393939393939394, precision: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 813, loss: 0.22197088599205017, acc: 0.921875, recall: 0.9104477611940298, precision: 0.9384615384615385, f_beta: 0.9242424242424243\n",
      "train: step: 814, loss: 0.2228250801563263, acc: 0.890625, recall: 0.8571428571428571, precision: 0.8888888888888888, f_beta: 0.8727272727272727\n",
      "train: step: 815, loss: 0.21045750379562378, acc: 0.9140625, recall: 0.8833333333333333, precision: 0.9298245614035088, f_beta: 0.905982905982906\n",
      "train: step: 816, loss: 0.20957323908805847, acc: 0.921875, recall: 0.8983050847457628, precision: 0.9298245614035088, f_beta: 0.9137931034482759\n",
      "train: step: 817, loss: 0.18405331671237946, acc: 0.9375, recall: 0.9344262295081968, precision: 0.9344262295081968, f_beta: 0.9344262295081968\n",
      "train: step: 818, loss: 0.2942724823951721, acc: 0.8828125, recall: 0.9411764705882353, precision: 0.8533333333333334, f_beta: 0.8951048951048952\n",
      "train: step: 819, loss: 0.18380598723888397, acc: 0.9296875, recall: 0.9452054794520548, precision: 0.9324324324324325, f_beta: 0.9387755102040816\n",
      "train: step: 820, loss: 0.24637141823768616, acc: 0.90625, recall: 0.9464285714285714, precision: 0.8548387096774194, f_beta: 0.8983050847457628\n",
      "train: step: 821, loss: 0.16565842926502228, acc: 0.9375, recall: 0.953125, precision: 0.9242424242424242, f_beta: 0.9384615384615383\n",
      "train: step: 822, loss: 0.299311101436615, acc: 0.9140625, recall: 0.9365079365079365, precision: 0.8939393939393939, f_beta: 0.9147286821705426\n",
      "train: step: 823, loss: 0.33327245712280273, acc: 0.875, recall: 0.8939393939393939, precision: 0.8676470588235294, f_beta: 0.8805970149253731\n",
      "train: step: 824, loss: 0.11556664109230042, acc: 0.96875, recall: 0.9571428571428572, precision: 0.9852941176470589, f_beta: 0.9710144927536232\n",
      "train: step: 825, loss: 0.12919391691684723, acc: 0.9765625, recall: 0.9848484848484849, precision: 0.9701492537313433, f_beta: 0.9774436090225564\n",
      "train: step: 826, loss: 0.1815696358680725, acc: 0.921875, recall: 0.9508196721311475, precision: 0.8923076923076924, f_beta: 0.9206349206349206\n",
      "train: step: 827, loss: 0.25195175409317017, acc: 0.9296875, recall: 0.9041095890410958, precision: 0.9705882352941176, f_beta: 0.9361702127659575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 828, loss: 0.2058778703212738, acc: 0.9140625, recall: 0.9230769230769231, precision: 0.9090909090909091, f_beta: 0.9160305343511451\n",
      "train: step: 829, loss: 0.1405954509973526, acc: 0.953125, recall: 0.972972972972973, precision: 0.9473684210526315, f_beta: 0.9599999999999999\n",
      "train: step: 830, loss: 0.26885342597961426, acc: 0.921875, recall: 0.9558823529411765, precision: 0.9027777777777778, f_beta: 0.9285714285714286\n",
      "train: step: 831, loss: 0.1452949345111847, acc: 0.9609375, recall: 0.9818181818181818, precision: 0.9310344827586207, f_beta: 0.9557522123893805\n",
      "train: step: 832, loss: 0.15728113055229187, acc: 0.9609375, recall: 0.9452054794520548, precision: 0.9857142857142858, f_beta: 0.9650349650349651\n",
      "train: step: 833, loss: 0.31893905997276306, acc: 0.890625, recall: 0.8873239436619719, precision: 0.9130434782608695, f_beta: 0.9\n",
      "train: step: 834, loss: 0.15453198552131653, acc: 0.9375, recall: 0.9402985074626866, precision: 0.9402985074626866, f_beta: 0.9402985074626865\n",
      "train: step: 835, loss: 0.21715110540390015, acc: 0.9296875, recall: 0.984375, precision: 0.8873239436619719, f_beta: 0.9333333333333333\n",
      "train: step: 836, loss: 0.15949709713459015, acc: 0.9375, recall: 0.9444444444444444, precision: 0.9444444444444444, f_beta: 0.9444444444444444\n",
      "train: step: 837, loss: 0.30847254395484924, acc: 0.8671875, recall: 0.9014084507042254, precision: 0.8648648648648649, f_beta: 0.8827586206896552\n",
      "train: step: 838, loss: 0.13453736901283264, acc: 0.96875, recall: 0.9807692307692307, precision: 0.9444444444444444, f_beta: 0.9622641509433962\n",
      "train: step: 839, loss: 0.1765052080154419, acc: 0.9453125, recall: 0.9166666666666666, precision: 0.9649122807017544, f_beta: 0.9401709401709402\n",
      "train: step: 840, loss: 0.14848393201828003, acc: 0.953125, recall: 0.9402985074626866, precision: 0.9692307692307692, f_beta: 0.9545454545454547\n",
      "train: step: 841, loss: 0.21172936260700226, acc: 0.9296875, recall: 0.896551724137931, precision: 0.9454545454545454, f_beta: 0.920353982300885\n",
      "train: step: 842, loss: 0.1789831519126892, acc: 0.9453125, recall: 0.9516129032258065, precision: 0.9365079365079365, f_beta: 0.944\n",
      "train: step: 843, loss: 0.1967831403017044, acc: 0.9296875, recall: 0.9354838709677419, precision: 0.9206349206349206, f_beta: 0.9279999999999999\n",
      "train: step: 844, loss: 0.24357956647872925, acc: 0.9140625, recall: 0.9310344827586207, precision: 0.8852459016393442, f_beta: 0.9075630252100839\n",
      "train: step: 845, loss: 0.15010207891464233, acc: 0.9375, recall: 0.8787878787878788, precision: 1.0, f_beta: 0.9354838709677419\n",
      "train: step: 846, loss: 0.17878828942775726, acc: 0.953125, recall: 0.9090909090909091, precision: 1.0, f_beta: 0.9523809523809523\n",
      "train: step: 847, loss: 0.14977297186851501, acc: 0.9609375, recall: 0.9305555555555556, precision: 1.0, f_beta: 0.9640287769784173\n",
      "train: step: 848, loss: 0.21842047572135925, acc: 0.9296875, recall: 0.890625, precision: 0.9661016949152542, f_beta: 0.9268292682926829\n",
      "train: step: 849, loss: 0.2202387899160385, acc: 0.921875, recall: 0.9375, precision: 0.9090909090909091, f_beta: 0.923076923076923\n",
      "train: step: 850, loss: 0.25607550144195557, acc: 0.890625, recall: 0.90625, precision: 0.8787878787878788, f_beta: 0.8923076923076922\n",
      "train: step: 851, loss: 0.2414582222700119, acc: 0.90625, recall: 0.9523809523809523, precision: 0.8695652173913043, f_beta: 0.909090909090909\n",
      "train: step: 852, loss: 0.18187695741653442, acc: 0.9296875, recall: 0.9180327868852459, precision: 0.9333333333333333, f_beta: 0.9256198347107439\n",
      "train: step: 853, loss: 0.29123860597610474, acc: 0.9296875, recall: 0.8793103448275862, precision: 0.9622641509433962, f_beta: 0.9189189189189189\n",
      "train: step: 854, loss: 0.27386748790740967, acc: 0.890625, recall: 0.9047619047619048, precision: 0.8769230769230769, f_beta: 0.890625\n",
      "train: step: 855, loss: 0.14991387724876404, acc: 0.9453125, recall: 0.9344262295081968, precision: 0.95, f_beta: 0.9421487603305784\n",
      "train: step: 856, loss: 0.2599326968193054, acc: 0.921875, recall: 0.8923076923076924, precision: 0.9508196721311475, f_beta: 0.9206349206349206\n",
      "train: step: 857, loss: 0.1790570318698883, acc: 0.9375, recall: 0.9459459459459459, precision: 0.9459459459459459, f_beta: 0.9459459459459459\n",
      "train: step: 858, loss: 0.16089969873428345, acc: 0.953125, recall: 0.9285714285714286, precision: 0.9848484848484849, f_beta: 0.9558823529411765\n",
      "train: step: 859, loss: 0.24598968029022217, acc: 0.8984375, recall: 0.864406779661017, precision: 0.9107142857142857, f_beta: 0.8869565217391304\n",
      "train: step: 860, loss: 0.18394248187541962, acc: 0.9140625, recall: 0.9122807017543859, precision: 0.896551724137931, f_beta: 0.9043478260869565\n",
      "train: step: 861, loss: 0.2252606302499771, acc: 0.9296875, recall: 0.8805970149253731, precision: 0.9833333333333333, f_beta: 0.9291338582677166\n",
      "train: step: 862, loss: 0.099612757563591, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 863, loss: 0.22452746331691742, acc: 0.9140625, recall: 0.847457627118644, precision: 0.9615384615384616, f_beta: 0.9009009009009009\n",
      "train: step: 864, loss: 0.12456317991018295, acc: 0.96875, recall: 0.9830508474576272, precision: 0.9508196721311475, f_beta: 0.9666666666666667\n",
      "train: step: 865, loss: 0.29565829038619995, acc: 0.8984375, recall: 0.9193548387096774, precision: 0.8769230769230769, f_beta: 0.8976377952755904\n",
      "train: step: 866, loss: 0.23517940938472748, acc: 0.9140625, recall: 0.9846153846153847, precision: 0.8648648648648649, f_beta: 0.920863309352518\n",
      "train: step: 867, loss: 0.20588478446006775, acc: 0.921875, recall: 0.9552238805970149, precision: 0.9014084507042254, f_beta: 0.927536231884058\n",
      "train: step: 868, loss: 0.2655051350593567, acc: 0.9140625, recall: 0.9523809523809523, precision: 0.8823529411764706, f_beta: 0.916030534351145\n",
      "train: step: 869, loss: 0.2550307512283325, acc: 0.9140625, recall: 0.9821428571428571, precision: 0.8461538461538461, f_beta: 0.9090909090909091\n",
      "train: step: 870, loss: 0.2002771496772766, acc: 0.90625, recall: 0.9242424242424242, precision: 0.8970588235294118, f_beta: 0.9104477611940298\n",
      "train: step: 871, loss: 0.12465312331914902, acc: 0.953125, recall: 0.9552238805970149, precision: 0.9552238805970149, f_beta: 0.9552238805970149\n",
      "train: step: 872, loss: 0.1271369755268097, acc: 0.953125, recall: 0.9552238805970149, precision: 0.9552238805970149, f_beta: 0.9552238805970149\n",
      "train: step: 873, loss: 0.175101175904274, acc: 0.9375, recall: 0.8923076923076924, precision: 0.9830508474576272, f_beta: 0.9354838709677421\n",
      "train: step: 874, loss: 0.1521633267402649, acc: 0.953125, recall: 0.9047619047619048, precision: 1.0, f_beta: 0.9500000000000001\n",
      "train: step: 875, loss: 0.16205662488937378, acc: 0.9375, recall: 0.8939393939393939, precision: 0.9833333333333333, f_beta: 0.9365079365079364\n",
      "train: step: 876, loss: 0.17978748679161072, acc: 0.9375, recall: 0.9041095890410958, precision: 0.9850746268656716, f_beta: 0.9428571428571428\n",
      "train: step: 877, loss: 0.2446666955947876, acc: 0.9296875, recall: 0.9516129032258065, precision: 0.9076923076923077, f_beta: 0.9291338582677167\n",
      "train: step: 878, loss: 0.1994311809539795, acc: 0.9296875, recall: 0.8615384615384616, precision: 1.0, f_beta: 0.9256198347107438\n",
      "train: step: 879, loss: 0.14825928211212158, acc: 0.9453125, recall: 0.984375, precision: 0.9130434782608695, f_beta: 0.9473684210526315\n",
      "train: step: 880, loss: 0.20767027139663696, acc: 0.9140625, recall: 0.9342105263157895, precision: 0.922077922077922, f_beta: 0.9281045751633986\n",
      "train: step: 881, loss: 0.21916256844997406, acc: 0.9296875, recall: 0.9696969696969697, precision: 0.9014084507042254, f_beta: 0.9343065693430657\n",
      "train: step: 882, loss: 0.21917109191417694, acc: 0.9296875, recall: 0.9846153846153847, precision: 0.8888888888888888, f_beta: 0.9343065693430657\n",
      "train: step: 883, loss: 0.19794638454914093, acc: 0.9375, recall: 0.9180327868852459, precision: 0.9491525423728814, f_beta: 0.9333333333333333\n",
      "train: step: 884, loss: 0.23287628591060638, acc: 0.9140625, recall: 0.9272727272727272, precision: 0.8793103448275862, f_beta: 0.902654867256637\n",
      "train: step: 885, loss: 0.2619720995426178, acc: 0.9140625, recall: 0.9295774647887324, precision: 0.9166666666666666, f_beta: 0.9230769230769231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 886, loss: 0.2909656763076782, acc: 0.8828125, recall: 0.8142857142857143, precision: 0.9661016949152542, f_beta: 0.8837209302325583\n",
      "train: step: 887, loss: 0.13804960250854492, acc: 0.9453125, recall: 0.9454545454545454, precision: 0.9285714285714286, f_beta: 0.9369369369369368\n",
      "train: step: 888, loss: 0.15153968334197998, acc: 0.953125, recall: 0.9571428571428572, precision: 0.9571428571428572, f_beta: 0.9571428571428572\n",
      "train: step: 889, loss: 0.21125854551792145, acc: 0.9140625, recall: 0.8620689655172413, precision: 0.9433962264150944, f_beta: 0.9009009009009009\n",
      "train: step: 890, loss: 0.10371610522270203, acc: 0.9609375, recall: 0.9655172413793104, precision: 0.9491525423728814, f_beta: 0.9572649572649573\n",
      "train: step: 891, loss: 0.09738039970397949, acc: 0.96875, recall: 1.0, precision: 0.9322033898305084, f_beta: 0.9649122807017544\n",
      "train: step: 892, loss: 0.06949960440397263, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 893, loss: 0.175471693277359, acc: 0.9375, recall: 0.9285714285714286, precision: 0.9558823529411765, f_beta: 0.9420289855072465\n",
      "train: step: 894, loss: 0.22396332025527954, acc: 0.8984375, recall: 0.9, precision: 0.8852459016393442, f_beta: 0.8925619834710743\n",
      "train: step: 895, loss: 0.16083119809627533, acc: 0.9453125, recall: 0.9836065573770492, precision: 0.9090909090909091, f_beta: 0.9448818897637795\n",
      "train: step: 896, loss: 0.14358633756637573, acc: 0.9375, recall: 0.967741935483871, precision: 0.9090909090909091, f_beta: 0.9374999999999999\n",
      "train: step: 897, loss: 0.16260340809822083, acc: 0.9375, recall: 0.9393939393939394, precision: 0.9393939393939394, f_beta: 0.9393939393939394\n",
      "train: step: 898, loss: 0.1633804440498352, acc: 0.9609375, recall: 0.9655172413793104, precision: 0.9491525423728814, f_beta: 0.9572649572649573\n",
      "train: step: 899, loss: 0.30181053280830383, acc: 0.8828125, recall: 0.9032258064516129, precision: 0.8615384615384616, f_beta: 0.8818897637795274\n",
      "train: step: 900, loss: 0.14306163787841797, acc: 0.9453125, recall: 0.9565217391304348, precision: 0.9428571428571428, f_beta: 0.9496402877697843\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T15:00:43.442441, step: 900, loss: 0.3509512551319905, acc: 0.8619791666666666,precision: 0.8611961436957225, recall: 0.8638064723446343, f_beta: 0.8617450587636528\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-900\n",
      "\n",
      "train: step: 901, loss: 0.202314093708992, acc: 0.9375, recall: 0.9333333333333333, precision: 0.958904109589041, f_beta: 0.9459459459459458\n",
      "train: step: 902, loss: 0.1817057877779007, acc: 0.9375, recall: 0.95, precision: 0.9193548387096774, f_beta: 0.9344262295081968\n",
      "train: step: 903, loss: 0.1849079728126526, acc: 0.9453125, recall: 0.9206349206349206, precision: 0.9666666666666667, f_beta: 0.943089430894309\n",
      "train: step: 904, loss: 0.11418455839157104, acc: 0.96875, recall: 0.9322033898305084, precision: 1.0, f_beta: 0.9649122807017544\n",
      "train: step: 905, loss: 0.16760165989398956, acc: 0.9609375, recall: 0.9852941176470589, precision: 0.9436619718309859, f_beta: 0.9640287769784172\n",
      "train: step: 906, loss: 0.23561976850032806, acc: 0.9296875, recall: 0.9482758620689655, precision: 0.9016393442622951, f_beta: 0.9243697478991596\n",
      "train: step: 907, loss: 0.13454575836658478, acc: 0.96875, recall: 0.9705882352941176, precision: 0.9705882352941176, f_beta: 0.9705882352941176\n",
      "train: step: 908, loss: 0.24243462085723877, acc: 0.8984375, recall: 0.9473684210526315, precision: 0.84375, f_beta: 0.8925619834710744\n",
      "train: step: 909, loss: 0.15336400270462036, acc: 0.9453125, recall: 0.9393939393939394, precision: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 910, loss: 0.14106124639511108, acc: 0.9296875, recall: 0.953125, precision: 0.9104477611940298, f_beta: 0.931297709923664\n",
      "train: step: 911, loss: 0.16473080217838287, acc: 0.9296875, recall: 0.9402985074626866, precision: 0.9264705882352942, f_beta: 0.9333333333333335\n",
      "train: step: 912, loss: 0.11696800589561462, acc: 0.96875, recall: 0.967741935483871, precision: 0.967741935483871, f_beta: 0.967741935483871\n",
      "train: step: 913, loss: 0.19020292162895203, acc: 0.9453125, recall: 0.9464285714285714, precision: 0.9298245614035088, f_beta: 0.9380530973451328\n",
      "train: step: 914, loss: 0.15662571787834167, acc: 0.9453125, recall: 0.9454545454545454, precision: 0.9285714285714286, f_beta: 0.9369369369369368\n",
      "train: step: 915, loss: 0.2828325033187866, acc: 0.8984375, recall: 0.9264705882352942, precision: 0.8873239436619719, f_beta: 0.906474820143885\n",
      "train: step: 916, loss: 0.2314513921737671, acc: 0.8984375, recall: 0.85, precision: 0.9272727272727272, f_beta: 0.8869565217391303\n",
      "train: step: 917, loss: 0.1830468773841858, acc: 0.9453125, recall: 0.9285714285714286, precision: 0.9701492537313433, f_beta: 0.948905109489051\n",
      "train: step: 918, loss: 0.18559212982654572, acc: 0.921875, recall: 0.9, precision: 0.9310344827586207, f_beta: 0.9152542372881356\n",
      "train: step: 919, loss: 0.24189777672290802, acc: 0.90625, recall: 0.8611111111111112, precision: 0.96875, f_beta: 0.911764705882353\n",
      "train: step: 920, loss: 0.1762603223323822, acc: 0.8984375, recall: 0.8703703703703703, precision: 0.8867924528301887, f_beta: 0.8785046728971964\n",
      "train: step: 921, loss: 0.20058727264404297, acc: 0.9296875, recall: 0.9206349206349206, precision: 0.9354838709677419, f_beta: 0.9279999999999999\n",
      "train: step: 922, loss: 0.16325540840625763, acc: 0.9453125, recall: 0.9375, precision: 0.9183673469387755, f_beta: 0.9278350515463918\n",
      "train: step: 923, loss: 0.15389055013656616, acc: 0.953125, recall: 0.958904109589041, precision: 0.958904109589041, f_beta: 0.958904109589041\n",
      "train: step: 924, loss: 0.22175511717796326, acc: 0.9296875, recall: 0.9705882352941176, precision: 0.9041095890410958, f_beta: 0.9361702127659575\n",
      "train: step: 925, loss: 0.20456385612487793, acc: 0.921875, recall: 0.967741935483871, precision: 0.8823529411764706, f_beta: 0.923076923076923\n",
      "train: step: 926, loss: 0.23067313432693481, acc: 0.90625, recall: 1.0, precision: 0.8481012658227848, f_beta: 0.9178082191780821\n",
      "train: step: 927, loss: 0.4038116931915283, acc: 0.8203125, recall: 1.0, precision: 0.7415730337078652, f_beta: 0.8516129032258065\n",
      "train: step: 928, loss: 0.3343343436717987, acc: 0.84375, recall: 1.0, precision: 0.7402597402597403, f_beta: 0.8507462686567165\n",
      "train: step: 929, loss: 0.34940043091773987, acc: 0.8125, recall: 0.9705882352941176, precision: 0.75, f_beta: 0.846153846153846\n",
      "train: step: 930, loss: 0.2776622474193573, acc: 0.8828125, recall: 0.984375, precision: 0.8181818181818182, f_beta: 0.8936170212765957\n",
      "train: step: 931, loss: 0.19065335392951965, acc: 0.9375, recall: 0.9305555555555556, precision: 0.9571428571428572, f_beta: 0.943661971830986\n",
      "train: step: 932, loss: 0.21510057151317596, acc: 0.9609375, recall: 0.9545454545454546, precision: 0.9692307692307692, f_beta: 0.9618320610687022\n",
      "train: step: 933, loss: 0.19956611096858978, acc: 0.9375, recall: 0.9, precision: 0.984375, f_beta: 0.9402985074626866\n",
      "train: step: 934, loss: 0.31452029943466187, acc: 0.8984375, recall: 0.8088235294117647, precision: 1.0, f_beta: 0.8943089430894309\n",
      "train: step: 935, loss: 0.24479147791862488, acc: 0.9140625, recall: 0.8615384615384616, precision: 0.9655172413793104, f_beta: 0.9105691056910569\n",
      "train: step: 936, loss: 0.3077666163444519, acc: 0.859375, recall: 0.8169014084507042, precision: 0.9206349206349206, f_beta: 0.8656716417910447\n",
      "start training model\n",
      "train: step: 937, loss: 0.18583983182907104, acc: 0.9453125, recall: 0.90625, precision: 0.9830508474576272, f_beta: 0.943089430894309\n",
      "train: step: 938, loss: 0.1333760768175125, acc: 0.984375, recall: 0.9830508474576272, precision: 0.9830508474576272, f_beta: 0.9830508474576272\n",
      "train: step: 939, loss: 0.1737280637025833, acc: 0.953125, recall: 0.9384615384615385, precision: 0.9682539682539683, f_beta: 0.953125\n",
      "train: step: 940, loss: 0.20001697540283203, acc: 0.9140625, recall: 0.9322033898305084, precision: 0.8870967741935484, f_beta: 0.9090909090909092\n",
      "train: step: 941, loss: 0.1565212607383728, acc: 0.953125, recall: 0.9696969696969697, precision: 0.9411764705882353, f_beta: 0.955223880597015\n",
      "train: step: 942, loss: 0.19028843939304352, acc: 0.953125, recall: 0.9701492537313433, precision: 0.9420289855072463, f_beta: 0.9558823529411764\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 943, loss: 0.20600153505802155, acc: 0.953125, recall: 0.9841269841269841, precision: 0.9253731343283582, f_beta: 0.9538461538461538\n",
      "train: step: 944, loss: 0.1698015034198761, acc: 0.9375, recall: 0.9333333333333333, precision: 0.9333333333333333, f_beta: 0.9333333333333333\n",
      "train: step: 945, loss: 0.1746782660484314, acc: 0.9453125, recall: 0.9090909090909091, precision: 0.9615384615384616, f_beta: 0.9345794392523366\n",
      "train: step: 946, loss: 0.18093520402908325, acc: 0.9296875, recall: 0.9, precision: 0.9692307692307692, f_beta: 0.9333333333333333\n",
      "train: step: 947, loss: 0.13508766889572144, acc: 0.9453125, recall: 0.9154929577464789, precision: 0.9848484848484849, f_beta: 0.948905109489051\n",
      "train: step: 948, loss: 0.26240286231040955, acc: 0.9140625, recall: 0.8888888888888888, precision: 0.9333333333333333, f_beta: 0.9105691056910569\n",
      "train: step: 949, loss: 0.20582230389118195, acc: 0.9453125, recall: 0.921875, precision: 0.9672131147540983, f_beta: 0.944\n",
      "train: step: 950, loss: 0.16140493750572205, acc: 0.9375, recall: 0.9384615384615385, precision: 0.9384615384615385, f_beta: 0.9384615384615385\n",
      "train: step: 951, loss: 0.13779282569885254, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
      "train: step: 952, loss: 0.10423204302787781, acc: 0.9609375, recall: 0.9384615384615385, precision: 0.9838709677419355, f_beta: 0.9606299212598426\n",
      "train: step: 953, loss: 0.12539243698120117, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 954, loss: 0.1228073462843895, acc: 0.96875, recall: 0.9428571428571428, precision: 1.0, f_beta: 0.9705882352941176\n",
      "train: step: 955, loss: 0.2489284723997116, acc: 0.9296875, recall: 0.9310344827586207, precision: 0.9152542372881356, f_beta: 0.923076923076923\n",
      "train: step: 956, loss: 0.1749136596918106, acc: 0.921875, recall: 0.9375, precision: 0.9090909090909091, f_beta: 0.923076923076923\n",
      "train: step: 957, loss: 0.1288861334323883, acc: 0.953125, recall: 0.9855072463768116, precision: 0.9315068493150684, f_beta: 0.9577464788732394\n",
      "train: step: 958, loss: 0.17728570103645325, acc: 0.9609375, recall: 0.9625, precision: 0.9746835443037974, f_beta: 0.9685534591194969\n",
      "train: step: 959, loss: 0.16176019608974457, acc: 0.9453125, recall: 0.9655172413793104, precision: 0.9180327868852459, f_beta: 0.9411764705882353\n",
      "train: step: 960, loss: 0.19688773155212402, acc: 0.9453125, recall: 0.9841269841269841, precision: 0.9117647058823529, f_beta: 0.9465648854961831\n",
      "train: step: 961, loss: 0.16583898663520813, acc: 0.9453125, recall: 0.9305555555555556, precision: 0.9710144927536232, f_beta: 0.9503546099290779\n",
      "train: step: 962, loss: 0.2018820196390152, acc: 0.90625, recall: 0.8970588235294118, precision: 0.9242424242424242, f_beta: 0.9104477611940298\n",
      "train: step: 963, loss: 0.19325138628482819, acc: 0.9453125, recall: 0.9047619047619048, precision: 0.9827586206896551, f_beta: 0.9421487603305785\n",
      "train: step: 964, loss: 0.18104293942451477, acc: 0.9296875, recall: 0.9047619047619048, precision: 0.95, f_beta: 0.9268292682926829\n",
      "train: step: 965, loss: 0.10186345875263214, acc: 0.9609375, recall: 0.9661016949152542, precision: 0.95, f_beta: 0.957983193277311\n",
      "train: step: 966, loss: 0.055002130568027496, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 967, loss: 0.06003459915518761, acc: 0.9765625, recall: 0.9818181818181818, precision: 0.9642857142857143, f_beta: 0.972972972972973\n",
      "train: step: 968, loss: 0.1654316782951355, acc: 0.9453125, recall: 0.9206349206349206, precision: 0.9666666666666667, f_beta: 0.943089430894309\n",
      "train: step: 969, loss: 0.08584775030612946, acc: 0.96875, recall: 0.984375, precision: 0.9545454545454546, f_beta: 0.9692307692307692\n",
      "train: step: 970, loss: 0.0983925387263298, acc: 0.9609375, recall: 0.9655172413793104, precision: 0.9491525423728814, f_beta: 0.9572649572649573\n",
      "train: step: 971, loss: 0.20324572920799255, acc: 0.9453125, recall: 0.9672131147540983, precision: 0.921875, f_beta: 0.944\n",
      "train: step: 972, loss: 0.20578822493553162, acc: 0.9375, recall: 0.9672131147540983, precision: 0.9076923076923077, f_beta: 0.9365079365079365\n",
      "train: step: 973, loss: 0.085789754986763, acc: 0.953125, recall: 1.0, precision: 0.9104477611940298, f_beta: 0.953125\n",
      "train: step: 974, loss: 0.11465643346309662, acc: 0.9609375, recall: 0.9824561403508771, precision: 0.9333333333333333, f_beta: 0.9572649572649572\n",
      "train: step: 975, loss: 0.1243380680680275, acc: 0.953125, recall: 0.9016393442622951, precision: 1.0, f_beta: 0.9482758620689655\n",
      "train: step: 976, loss: 0.20696856081485748, acc: 0.9375, recall: 0.8840579710144928, precision: 1.0, f_beta: 0.9384615384615386\n",
      "train: step: 977, loss: 0.08831934630870819, acc: 0.9765625, recall: 0.9538461538461539, precision: 1.0, f_beta: 0.9763779527559054\n",
      "train: step: 978, loss: 0.12294425070285797, acc: 0.9609375, recall: 0.9402985074626866, precision: 0.984375, f_beta: 0.9618320610687023\n",
      "train: step: 979, loss: 0.077695332467556, acc: 0.9765625, recall: 0.9859154929577465, precision: 0.9722222222222222, f_beta: 0.979020979020979\n",
      "train: step: 980, loss: 0.15536236763000488, acc: 0.9609375, recall: 0.9714285714285714, precision: 0.9577464788732394, f_beta: 0.9645390070921985\n",
      "train: step: 981, loss: 0.14038527011871338, acc: 0.9453125, recall: 0.96875, precision: 0.9253731343283582, f_beta: 0.9465648854961832\n",
      "train: step: 982, loss: 0.12242759764194489, acc: 0.953125, recall: 0.9411764705882353, precision: 0.9696969696969697, f_beta: 0.955223880597015\n",
      "train: step: 983, loss: 0.13638663291931152, acc: 0.96875, recall: 0.9859154929577465, precision: 0.958904109589041, f_beta: 0.9722222222222222\n",
      "train: step: 984, loss: 0.14145956933498383, acc: 0.96875, recall: 0.9672131147540983, precision: 0.9672131147540983, f_beta: 0.9672131147540983\n",
      "train: step: 985, loss: 0.13895507156848907, acc: 0.9609375, recall: 0.9714285714285714, precision: 0.9577464788732394, f_beta: 0.9645390070921985\n",
      "train: step: 986, loss: 0.09832322597503662, acc: 0.9609375, recall: 0.967741935483871, precision: 0.9523809523809523, f_beta: 0.96\n",
      "train: step: 987, loss: 0.05571579933166504, acc: 0.984375, recall: 0.9836065573770492, precision: 0.9836065573770492, f_beta: 0.9836065573770492\n",
      "train: step: 988, loss: 0.1573622077703476, acc: 0.9453125, recall: 0.9333333333333333, precision: 0.9491525423728814, f_beta: 0.9411764705882353\n",
      "train: step: 989, loss: 0.15242376923561096, acc: 0.9453125, recall: 0.9180327868852459, precision: 0.9655172413793104, f_beta: 0.9411764705882353\n",
      "train: step: 990, loss: 0.11615175008773804, acc: 0.9765625, recall: 0.9672131147540983, precision: 0.9833333333333333, f_beta: 0.9752066115702478\n",
      "train: step: 991, loss: 0.17781352996826172, acc: 0.953125, recall: 0.9122807017543859, precision: 0.9811320754716981, f_beta: 0.9454545454545454\n",
      "train: step: 992, loss: 0.10525187849998474, acc: 0.9609375, recall: 0.9315068493150684, precision: 1.0, f_beta: 0.9645390070921985\n",
      "train: step: 993, loss: 0.12635336816310883, acc: 0.96875, recall: 0.9393939393939394, precision: 1.0, f_beta: 0.96875\n",
      "train: step: 994, loss: 0.16038288176059723, acc: 0.9453125, recall: 0.9253731343283582, precision: 0.96875, f_beta: 0.9465648854961832\n",
      "train: step: 995, loss: 0.10099896043539047, acc: 0.9765625, recall: 0.9830508474576272, precision: 0.9666666666666667, f_beta: 0.9747899159663865\n",
      "train: step: 996, loss: 0.15934249758720398, acc: 0.953125, recall: 0.9538461538461539, precision: 0.9538461538461539, f_beta: 0.9538461538461539\n",
      "train: step: 997, loss: 0.06291425228118896, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 998, loss: 0.19297350943088531, acc: 0.9375, recall: 0.9305555555555556, precision: 0.9571428571428572, f_beta: 0.943661971830986\n",
      "train: step: 999, loss: 0.13476359844207764, acc: 0.953125, recall: 0.9393939393939394, precision: 0.96875, f_beta: 0.9538461538461539\n",
      "train: step: 1000, loss: 0.15878629684448242, acc: 0.9296875, recall: 0.8983050847457628, precision: 0.9464285714285714, f_beta: 0.9217391304347826\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T15:04:33.113200, step: 1000, loss: 0.41330493566317433, acc: 0.8701923076923077,precision: 0.8910874978596525, recall: 0.8563791593862189, f_beta: 0.8725868429185909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-1000\n",
      "\n",
      "train: step: 1001, loss: 0.25436484813690186, acc: 0.9140625, recall: 0.9538461538461539, precision: 0.8857142857142857, f_beta: 0.9185185185185185\n",
      "train: step: 1002, loss: 0.11616646498441696, acc: 0.96875, recall: 0.9344262295081968, precision: 1.0, f_beta: 0.9661016949152543\n",
      "train: step: 1003, loss: 0.1820349395275116, acc: 0.9375, recall: 0.9444444444444444, precision: 0.9444444444444444, f_beta: 0.9444444444444444\n",
      "train: step: 1004, loss: 0.21668481826782227, acc: 0.9453125, recall: 0.9666666666666667, precision: 0.9206349206349206, f_beta: 0.943089430894309\n",
      "train: step: 1005, loss: 0.12134844809770584, acc: 0.9453125, recall: 0.8923076923076924, precision: 1.0, f_beta: 0.9430894308943091\n",
      "train: step: 1006, loss: 0.16700828075408936, acc: 0.9296875, recall: 0.9047619047619048, precision: 0.95, f_beta: 0.9268292682926829\n",
      "train: step: 1007, loss: 0.09102118015289307, acc: 0.953125, recall: 0.9310344827586207, precision: 0.9642857142857143, f_beta: 0.9473684210526316\n",
      "train: step: 1008, loss: 0.141306072473526, acc: 0.96875, recall: 0.9655172413793104, precision: 0.9655172413793104, f_beta: 0.9655172413793104\n",
      "train: step: 1009, loss: 0.16039712727069855, acc: 0.9453125, recall: 0.9393939393939394, precision: 0.9538461538461539, f_beta: 0.9465648854961831\n",
      "train: step: 1010, loss: 0.17060382664203644, acc: 0.9609375, recall: 0.9491525423728814, precision: 0.9655172413793104, f_beta: 0.9572649572649573\n",
      "train: step: 1011, loss: 0.0966760590672493, acc: 0.9765625, recall: 1.0, precision: 0.9508196721311475, f_beta: 0.9747899159663865\n",
      "train: step: 1012, loss: 0.08015458285808563, acc: 0.984375, recall: 1.0, precision: 0.9722222222222222, f_beta: 0.9859154929577464\n",
      "train: step: 1013, loss: 0.25237226486206055, acc: 0.90625, recall: 0.9272727272727272, precision: 0.864406779661017, f_beta: 0.8947368421052632\n",
      "train: step: 1014, loss: 0.13225264847278595, acc: 0.953125, recall: 0.967741935483871, precision: 0.9375, f_beta: 0.9523809523809523\n",
      "train: step: 1015, loss: 0.09435160458087921, acc: 0.9609375, recall: 0.9846153846153847, precision: 0.9411764705882353, f_beta: 0.962406015037594\n",
      "train: step: 1016, loss: 0.08976675570011139, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 1017, loss: 0.12836378812789917, acc: 0.96875, recall: 1.0, precision: 0.9310344827586207, f_beta: 0.9642857142857143\n",
      "train: step: 1018, loss: 0.0883297249674797, acc: 0.96875, recall: 0.9464285714285714, precision: 0.9814814814814815, f_beta: 0.9636363636363636\n",
      "train: step: 1019, loss: 0.18271490931510925, acc: 0.9453125, recall: 0.9344262295081968, precision: 0.95, f_beta: 0.9421487603305784\n",
      "train: step: 1020, loss: 0.19487740099430084, acc: 0.9375, recall: 0.9130434782608695, precision: 0.9692307692307692, f_beta: 0.9402985074626865\n",
      "train: step: 1021, loss: 0.1360449194908142, acc: 0.9453125, recall: 0.9605263157894737, precision: 0.948051948051948, f_beta: 0.9542483660130718\n",
      "train: step: 1022, loss: 0.1724638044834137, acc: 0.9296875, recall: 0.9041095890410958, precision: 0.9705882352941176, f_beta: 0.9361702127659575\n",
      "train: step: 1023, loss: 0.1620214283466339, acc: 0.9296875, recall: 0.9193548387096774, precision: 0.9344262295081968, f_beta: 0.9268292682926829\n",
      "train: step: 1024, loss: 0.13524959981441498, acc: 0.9453125, recall: 0.9827586206896551, precision: 0.9047619047619048, f_beta: 0.9421487603305785\n",
      "train: step: 1025, loss: 0.09274578094482422, acc: 0.96875, recall: 0.984375, precision: 0.9545454545454546, f_beta: 0.9692307692307692\n",
      "train: step: 1026, loss: 0.15866857767105103, acc: 0.9453125, recall: 1.0, precision: 0.9027777777777778, f_beta: 0.9489051094890512\n",
      "train: step: 1027, loss: 0.3047393560409546, acc: 0.90625, recall: 0.9852941176470589, precision: 0.8589743589743589, f_beta: 0.9178082191780821\n",
      "train: step: 1028, loss: 0.14578384160995483, acc: 0.953125, recall: 0.9868421052631579, precision: 0.9375, f_beta: 0.9615384615384615\n",
      "train: step: 1029, loss: 0.14055228233337402, acc: 0.984375, recall: 0.9836065573770492, precision: 0.9836065573770492, f_beta: 0.9836065573770492\n",
      "train: step: 1030, loss: 0.1521519273519516, acc: 0.9375, recall: 0.9538461538461539, precision: 0.9253731343283582, f_beta: 0.9393939393939394\n",
      "train: step: 1031, loss: 0.10637101531028748, acc: 0.9765625, recall: 0.9841269841269841, precision: 0.96875, f_beta: 0.9763779527559054\n",
      "train: step: 1032, loss: 0.17973504960536957, acc: 0.953125, recall: 0.9166666666666666, precision: 1.0, f_beta: 0.9565217391304348\n",
      "train: step: 1033, loss: 0.1798764169216156, acc: 0.9453125, recall: 0.9122807017543859, precision: 0.9629629629629629, f_beta: 0.9369369369369369\n",
      "train: step: 1034, loss: 0.1427019238471985, acc: 0.9375, recall: 0.8840579710144928, precision: 1.0, f_beta: 0.9384615384615386\n",
      "train: step: 1035, loss: 0.19587068259716034, acc: 0.9453125, recall: 0.9230769230769231, precision: 0.967741935483871, f_beta: 0.9448818897637796\n",
      "train: step: 1036, loss: 0.12211193144321442, acc: 0.9609375, recall: 0.953125, precision: 0.9682539682539683, f_beta: 0.9606299212598425\n",
      "train: step: 1037, loss: 0.09999418258666992, acc: 0.96875, recall: 0.9558823529411765, precision: 0.9848484848484849, f_beta: 0.9701492537313432\n",
      "train: step: 1038, loss: 0.07668519020080566, acc: 0.9765625, recall: 1.0, precision: 0.95, f_beta: 0.9743589743589743\n",
      "train: step: 1039, loss: 0.15532855689525604, acc: 0.9609375, recall: 0.9818181818181818, precision: 0.9310344827586207, f_beta: 0.9557522123893805\n",
      "train: step: 1040, loss: 0.14300629496574402, acc: 0.96875, recall: 0.9848484848484849, precision: 0.9558823529411765, f_beta: 0.9701492537313432\n",
      "train: step: 1041, loss: 0.09326901286840439, acc: 0.9765625, recall: 0.9833333333333333, precision: 0.9672131147540983, f_beta: 0.9752066115702478\n",
      "train: step: 1042, loss: 0.14414237439632416, acc: 0.9609375, recall: 0.9661016949152542, precision: 0.95, f_beta: 0.957983193277311\n",
      "train: step: 1043, loss: 0.13887175917625427, acc: 0.9609375, recall: 0.9701492537313433, precision: 0.9558823529411765, f_beta: 0.962962962962963\n",
      "train: step: 1044, loss: 0.16810360550880432, acc: 0.9375, recall: 0.921875, precision: 0.9516129032258065, f_beta: 0.9365079365079365\n",
      "train: step: 1045, loss: 0.12757086753845215, acc: 0.9609375, recall: 0.9838709677419355, precision: 0.9384615384615385, f_beta: 0.9606299212598426\n",
      "train: step: 1046, loss: 0.0741901844739914, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1047, loss: 0.20235338807106018, acc: 0.9140625, recall: 0.8823529411764706, precision: 0.9523809523809523, f_beta: 0.916030534351145\n",
      "train: step: 1048, loss: 0.08662430942058563, acc: 0.984375, recall: 1.0, precision: 0.9726027397260274, f_beta: 0.9861111111111112\n",
      "train: step: 1049, loss: 0.20479345321655273, acc: 0.9453125, recall: 0.9104477611940298, precision: 0.9838709677419355, f_beta: 0.9457364341085271\n",
      "train: step: 1050, loss: 0.15065467357635498, acc: 0.953125, recall: 0.9365079365079365, precision: 0.9672131147540983, f_beta: 0.9516129032258064\n",
      "train: step: 1051, loss: 0.12891802191734314, acc: 0.9453125, recall: 0.9365079365079365, precision: 0.9516129032258065, f_beta: 0.944\n",
      "train: step: 1052, loss: 0.09173162281513214, acc: 0.96875, recall: 0.9838709677419355, precision: 0.953125, f_beta: 0.9682539682539683\n",
      "train: step: 1053, loss: 0.17334923148155212, acc: 0.9453125, recall: 0.9558823529411765, precision: 0.9420289855072463, f_beta: 0.9489051094890512\n",
      "train: step: 1054, loss: 0.1859220266342163, acc: 0.9453125, recall: 0.9516129032258065, precision: 0.9365079365079365, f_beta: 0.944\n",
      "train: step: 1055, loss: 0.11100545525550842, acc: 0.9765625, recall: 1.0, precision: 0.961038961038961, f_beta: 0.9801324503311257\n",
      "train: step: 1056, loss: 0.08063580840826035, acc: 0.9765625, recall: 1.0, precision: 0.953125, f_beta: 0.976\n",
      "train: step: 1057, loss: 0.07096365839242935, acc: 0.984375, recall: 0.9850746268656716, precision: 0.9850746268656716, f_beta: 0.9850746268656716\n",
      "train: step: 1058, loss: 0.16739851236343384, acc: 0.953125, recall: 0.9459459459459459, precision: 0.9722222222222222, f_beta: 0.9589041095890412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1059, loss: 0.12385660409927368, acc: 0.9609375, recall: 0.9393939393939394, precision: 0.9841269841269841, f_beta: 0.9612403100775193\n",
      "train: step: 1060, loss: 0.11834560334682465, acc: 0.9453125, recall: 0.9827586206896551, precision: 0.9047619047619048, f_beta: 0.9421487603305785\n",
      "train: step: 1061, loss: 0.07376015931367874, acc: 0.9765625, recall: 0.984375, precision: 0.9692307692307692, f_beta: 0.9767441860465116\n",
      "train: step: 1062, loss: 0.1115548312664032, acc: 0.9609375, recall: 0.9692307692307692, precision: 0.9545454545454546, f_beta: 0.9618320610687022\n",
      "train: step: 1063, loss: 0.12668786942958832, acc: 0.953125, recall: 0.9848484848484849, precision: 0.9285714285714286, f_beta: 0.9558823529411765\n",
      "train: step: 1064, loss: 0.15915589034557343, acc: 0.9296875, recall: 0.8688524590163934, precision: 0.9814814814814815, f_beta: 0.9217391304347826\n",
      "train: step: 1065, loss: 0.11531025171279907, acc: 0.9765625, recall: 1.0, precision: 0.9565217391304348, f_beta: 0.9777777777777777\n",
      "train: step: 1066, loss: 0.16740286350250244, acc: 0.921875, recall: 0.8688524590163934, precision: 0.9636363636363636, f_beta: 0.9137931034482758\n",
      "train: step: 1067, loss: 0.13723017275333405, acc: 0.953125, recall: 0.9242424242424242, precision: 0.9838709677419355, f_beta: 0.9531249999999999\n",
      "train: step: 1068, loss: 0.1761658489704132, acc: 0.9375, recall: 0.918918918918919, precision: 0.9714285714285714, f_beta: 0.9444444444444445\n",
      "train: step: 1069, loss: 0.14392967522144318, acc: 0.953125, recall: 0.921875, precision: 0.9833333333333333, f_beta: 0.9516129032258064\n",
      "train: step: 1070, loss: 0.14633354544639587, acc: 0.953125, recall: 0.9655172413793104, precision: 0.9333333333333333, f_beta: 0.9491525423728815\n",
      "train: step: 1071, loss: 0.06034000962972641, acc: 0.9765625, recall: 0.9836065573770492, precision: 0.967741935483871, f_beta: 0.975609756097561\n",
      "train: step: 1072, loss: 0.12593328952789307, acc: 0.953125, recall: 0.9841269841269841, precision: 0.9253731343283582, f_beta: 0.9538461538461538\n",
      "train: step: 1073, loss: 0.17319980263710022, acc: 0.9296875, recall: 0.9850746268656716, precision: 0.8918918918918919, f_beta: 0.9361702127659575\n",
      "train: step: 1074, loss: 0.16494205594062805, acc: 0.9453125, recall: 0.9672131147540983, precision: 0.921875, f_beta: 0.944\n",
      "train: step: 1075, loss: 0.19226765632629395, acc: 0.9375, recall: 0.9622641509433962, precision: 0.8947368421052632, f_beta: 0.9272727272727272\n",
      "train: step: 1076, loss: 0.17407220602035522, acc: 0.953125, recall: 0.9629629629629629, precision: 0.9285714285714286, f_beta: 0.9454545454545454\n",
      "train: step: 1077, loss: 0.28191542625427246, acc: 0.8828125, recall: 0.8857142857142857, precision: 0.8985507246376812, f_beta: 0.8920863309352518\n",
      "train: step: 1078, loss: 0.19472633302211761, acc: 0.9140625, recall: 0.8769230769230769, precision: 0.95, f_beta: 0.912\n",
      "train: step: 1079, loss: 0.11206042021512985, acc: 0.953125, recall: 0.9310344827586207, precision: 0.9642857142857143, f_beta: 0.9473684210526316\n",
      "train: step: 1080, loss: 0.15070629119873047, acc: 0.9453125, recall: 0.9298245614035088, precision: 0.9464285714285714, f_beta: 0.9380530973451328\n",
      "train: step: 1081, loss: 0.11924511939287186, acc: 0.9453125, recall: 0.9076923076923077, precision: 0.9833333333333333, f_beta: 0.944\n",
      "train: step: 1082, loss: 0.1644243448972702, acc: 0.953125, recall: 0.9701492537313433, precision: 0.9420289855072463, f_beta: 0.9558823529411764\n",
      "train: step: 1083, loss: 0.14160498976707458, acc: 0.953125, recall: 0.9454545454545454, precision: 0.9454545454545454, f_beta: 0.9454545454545454\n",
      "train: step: 1084, loss: 0.1076396182179451, acc: 0.96875, recall: 0.9523809523809523, precision: 0.9836065573770492, f_beta: 0.9677419354838709\n",
      "train: step: 1085, loss: 0.05435793101787567, acc: 0.984375, recall: 1.0, precision: 0.9649122807017544, f_beta: 0.9821428571428572\n",
      "train: step: 1086, loss: 0.20489677786827087, acc: 0.9375, recall: 0.9344262295081968, precision: 0.9344262295081968, f_beta: 0.9344262295081968\n",
      "train: step: 1087, loss: 0.12791737914085388, acc: 0.9375, recall: 0.9833333333333333, precision: 0.8939393939393939, f_beta: 0.9365079365079364\n",
      "train: step: 1088, loss: 0.1691157966852188, acc: 0.9453125, recall: 0.9402985074626866, precision: 0.9545454545454546, f_beta: 0.9473684210526316\n",
      "train: step: 1089, loss: 0.30516430735588074, acc: 0.90625, recall: 0.9047619047619048, precision: 0.9047619047619048, f_beta: 0.9047619047619048\n",
      "train: step: 1090, loss: 0.0735006332397461, acc: 0.9765625, recall: 0.9857142857142858, precision: 0.971830985915493, f_beta: 0.9787234042553192\n",
      "train: step: 1091, loss: 0.09863659739494324, acc: 0.96875, recall: 0.9841269841269841, precision: 0.9538461538461539, f_beta: 0.96875\n",
      "train: step: 1092, loss: 0.1808481514453888, acc: 0.9296875, recall: 0.9354838709677419, precision: 0.9206349206349206, f_beta: 0.9279999999999999\n",
      "start training model\n",
      "train: step: 1093, loss: 0.1403222680091858, acc: 0.953125, recall: 0.9365079365079365, precision: 0.9672131147540983, f_beta: 0.9516129032258064\n",
      "train: step: 1094, loss: 0.10732671618461609, acc: 0.9765625, recall: 0.9838709677419355, precision: 0.9682539682539683, f_beta: 0.976\n",
      "train: step: 1095, loss: 0.0755351334810257, acc: 0.96875, recall: 1.0, precision: 0.9354838709677419, f_beta: 0.9666666666666666\n",
      "train: step: 1096, loss: 0.10711383074522018, acc: 0.9765625, recall: 0.9836065573770492, precision: 0.967741935483871, f_beta: 0.975609756097561\n",
      "train: step: 1097, loss: 0.046918462961912155, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1098, loss: 0.055179014801979065, acc: 0.9921875, recall: 0.9836065573770492, precision: 1.0, f_beta: 0.9917355371900827\n",
      "train: step: 1099, loss: 0.16250643134117126, acc: 0.9453125, recall: 0.9682539682539683, precision: 0.9242424242424242, f_beta: 0.9457364341085271\n",
      "train: step: 1100, loss: 0.12381689250469208, acc: 0.9609375, recall: 0.9393939393939394, precision: 0.9841269841269841, f_beta: 0.9612403100775193\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T15:08:22.402387, step: 1100, loss: 0.38566916455061007, acc: 0.8665865384615384,precision: 0.8534134996629433, recall: 0.8811748644120496, f_beta: 0.8657027728346812\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-1100\n",
      "\n",
      "train: step: 1101, loss: 0.13452409207820892, acc: 0.9765625, recall: 0.9552238805970149, precision: 1.0, f_beta: 0.9770992366412213\n",
      "train: step: 1102, loss: 0.05030613765120506, acc: 0.9921875, recall: 0.9846153846153847, precision: 1.0, f_beta: 0.9922480620155039\n",
      "train: step: 1103, loss: 0.13295242190361023, acc: 0.953125, recall: 0.9152542372881356, precision: 0.9818181818181818, f_beta: 0.9473684210526316\n",
      "train: step: 1104, loss: 0.08714081346988678, acc: 0.9765625, recall: 0.9848484848484849, precision: 0.9701492537313433, f_beta: 0.9774436090225564\n",
      "train: step: 1105, loss: 0.0771227777004242, acc: 0.9765625, recall: 0.9552238805970149, precision: 1.0, f_beta: 0.9770992366412213\n",
      "train: step: 1106, loss: 0.15075045824050903, acc: 0.9765625, recall: 0.9692307692307692, precision: 0.984375, f_beta: 0.9767441860465116\n",
      "train: step: 1107, loss: 0.14609014987945557, acc: 0.96875, recall: 0.9661016949152542, precision: 0.9661016949152542, f_beta: 0.9661016949152542\n",
      "train: step: 1108, loss: 0.14541998505592346, acc: 0.96875, recall: 1.0, precision: 0.9322033898305084, f_beta: 0.9649122807017544\n",
      "train: step: 1109, loss: 0.13935235142707825, acc: 0.9453125, recall: 0.9642857142857143, precision: 0.9152542372881356, f_beta: 0.9391304347826087\n",
      "train: step: 1110, loss: 0.11087536811828613, acc: 0.9765625, recall: 1.0, precision: 0.9571428571428572, f_beta: 0.9781021897810218\n",
      "train: step: 1111, loss: 0.15707512199878693, acc: 0.9609375, recall: 0.9529411764705882, precision: 0.9878048780487805, f_beta: 0.9700598802395209\n",
      "train: step: 1112, loss: 0.07466676831245422, acc: 0.984375, recall: 0.9846153846153847, precision: 0.9846153846153847, f_beta: 0.9846153846153847\n",
      "train: step: 1113, loss: 0.09389568120241165, acc: 0.984375, recall: 0.9859154929577465, precision: 0.9859154929577465, f_beta: 0.9859154929577465\n",
      "train: step: 1114, loss: 0.051440268754959106, acc: 0.984375, recall: 0.9710144927536232, precision: 1.0, f_beta: 0.9852941176470589\n",
      "train: step: 1115, loss: 0.09009797126054764, acc: 0.9609375, recall: 0.9272727272727272, precision: 0.9807692307692307, f_beta: 0.9532710280373831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1116, loss: 0.061958156526088715, acc: 0.984375, recall: 0.9833333333333333, precision: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 1117, loss: 0.102821946144104, acc: 0.9609375, recall: 0.9655172413793104, precision: 0.9491525423728814, f_beta: 0.9572649572649573\n",
      "train: step: 1118, loss: 0.06493853777647018, acc: 0.984375, recall: 0.984375, precision: 0.984375, f_beta: 0.984375\n",
      "train: step: 1119, loss: 0.08379637449979782, acc: 0.9765625, recall: 0.9824561403508771, precision: 0.9655172413793104, f_beta: 0.9739130434782608\n",
      "train: step: 1120, loss: 0.06981062889099121, acc: 0.9765625, recall: 0.9508196721311475, precision: 1.0, f_beta: 0.9747899159663865\n",
      "train: step: 1121, loss: 0.08311907947063446, acc: 0.96875, recall: 1.0, precision: 0.9487179487179487, f_beta: 0.9736842105263158\n",
      "train: step: 1122, loss: 0.09457521885633469, acc: 0.984375, recall: 1.0, precision: 0.9672131147540983, f_beta: 0.9833333333333333\n",
      "train: step: 1123, loss: 0.04654635488986969, acc: 0.9921875, recall: 0.9866666666666667, precision: 1.0, f_beta: 0.9932885906040269\n",
      "train: step: 1124, loss: 0.03140302747488022, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1125, loss: 0.032504867762327194, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1126, loss: 0.07218116521835327, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1127, loss: 0.095875084400177, acc: 0.96875, recall: 1.0, precision: 0.9393939393939394, f_beta: 0.96875\n",
      "train: step: 1128, loss: 0.09051074087619781, acc: 0.96875, recall: 0.9393939393939394, precision: 1.0, f_beta: 0.96875\n",
      "train: step: 1129, loss: 0.10300464183092117, acc: 0.9765625, recall: 0.9642857142857143, precision: 0.9818181818181818, f_beta: 0.972972972972973\n",
      "train: step: 1130, loss: 0.1289811134338379, acc: 0.953125, recall: 0.9428571428571428, precision: 0.9705882352941176, f_beta: 0.9565217391304348\n",
      "train: step: 1131, loss: 0.1549311876296997, acc: 0.9453125, recall: 0.9354838709677419, precision: 0.9508196721311475, f_beta: 0.943089430894309\n",
      "train: step: 1132, loss: 0.07266122102737427, acc: 0.9765625, recall: 0.9850746268656716, precision: 0.9705882352941176, f_beta: 0.9777777777777777\n",
      "train: step: 1133, loss: 0.1531616449356079, acc: 0.953125, recall: 0.9827586206896551, precision: 0.9193548387096774, f_beta: 0.95\n",
      "train: step: 1134, loss: 0.13684454560279846, acc: 0.9609375, recall: 0.9818181818181818, precision: 0.9310344827586207, f_beta: 0.9557522123893805\n",
      "train: step: 1135, loss: 0.10011422634124756, acc: 0.96875, recall: 0.9696969696969697, precision: 0.9696969696969697, f_beta: 0.9696969696969697\n",
      "train: step: 1136, loss: 0.030517252162098885, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1137, loss: 0.03623264282941818, acc: 0.984375, recall: 1.0, precision: 0.9672131147540983, f_beta: 0.9833333333333333\n",
      "train: step: 1138, loss: 0.14999374747276306, acc: 0.9609375, recall: 0.9682539682539683, precision: 0.953125, f_beta: 0.9606299212598425\n",
      "train: step: 1139, loss: 0.02511421963572502, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1140, loss: 0.06512847542762756, acc: 0.9921875, recall: 1.0, precision: 0.9852941176470589, f_beta: 0.9925925925925926\n",
      "train: step: 1141, loss: 0.06293119490146637, acc: 0.96875, recall: 0.9516129032258065, precision: 0.9833333333333333, f_beta: 0.9672131147540983\n",
      "train: step: 1142, loss: 0.051367003470659256, acc: 0.96875, recall: 0.9545454545454546, precision: 0.984375, f_beta: 0.9692307692307692\n",
      "train: step: 1143, loss: 0.07447254657745361, acc: 0.96875, recall: 0.9538461538461539, precision: 0.9841269841269841, f_beta: 0.96875\n",
      "train: step: 1144, loss: 0.03994528949260712, acc: 0.984375, recall: 0.9696969696969697, precision: 1.0, f_beta: 0.9846153846153847\n",
      "train: step: 1145, loss: 0.07704942673444748, acc: 0.984375, recall: 0.9666666666666667, precision: 1.0, f_beta: 0.983050847457627\n",
      "train: step: 1146, loss: 0.11579062044620514, acc: 0.9609375, recall: 0.9444444444444444, precision: 0.9855072463768116, f_beta: 0.9645390070921985\n",
      "train: step: 1147, loss: 0.05960804224014282, acc: 0.984375, recall: 0.984375, precision: 0.984375, f_beta: 0.984375\n",
      "train: step: 1148, loss: 0.07252418249845505, acc: 0.9765625, recall: 0.9726027397260274, precision: 0.9861111111111112, f_beta: 0.9793103448275863\n",
      "train: step: 1149, loss: 0.10732913017272949, acc: 0.9765625, recall: 0.9811320754716981, precision: 0.9629629629629629, f_beta: 0.9719626168224299\n",
      "train: step: 1150, loss: 0.06820927560329437, acc: 0.96875, recall: 1.0, precision: 0.9384615384615385, f_beta: 0.9682539682539683\n",
      "train: step: 1151, loss: 0.142843559384346, acc: 0.96875, recall: 1.0, precision: 0.9420289855072463, f_beta: 0.9701492537313433\n",
      "train: step: 1152, loss: 0.0627509132027626, acc: 0.9921875, recall: 0.9848484848484849, precision: 1.0, f_beta: 0.9923664122137404\n",
      "train: step: 1153, loss: 0.038838911801576614, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1154, loss: 0.09541960060596466, acc: 0.9765625, recall: 0.9672131147540983, precision: 0.9833333333333333, f_beta: 0.9752066115702478\n",
      "train: step: 1155, loss: 0.11046192049980164, acc: 0.96875, recall: 0.9811320754716981, precision: 0.9454545454545454, f_beta: 0.9629629629629629\n",
      "train: step: 1156, loss: 0.1087946742773056, acc: 0.9765625, recall: 0.9824561403508771, precision: 0.9655172413793104, f_beta: 0.9739130434782608\n",
      "train: step: 1157, loss: 0.1631058156490326, acc: 0.9609375, recall: 0.9354838709677419, precision: 0.9830508474576272, f_beta: 0.9586776859504132\n",
      "train: step: 1158, loss: 0.07427775859832764, acc: 0.984375, recall: 0.9859154929577465, precision: 0.9859154929577465, f_beta: 0.9859154929577465\n",
      "train: step: 1159, loss: 0.12873411178588867, acc: 0.9609375, recall: 0.9393939393939394, precision: 0.9841269841269841, f_beta: 0.9612403100775193\n",
      "train: step: 1160, loss: 0.13308507204055786, acc: 0.9609375, recall: 0.9354838709677419, precision: 0.9830508474576272, f_beta: 0.9586776859504132\n",
      "train: step: 1161, loss: 0.13016445934772491, acc: 0.9609375, recall: 0.9642857142857143, precision: 0.9473684210526315, f_beta: 0.9557522123893805\n",
      "train: step: 1162, loss: 0.13197387754917145, acc: 0.9765625, recall: 1.0, precision: 0.9538461538461539, f_beta: 0.9763779527559054\n",
      "train: step: 1163, loss: 0.05137600749731064, acc: 0.96875, recall: 0.9491525423728814, precision: 0.9824561403508771, f_beta: 0.9655172413793103\n",
      "train: step: 1164, loss: 0.04096458479762077, acc: 0.9921875, recall: 1.0, precision: 0.9855072463768116, f_beta: 0.9927007299270074\n",
      "train: step: 1165, loss: 0.10403965413570404, acc: 0.953125, recall: 0.9714285714285714, precision: 0.9444444444444444, f_beta: 0.9577464788732395\n",
      "train: step: 1166, loss: 0.09234531223773956, acc: 0.9765625, recall: 0.9836065573770492, precision: 0.967741935483871, f_beta: 0.975609756097561\n",
      "train: step: 1167, loss: 0.11311668157577515, acc: 0.9609375, recall: 0.9538461538461539, precision: 0.96875, f_beta: 0.9612403100775193\n",
      "train: step: 1168, loss: 0.06422058492898941, acc: 0.9921875, recall: 0.9827586206896551, precision: 1.0, f_beta: 0.9913043478260869\n",
      "train: step: 1169, loss: 0.06809423863887787, acc: 0.9765625, recall: 0.9666666666666667, precision: 0.9830508474576272, f_beta: 0.9747899159663865\n",
      "train: step: 1170, loss: 0.1413498818874359, acc: 0.953125, recall: 0.9692307692307692, precision: 0.9402985074626866, f_beta: 0.9545454545454547\n",
      "train: step: 1171, loss: 0.16026544570922852, acc: 0.9296875, recall: 0.9272727272727272, precision: 0.9107142857142857, f_beta: 0.918918918918919\n",
      "train: step: 1172, loss: 0.040033768862485886, acc: 0.9921875, recall: 0.9848484848484849, precision: 1.0, f_beta: 0.9923664122137404\n",
      "train: step: 1173, loss: 0.0381966307759285, acc: 0.984375, recall: 0.967741935483871, precision: 1.0, f_beta: 0.9836065573770492\n",
      "train: step: 1174, loss: 0.06832211464643478, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 1175, loss: 0.1117878258228302, acc: 0.9609375, recall: 0.9444444444444444, precision: 0.9622641509433962, f_beta: 0.9532710280373832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1176, loss: 0.09152106940746307, acc: 0.9765625, recall: 0.9836065573770492, precision: 0.967741935483871, f_beta: 0.975609756097561\n",
      "train: step: 1177, loss: 0.06341763585805893, acc: 0.984375, recall: 0.967741935483871, precision: 1.0, f_beta: 0.9836065573770492\n",
      "train: step: 1178, loss: 0.06225702911615372, acc: 0.9765625, recall: 0.9516129032258065, precision: 1.0, f_beta: 0.9752066115702479\n",
      "train: step: 1179, loss: 0.043773651123046875, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1180, loss: 0.06116078794002533, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1181, loss: 0.10328551381826401, acc: 0.9609375, recall: 0.9661016949152542, precision: 0.95, f_beta: 0.957983193277311\n",
      "train: step: 1182, loss: 0.12068137526512146, acc: 0.953125, recall: 0.9661016949152542, precision: 0.9344262295081968, f_beta: 0.95\n",
      "train: step: 1183, loss: 0.08964253216981888, acc: 0.9609375, recall: 0.95, precision: 0.9661016949152542, f_beta: 0.957983193277311\n",
      "train: step: 1184, loss: 0.15237833559513092, acc: 0.9609375, recall: 0.9344262295081968, precision: 0.9827586206896551, f_beta: 0.9579831932773109\n",
      "train: step: 1185, loss: 0.0283895842730999, acc: 0.9921875, recall: 0.9857142857142858, precision: 1.0, f_beta: 0.9928057553956835\n",
      "train: step: 1186, loss: 0.09926795959472656, acc: 0.984375, recall: 0.984375, precision: 0.984375, f_beta: 0.984375\n",
      "train: step: 1187, loss: 0.08528938889503479, acc: 0.96875, recall: 0.9523809523809523, precision: 0.9836065573770492, f_beta: 0.9677419354838709\n",
      "train: step: 1188, loss: 0.13804222643375397, acc: 0.9609375, recall: 0.9692307692307692, precision: 0.9545454545454546, f_beta: 0.9618320610687022\n",
      "train: step: 1189, loss: 0.01812371239066124, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1190, loss: 0.04986004903912544, acc: 0.984375, recall: 0.9701492537313433, precision: 1.0, f_beta: 0.9848484848484849\n",
      "train: step: 1191, loss: 0.06669674813747406, acc: 0.9765625, recall: 0.9705882352941176, precision: 0.9850746268656716, f_beta: 0.9777777777777777\n",
      "train: step: 1192, loss: 0.09778580069541931, acc: 0.984375, recall: 0.9833333333333333, precision: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 1193, loss: 0.10830027610063553, acc: 0.96875, recall: 0.9375, precision: 1.0, f_beta: 0.967741935483871\n",
      "train: step: 1194, loss: 0.04222993552684784, acc: 0.984375, recall: 0.98, precision: 0.98, f_beta: 0.98\n",
      "train: step: 1195, loss: 0.034902725368738174, acc: 0.9921875, recall: 0.9814814814814815, precision: 1.0, f_beta: 0.9906542056074767\n",
      "train: step: 1196, loss: 0.10765893757343292, acc: 0.9765625, recall: 0.967741935483871, precision: 0.9836065573770492, f_beta: 0.975609756097561\n",
      "train: step: 1197, loss: 0.15206274390220642, acc: 0.96875, recall: 0.9848484848484849, precision: 0.9558823529411765, f_beta: 0.9701492537313432\n",
      "train: step: 1198, loss: 0.244617760181427, acc: 0.9296875, recall: 0.9230769230769231, precision: 0.9375, f_beta: 0.9302325581395349\n",
      "train: step: 1199, loss: 0.1883522868156433, acc: 0.953125, recall: 0.9444444444444444, precision: 0.9714285714285714, f_beta: 0.9577464788732395\n",
      "train: step: 1200, loss: 0.08078523725271225, acc: 0.96875, recall: 0.9836065573770492, precision: 0.9523809523809523, f_beta: 0.9677419354838709\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T15:12:12.930670, step: 1200, loss: 0.45785651298669666, acc: 0.8673878205128205,precision: 0.8589693343680553, recall: 0.8774741403233213, f_beta: 0.8672408692549436\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-1200\n",
      "\n",
      "train: step: 1201, loss: 0.06276774406433105, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1202, loss: 0.18272370100021362, acc: 0.953125, recall: 0.967741935483871, precision: 0.9375, f_beta: 0.9523809523809523\n",
      "train: step: 1203, loss: 0.044997669756412506, acc: 0.9921875, recall: 1.0, precision: 0.9824561403508771, f_beta: 0.9911504424778761\n",
      "train: step: 1204, loss: 0.11729606986045837, acc: 0.9609375, recall: 0.9508196721311475, precision: 0.9666666666666667, f_beta: 0.9586776859504132\n",
      "train: step: 1205, loss: 0.070264533162117, acc: 0.984375, recall: 0.9682539682539683, precision: 1.0, f_beta: 0.9838709677419354\n",
      "train: step: 1206, loss: 0.11103564500808716, acc: 0.9765625, recall: 0.9833333333333333, precision: 0.9672131147540983, f_beta: 0.9752066115702478\n",
      "train: step: 1207, loss: 0.09489607065916061, acc: 0.96875, recall: 0.9701492537313433, precision: 0.9701492537313433, f_beta: 0.9701492537313433\n",
      "train: step: 1208, loss: 0.16567789018154144, acc: 0.9609375, recall: 0.9594594594594594, precision: 0.9726027397260274, f_beta: 0.9659863945578231\n",
      "train: step: 1209, loss: 0.13715916872024536, acc: 0.9609375, recall: 0.9375, precision: 0.9836065573770492, f_beta: 0.96\n",
      "train: step: 1210, loss: 0.020213918760418892, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1211, loss: 0.08238738030195236, acc: 0.9765625, recall: 0.9838709677419355, precision: 0.9682539682539683, f_beta: 0.976\n",
      "train: step: 1212, loss: 0.09773874282836914, acc: 0.96875, recall: 0.961038961038961, precision: 0.9866666666666667, f_beta: 0.9736842105263157\n",
      "train: step: 1213, loss: 0.07250040769577026, acc: 0.9765625, recall: 0.96875, precision: 0.9841269841269841, f_beta: 0.9763779527559054\n",
      "train: step: 1214, loss: 0.13060151040554047, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 1215, loss: 0.06184373423457146, acc: 0.9765625, recall: 0.9830508474576272, precision: 0.9666666666666667, f_beta: 0.9747899159663865\n",
      "train: step: 1216, loss: 0.08744299411773682, acc: 0.9765625, recall: 0.9852941176470589, precision: 0.9710144927536232, f_beta: 0.9781021897810219\n",
      "train: step: 1217, loss: 0.10061721503734589, acc: 0.9609375, recall: 0.9722222222222222, precision: 0.958904109589041, f_beta: 0.9655172413793104\n",
      "train: step: 1218, loss: 0.045408882200717926, acc: 0.984375, recall: 1.0, precision: 0.9726027397260274, f_beta: 0.9861111111111112\n",
      "train: step: 1219, loss: 0.06895554065704346, acc: 0.984375, recall: 0.9868421052631579, precision: 0.9868421052631579, f_beta: 0.9868421052631579\n",
      "train: step: 1220, loss: 0.13185836374759674, acc: 0.953125, recall: 0.9692307692307692, precision: 0.9402985074626866, f_beta: 0.9545454545454547\n",
      "train: step: 1221, loss: 0.17449910938739777, acc: 0.9609375, recall: 0.9538461538461539, precision: 0.96875, f_beta: 0.9612403100775193\n",
      "train: step: 1222, loss: 0.13853588700294495, acc: 0.9609375, recall: 0.9552238805970149, precision: 0.9696969696969697, f_beta: 0.9624060150375939\n",
      "train: step: 1223, loss: 0.07549114525318146, acc: 0.984375, recall: 0.9714285714285714, precision: 1.0, f_beta: 0.9855072463768115\n",
      "train: step: 1224, loss: 0.05857895687222481, acc: 0.9765625, recall: 0.9642857142857143, precision: 1.0, f_beta: 0.9818181818181818\n",
      "train: step: 1225, loss: 0.10240767896175385, acc: 0.9765625, recall: 0.9827586206896551, precision: 0.9661016949152542, f_beta: 0.9743589743589743\n",
      "train: step: 1226, loss: 0.024819497019052505, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1227, loss: 0.16748085618019104, acc: 0.953125, recall: 0.927536231884058, precision: 0.9846153846153847, f_beta: 0.9552238805970149\n",
      "train: step: 1228, loss: 0.14698874950408936, acc: 0.9609375, recall: 0.95, precision: 0.9661016949152542, f_beta: 0.957983193277311\n",
      "train: step: 1229, loss: 0.19111406803131104, acc: 0.9375, recall: 0.9402985074626866, precision: 0.9402985074626866, f_beta: 0.9402985074626865\n",
      "train: step: 1230, loss: 0.16305983066558838, acc: 0.953125, recall: 1.0, precision: 0.9117647058823529, f_beta: 0.9538461538461539\n",
      "train: step: 1231, loss: 0.10582630336284637, acc: 0.96875, recall: 0.9836065573770492, precision: 0.9523809523809523, f_beta: 0.9677419354838709\n",
      "train: step: 1232, loss: 0.10383576154708862, acc: 0.9765625, recall: 0.987012987012987, precision: 0.9743589743589743, f_beta: 0.9806451612903225\n",
      "train: step: 1233, loss: 0.0969538539648056, acc: 0.96875, recall: 0.9538461538461539, precision: 0.9841269841269841, f_beta: 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1234, loss: 0.15785837173461914, acc: 0.953125, recall: 0.9130434782608695, precision: 1.0, f_beta: 0.9545454545454545\n",
      "train: step: 1235, loss: 0.05448541045188904, acc: 0.984375, recall: 0.9855072463768116, precision: 0.9855072463768116, f_beta: 0.9855072463768116\n",
      "train: step: 1236, loss: 0.03696390241384506, acc: 0.9921875, recall: 1.0, precision: 0.9864864864864865, f_beta: 0.9931972789115647\n",
      "train: step: 1237, loss: 0.18750202655792236, acc: 0.9453125, recall: 0.9508196721311475, precision: 0.9354838709677419, f_beta: 0.943089430894309\n",
      "train: step: 1238, loss: 0.02816827967762947, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1239, loss: 0.07874143123626709, acc: 0.96875, recall: 0.9523809523809523, precision: 0.9836065573770492, f_beta: 0.9677419354838709\n",
      "train: step: 1240, loss: 0.08271900564432144, acc: 0.9765625, recall: 0.9710144927536232, precision: 0.9852941176470589, f_beta: 0.9781021897810219\n",
      "train: step: 1241, loss: 0.12259303033351898, acc: 0.953125, recall: 0.9333333333333333, precision: 0.9655172413793104, f_beta: 0.9491525423728815\n",
      "train: step: 1242, loss: 0.07066331803798676, acc: 0.984375, recall: 0.9859154929577465, precision: 0.9859154929577465, f_beta: 0.9859154929577465\n",
      "train: step: 1243, loss: 0.11302925646305084, acc: 0.9765625, recall: 1.0, precision: 0.9523809523809523, f_beta: 0.975609756097561\n",
      "train: step: 1244, loss: 0.06800764799118042, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 1245, loss: 0.10560447722673416, acc: 0.96875, recall: 0.9661016949152542, precision: 0.9661016949152542, f_beta: 0.9661016949152542\n",
      "train: step: 1246, loss: 0.07540979981422424, acc: 0.9765625, recall: 0.9491525423728814, precision: 1.0, f_beta: 0.9739130434782608\n",
      "train: step: 1247, loss: 0.15847840905189514, acc: 0.9453125, recall: 0.9333333333333333, precision: 0.9491525423728814, f_beta: 0.9411764705882353\n",
      "train: step: 1248, loss: 0.08498544245958328, acc: 0.9609375, recall: 0.95, precision: 0.9661016949152542, f_beta: 0.957983193277311\n",
      "start training model\n",
      "train: step: 1249, loss: 0.04684857279062271, acc: 0.984375, recall: 0.9855072463768116, precision: 0.9855072463768116, f_beta: 0.9855072463768116\n",
      "train: step: 1250, loss: 0.0368240550160408, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1251, loss: 0.13088800013065338, acc: 0.96875, recall: 0.984375, precision: 0.9545454545454546, f_beta: 0.9692307692307692\n",
      "train: step: 1252, loss: 0.07556769251823425, acc: 0.9765625, recall: 0.971830985915493, precision: 0.9857142857142858, f_beta: 0.9787234042553192\n",
      "train: step: 1253, loss: 0.14352187514305115, acc: 0.9609375, recall: 0.971830985915493, precision: 0.9583333333333334, f_beta: 0.965034965034965\n",
      "train: step: 1254, loss: 0.019002536311745644, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1255, loss: 0.0705999881029129, acc: 0.984375, recall: 0.9864864864864865, precision: 0.9864864864864865, f_beta: 0.9864864864864865\n",
      "train: step: 1256, loss: 0.055672816932201385, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1257, loss: 0.05770310014486313, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1258, loss: 0.04090516269207001, acc: 0.984375, recall: 0.9836065573770492, precision: 0.9836065573770492, f_beta: 0.9836065573770492\n",
      "train: step: 1259, loss: 0.06118706613779068, acc: 0.984375, recall: 0.9726027397260274, precision: 1.0, f_beta: 0.9861111111111112\n",
      "train: step: 1260, loss: 0.051385726779699326, acc: 0.984375, recall: 0.9850746268656716, precision: 0.9850746268656716, f_beta: 0.9850746268656716\n",
      "train: step: 1261, loss: 0.02204759046435356, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1262, loss: 0.07406648248434067, acc: 0.984375, recall: 0.9666666666666667, precision: 1.0, f_beta: 0.983050847457627\n",
      "train: step: 1263, loss: 0.04981521517038345, acc: 0.9921875, recall: 0.9833333333333333, precision: 1.0, f_beta: 0.9915966386554621\n",
      "train: step: 1264, loss: 0.14456802606582642, acc: 0.96875, recall: 0.9850746268656716, precision: 0.9565217391304348, f_beta: 0.9705882352941176\n",
      "train: step: 1265, loss: 0.053910188376903534, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1266, loss: 0.08455106616020203, acc: 0.9765625, recall: 0.9852941176470589, precision: 0.9710144927536232, f_beta: 0.9781021897810219\n",
      "train: step: 1267, loss: 0.01453875470906496, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1268, loss: 0.052680205553770065, acc: 0.9921875, recall: 1.0, precision: 0.9830508474576272, f_beta: 0.9914529914529915\n",
      "train: step: 1269, loss: 0.055398400872945786, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1270, loss: 0.06389567255973816, acc: 0.984375, recall: 0.984375, precision: 0.984375, f_beta: 0.984375\n",
      "train: step: 1271, loss: 0.028986159712076187, acc: 0.9921875, recall: 0.984375, precision: 1.0, f_beta: 0.9921259842519685\n",
      "train: step: 1272, loss: 0.019397517666220665, acc: 0.9921875, recall: 1.0, precision: 0.9833333333333333, f_beta: 0.9915966386554621\n",
      "train: step: 1273, loss: 0.04583849012851715, acc: 0.9921875, recall: 0.9864864864864865, precision: 1.0, f_beta: 0.9931972789115647\n",
      "train: step: 1274, loss: 0.06863176822662354, acc: 0.984375, recall: 0.9833333333333333, precision: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 1275, loss: 0.04891789332032204, acc: 0.9765625, recall: 0.9830508474576272, precision: 0.9666666666666667, f_beta: 0.9747899159663865\n",
      "train: step: 1276, loss: 0.07391375303268433, acc: 0.984375, recall: 1.0, precision: 0.972972972972973, f_beta: 0.9863013698630138\n",
      "train: step: 1277, loss: 0.09383933991193771, acc: 0.96875, recall: 0.9666666666666667, precision: 0.9666666666666667, f_beta: 0.9666666666666667\n",
      "train: step: 1278, loss: 0.06468727439641953, acc: 0.96875, recall: 0.967741935483871, precision: 0.967741935483871, f_beta: 0.967741935483871\n",
      "train: step: 1279, loss: 0.05952337384223938, acc: 0.9921875, recall: 1.0, precision: 0.984375, f_beta: 0.9921259842519685\n",
      "train: step: 1280, loss: 0.17210698127746582, acc: 0.9609375, recall: 0.9838709677419355, precision: 0.9384615384615385, f_beta: 0.9606299212598426\n",
      "train: step: 1281, loss: 0.03257083520293236, acc: 0.984375, recall: 1.0, precision: 0.9692307692307692, f_beta: 0.9843749999999999\n",
      "train: step: 1282, loss: 0.06198043376207352, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1283, loss: 0.018916556611657143, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1284, loss: 0.01913534291088581, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1285, loss: 0.06836242228746414, acc: 0.984375, recall: 0.984375, precision: 0.984375, f_beta: 0.984375\n",
      "train: step: 1286, loss: 0.06138801574707031, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1287, loss: 0.08959509432315826, acc: 0.984375, recall: 0.9830508474576272, precision: 0.9830508474576272, f_beta: 0.9830508474576272\n",
      "train: step: 1288, loss: 0.10995155572891235, acc: 0.9765625, recall: 0.9696969696969697, precision: 0.9846153846153847, f_beta: 0.9770992366412214\n",
      "train: step: 1289, loss: 0.0864356979727745, acc: 0.9765625, recall: 0.9861111111111112, precision: 0.9726027397260274, f_beta: 0.9793103448275863\n",
      "train: step: 1290, loss: 0.04855242371559143, acc: 0.9921875, recall: 0.9848484848484849, precision: 1.0, f_beta: 0.9923664122137404\n",
      "train: step: 1291, loss: 0.0582074336707592, acc: 0.96875, recall: 0.95, precision: 0.9827586206896551, f_beta: 0.9661016949152542\n",
      "train: step: 1292, loss: 0.10898084938526154, acc: 0.9765625, recall: 0.9714285714285714, precision: 0.9855072463768116, f_beta: 0.9784172661870504\n",
      "train: step: 1293, loss: 0.03321487829089165, acc: 0.9921875, recall: 0.9863013698630136, precision: 1.0, f_beta: 0.993103448275862\n",
      "train: step: 1294, loss: 0.11540454626083374, acc: 0.96875, recall: 0.9791666666666666, precision: 0.94, f_beta: 0.9591836734693877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1295, loss: 0.08060973137617111, acc: 0.953125, recall: 0.9827586206896551, precision: 0.9193548387096774, f_beta: 0.95\n",
      "train: step: 1296, loss: 0.10470552742481232, acc: 0.9765625, recall: 0.9692307692307692, precision: 0.984375, f_beta: 0.9767441860465116\n",
      "train: step: 1297, loss: 0.06218576803803444, acc: 0.9765625, recall: 0.9807692307692307, precision: 0.9622641509433962, f_beta: 0.9714285714285713\n",
      "train: step: 1298, loss: 0.0875232070684433, acc: 0.984375, recall: 1.0, precision: 0.9714285714285714, f_beta: 0.9855072463768115\n",
      "train: step: 1299, loss: 0.07186197489500046, acc: 0.9765625, recall: 0.9841269841269841, precision: 0.96875, f_beta: 0.9763779527559054\n",
      "train: step: 1300, loss: 0.03479905426502228, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T15:16:02.875204, step: 1300, loss: 0.4838109914308939, acc: 0.8651842948717948,precision: 0.8548980790912449, recall: 0.8778059040630544, f_beta: 0.8649296704044983\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-1300\n",
      "\n",
      "train: step: 1301, loss: 0.02814614214003086, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1302, loss: 0.02749301865696907, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1303, loss: 0.16446375846862793, acc: 0.9609375, recall: 0.9565217391304348, precision: 0.9705882352941176, f_beta: 0.9635036496350365\n",
      "train: step: 1304, loss: 0.11444397270679474, acc: 0.96875, recall: 0.9571428571428572, precision: 0.9852941176470589, f_beta: 0.9710144927536232\n",
      "train: step: 1305, loss: 0.21018806099891663, acc: 0.953125, recall: 0.9322033898305084, precision: 0.9649122807017544, f_beta: 0.9482758620689654\n",
      "train: step: 1306, loss: 0.03337695822119713, acc: 0.9921875, recall: 0.9859154929577465, precision: 1.0, f_beta: 0.9929078014184397\n",
      "train: step: 1307, loss: 0.02929934859275818, acc: 0.9921875, recall: 1.0, precision: 0.9824561403508771, f_beta: 0.9911504424778761\n",
      "train: step: 1308, loss: 0.13740913569927216, acc: 0.96875, recall: 0.95, precision: 0.9827586206896551, f_beta: 0.9661016949152542\n",
      "train: step: 1309, loss: 0.025935331359505653, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1310, loss: 0.023818597197532654, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1311, loss: 0.03883663937449455, acc: 0.9765625, recall: 0.9848484848484849, precision: 0.9701492537313433, f_beta: 0.9774436090225564\n",
      "train: step: 1312, loss: 0.0858713909983635, acc: 0.984375, recall: 1.0, precision: 0.96875, f_beta: 0.9841269841269841\n",
      "train: step: 1313, loss: 0.026136253029108047, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1314, loss: 0.019018013030290604, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1315, loss: 0.09224989265203476, acc: 0.984375, recall: 0.9726027397260274, precision: 1.0, f_beta: 0.9861111111111112\n",
      "train: step: 1316, loss: 0.016319960355758667, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1317, loss: 0.10244552046060562, acc: 0.9765625, recall: 0.9682539682539683, precision: 0.9838709677419355, f_beta: 0.976\n",
      "train: step: 1318, loss: 0.14009787142276764, acc: 0.9609375, recall: 0.9696969696969697, precision: 0.9552238805970149, f_beta: 0.9624060150375939\n",
      "train: step: 1319, loss: 0.04827278479933739, acc: 0.984375, recall: 0.9710144927536232, precision: 1.0, f_beta: 0.9852941176470589\n",
      "train: step: 1320, loss: 0.06800220906734467, acc: 0.984375, recall: 0.96875, precision: 1.0, f_beta: 0.9841269841269841\n",
      "train: step: 1321, loss: 0.05678946524858475, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1322, loss: 0.024056674912571907, acc: 0.9921875, recall: 0.987012987012987, precision: 1.0, f_beta: 0.9934640522875817\n",
      "train: step: 1323, loss: 0.09414538741111755, acc: 0.984375, recall: 0.9833333333333333, precision: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 1324, loss: 0.015292070806026459, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1325, loss: 0.08736415207386017, acc: 0.9765625, recall: 1.0, precision: 0.9508196721311475, f_beta: 0.9747899159663865\n",
      "train: step: 1326, loss: 0.05172151327133179, acc: 0.9765625, recall: 0.953125, precision: 1.0, f_beta: 0.976\n",
      "train: step: 1327, loss: 0.11628894507884979, acc: 0.96875, recall: 0.9841269841269841, precision: 0.9538461538461539, f_beta: 0.96875\n",
      "train: step: 1328, loss: 0.10987083613872528, acc: 0.96875, recall: 0.9861111111111112, precision: 0.9594594594594594, f_beta: 0.9726027397260274\n",
      "train: step: 1329, loss: 0.071724534034729, acc: 0.9765625, recall: 0.967741935483871, precision: 0.9836065573770492, f_beta: 0.975609756097561\n",
      "train: step: 1330, loss: 0.011241871863603592, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1331, loss: 0.11708904057741165, acc: 0.96875, recall: 0.9692307692307692, precision: 0.9692307692307692, f_beta: 0.9692307692307692\n",
      "train: step: 1332, loss: 0.04576792195439339, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1333, loss: 0.13206428289413452, acc: 0.96875, recall: 0.9833333333333333, precision: 0.9516129032258065, f_beta: 0.9672131147540983\n",
      "train: step: 1334, loss: 0.13884052634239197, acc: 0.953125, recall: 0.9420289855072463, precision: 0.9701492537313433, f_beta: 0.9558823529411764\n",
      "train: step: 1335, loss: 0.05807561054825783, acc: 0.984375, recall: 0.9852941176470589, precision: 0.9852941176470589, f_beta: 0.9852941176470589\n",
      "train: step: 1336, loss: 0.1406560093164444, acc: 0.96875, recall: 0.9552238805970149, precision: 0.9846153846153847, f_beta: 0.9696969696969696\n",
      "train: step: 1337, loss: 0.03443888574838638, acc: 0.984375, recall: 0.9836065573770492, precision: 0.9836065573770492, f_beta: 0.9836065573770492\n",
      "train: step: 1338, loss: 0.022028962150216103, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1339, loss: 0.017075592651963234, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1340, loss: 0.015236754901707172, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1341, loss: 0.07263341546058655, acc: 0.984375, recall: 1.0, precision: 0.96875, f_beta: 0.9841269841269841\n",
      "train: step: 1342, loss: 0.01401691883802414, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1343, loss: 0.03335459902882576, acc: 0.9921875, recall: 0.9863013698630136, precision: 1.0, f_beta: 0.993103448275862\n",
      "train: step: 1344, loss: 0.04729388654232025, acc: 0.9921875, recall: 1.0, precision: 0.9807692307692307, f_beta: 0.9902912621359222\n",
      "train: step: 1345, loss: 0.02438385970890522, acc: 0.9921875, recall: 0.984375, precision: 1.0, f_beta: 0.9921259842519685\n",
      "train: step: 1346, loss: 0.021270476281642914, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1347, loss: 0.05048319324851036, acc: 0.984375, recall: 1.0, precision: 0.9655172413793104, f_beta: 0.9824561403508771\n",
      "train: step: 1348, loss: 0.055249787867069244, acc: 0.984375, recall: 0.9629629629629629, precision: 1.0, f_beta: 0.9811320754716981\n",
      "train: step: 1349, loss: 0.01324864849448204, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1350, loss: 0.2053454965353012, acc: 0.9375, recall: 0.9122807017543859, precision: 0.9454545454545454, f_beta: 0.9285714285714285\n",
      "train: step: 1351, loss: 0.10156887024641037, acc: 0.984375, recall: 0.9852941176470589, precision: 0.9852941176470589, f_beta: 0.9852941176470589\n",
      "train: step: 1352, loss: 0.13979950547218323, acc: 0.9765625, recall: 0.9846153846153847, precision: 0.9696969696969697, f_beta: 0.9770992366412214\n",
      "train: step: 1353, loss: 0.012767070904374123, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1354, loss: 0.11851942539215088, acc: 0.9765625, recall: 0.9861111111111112, precision: 0.9726027397260274, f_beta: 0.9793103448275863\n",
      "train: step: 1355, loss: 0.17443761229515076, acc: 0.9609375, recall: 0.9661016949152542, precision: 0.95, f_beta: 0.957983193277311\n",
      "train: step: 1356, loss: 0.0988796055316925, acc: 0.96875, recall: 0.9473684210526315, precision: 0.9818181818181818, f_beta: 0.9642857142857142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1357, loss: 0.08274532854557037, acc: 0.984375, recall: 0.9855072463768116, precision: 0.9855072463768116, f_beta: 0.9855072463768116\n",
      "train: step: 1358, loss: 0.07937143743038177, acc: 0.9765625, recall: 0.9508196721311475, precision: 1.0, f_beta: 0.9747899159663865\n",
      "train: step: 1359, loss: 0.053471677005290985, acc: 0.9921875, recall: 1.0, precision: 0.9852941176470589, f_beta: 0.9925925925925926\n",
      "train: step: 1360, loss: 0.10124649107456207, acc: 0.9765625, recall: 0.9846153846153847, precision: 0.9696969696969697, f_beta: 0.9770992366412214\n",
      "train: step: 1361, loss: 0.09091555327177048, acc: 0.9765625, recall: 1.0, precision: 0.953125, f_beta: 0.976\n",
      "train: step: 1362, loss: 0.01753818616271019, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1363, loss: 0.09574136137962341, acc: 0.984375, recall: 0.96875, precision: 1.0, f_beta: 0.9841269841269841\n",
      "train: step: 1364, loss: 0.013519233092665672, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1365, loss: 0.04008271172642708, acc: 0.9921875, recall: 1.0, precision: 0.9836065573770492, f_beta: 0.9917355371900827\n",
      "train: step: 1366, loss: 0.01815200410783291, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1367, loss: 0.10636533051729202, acc: 0.984375, recall: 0.9692307692307692, precision: 1.0, f_beta: 0.9843749999999999\n",
      "train: step: 1368, loss: 0.02756803296506405, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1369, loss: 0.024903912097215652, acc: 0.9921875, recall: 0.984375, precision: 1.0, f_beta: 0.9921259842519685\n",
      "train: step: 1370, loss: 0.09986146539449692, acc: 0.9765625, recall: 0.9852941176470589, precision: 0.9710144927536232, f_beta: 0.9781021897810219\n",
      "train: step: 1371, loss: 0.042576901614665985, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1372, loss: 0.06660909950733185, acc: 0.9921875, recall: 1.0, precision: 0.9864864864864865, f_beta: 0.9931972789115647\n",
      "train: step: 1373, loss: 0.014383156783878803, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1374, loss: 0.16148249804973602, acc: 0.9609375, recall: 0.9649122807017544, precision: 0.9482758620689655, f_beta: 0.9565217391304347\n",
      "train: step: 1375, loss: 0.04905509203672409, acc: 0.9921875, recall: 0.9836065573770492, precision: 1.0, f_beta: 0.9917355371900827\n",
      "train: step: 1376, loss: 0.05078216269612312, acc: 0.9921875, recall: 1.0, precision: 0.9846153846153847, f_beta: 0.9922480620155039\n",
      "train: step: 1377, loss: 0.0460619293153286, acc: 0.984375, recall: 0.9821428571428571, precision: 0.9821428571428571, f_beta: 0.9821428571428571\n",
      "train: step: 1378, loss: 0.047715216875076294, acc: 0.9921875, recall: 0.9814814814814815, precision: 1.0, f_beta: 0.9906542056074767\n",
      "train: step: 1379, loss: 0.11168144643306732, acc: 0.9609375, recall: 0.971830985915493, precision: 0.9583333333333334, f_beta: 0.965034965034965\n",
      "train: step: 1380, loss: 0.017292991280555725, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1381, loss: 0.037909556180238724, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1382, loss: 0.02433958277106285, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1383, loss: 0.026823261752724648, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1384, loss: 0.023935213685035706, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1385, loss: 0.0545940026640892, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1386, loss: 0.10044045746326447, acc: 0.96875, recall: 0.9354838709677419, precision: 1.0, f_beta: 0.9666666666666666\n",
      "train: step: 1387, loss: 0.06275627017021179, acc: 0.984375, recall: 0.9818181818181818, precision: 0.9818181818181818, f_beta: 0.9818181818181818\n",
      "train: step: 1388, loss: 0.1611667275428772, acc: 0.96875, recall: 0.958904109589041, precision: 0.9859154929577465, f_beta: 0.9722222222222222\n",
      "train: step: 1389, loss: 0.04718220233917236, acc: 0.9765625, recall: 0.9836065573770492, precision: 0.967741935483871, f_beta: 0.975609756097561\n",
      "train: step: 1390, loss: 0.026474304497241974, acc: 0.984375, recall: 0.9821428571428571, precision: 0.9821428571428571, f_beta: 0.9821428571428571\n",
      "train: step: 1391, loss: 0.09649015963077545, acc: 0.96875, recall: 0.9661016949152542, precision: 0.9661016949152542, f_beta: 0.9661016949152542\n",
      "train: step: 1392, loss: 0.024783849716186523, acc: 0.9921875, recall: 1.0, precision: 0.9863013698630136, f_beta: 0.993103448275862\n",
      "train: step: 1393, loss: 0.14829617738723755, acc: 0.953125, recall: 0.9666666666666667, precision: 0.9354838709677419, f_beta: 0.9508196721311476\n",
      "train: step: 1394, loss: 0.021151253953576088, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1395, loss: 0.05581367760896683, acc: 0.9921875, recall: 1.0, precision: 0.9846153846153847, f_beta: 0.9922480620155039\n",
      "train: step: 1396, loss: 0.05082634463906288, acc: 0.984375, recall: 0.9682539682539683, precision: 1.0, f_beta: 0.9838709677419354\n",
      "train: step: 1397, loss: 0.04641224816441536, acc: 0.984375, recall: 1.0, precision: 0.9696969696969697, f_beta: 0.9846153846153847\n",
      "train: step: 1398, loss: 0.14624804258346558, acc: 0.96875, recall: 0.9375, precision: 0.9782608695652174, f_beta: 0.9574468085106383\n",
      "train: step: 1399, loss: 0.021070141345262527, acc: 0.9921875, recall: 0.984375, precision: 1.0, f_beta: 0.9921259842519685\n",
      "train: step: 1400, loss: 0.01940729282796383, acc: 0.9921875, recall: 0.984375, precision: 1.0, f_beta: 0.9921259842519685\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T15:19:52.233215, step: 1400, loss: 0.5129844424052116, acc: 0.8619791666666666,precision: 0.8540379905212745, recall: 0.8698511290810166, f_beta: 0.8612243855295714\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-1400\n",
      "\n",
      "train: step: 1401, loss: 0.06472957134246826, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1402, loss: 0.04927681386470795, acc: 0.9921875, recall: 0.984375, precision: 1.0, f_beta: 0.9921259842519685\n",
      "train: step: 1403, loss: 0.08436587452888489, acc: 0.984375, recall: 0.9850746268656716, precision: 0.9850746268656716, f_beta: 0.9850746268656716\n",
      "train: step: 1404, loss: 0.0909246876835823, acc: 0.984375, recall: 1.0, precision: 0.972972972972973, f_beta: 0.9863013698630138\n",
      "start training model\n",
      "train: step: 1405, loss: 0.011273294687271118, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1406, loss: 0.042988285422325134, acc: 0.9921875, recall: 0.9833333333333333, precision: 1.0, f_beta: 0.9915966386554621\n",
      "train: step: 1407, loss: 0.03897633031010628, acc: 0.9921875, recall: 1.0, precision: 0.9833333333333333, f_beta: 0.9915966386554621\n",
      "train: step: 1408, loss: 0.04653102532029152, acc: 0.984375, recall: 0.9855072463768116, precision: 0.9855072463768116, f_beta: 0.9855072463768116\n",
      "train: step: 1409, loss: 0.08864115923643112, acc: 0.9765625, recall: 0.9629629629629629, precision: 0.9811320754716981, f_beta: 0.9719626168224299\n",
      "train: step: 1410, loss: 0.04170318692922592, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1411, loss: 0.1104237362742424, acc: 0.9765625, recall: 0.9830508474576272, precision: 0.9666666666666667, f_beta: 0.9747899159663865\n",
      "train: step: 1412, loss: 0.04864272475242615, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1413, loss: 0.012501675635576248, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1414, loss: 0.038746096193790436, acc: 0.9921875, recall: 1.0, precision: 0.9841269841269841, f_beta: 0.9919999999999999\n",
      "train: step: 1415, loss: 0.015656938776373863, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1416, loss: 0.0159163661301136, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1417, loss: 0.01027627196162939, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1418, loss: 0.009109342470765114, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1419, loss: 0.019388094544410706, acc: 0.9921875, recall: 1.0, precision: 0.9841269841269841, f_beta: 0.9919999999999999\n",
      "train: step: 1420, loss: 0.08012446761131287, acc: 0.984375, recall: 0.9863013698630136, precision: 0.9863013698630136, f_beta: 0.9863013698630136\n",
      "train: step: 1421, loss: 0.07947368174791336, acc: 0.984375, recall: 0.9710144927536232, precision: 1.0, f_beta: 0.9852941176470589\n",
      "train: step: 1422, loss: 0.008680363185703754, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1423, loss: 0.00938320904970169, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1424, loss: 0.05768290162086487, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1425, loss: 0.04898009076714516, acc: 0.9921875, recall: 1.0, precision: 0.984375, f_beta: 0.9921259842519685\n",
      "train: step: 1426, loss: 0.04773065820336342, acc: 0.9921875, recall: 1.0, precision: 0.9852941176470589, f_beta: 0.9925925925925926\n",
      "train: step: 1427, loss: 0.07548884302377701, acc: 0.984375, recall: 0.9852941176470589, precision: 0.9852941176470589, f_beta: 0.9852941176470589\n",
      "train: step: 1428, loss: 0.054705917835235596, acc: 0.9921875, recall: 0.9827586206896551, precision: 1.0, f_beta: 0.9913043478260869\n",
      "train: step: 1429, loss: 0.05555121228098869, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1430, loss: 0.007547887973487377, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1431, loss: 0.11926049739122391, acc: 0.9765625, recall: 0.9848484848484849, precision: 0.9701492537313433, f_beta: 0.9774436090225564\n",
      "train: step: 1432, loss: 0.07174573838710785, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1433, loss: 0.05371944233775139, acc: 0.984375, recall: 1.0, precision: 0.967741935483871, f_beta: 0.9836065573770492\n",
      "train: step: 1434, loss: 0.057213351130485535, acc: 0.9921875, recall: 0.9836065573770492, precision: 1.0, f_beta: 0.9917355371900827\n",
      "train: step: 1435, loss: 0.018046706914901733, acc: 0.9921875, recall: 1.0, precision: 0.9855072463768116, f_beta: 0.9927007299270074\n",
      "train: step: 1436, loss: 0.01699005998671055, acc: 0.9921875, recall: 0.9848484848484849, precision: 1.0, f_beta: 0.9923664122137404\n",
      "train: step: 1437, loss: 0.007495105732232332, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1438, loss: 0.04277600720524788, acc: 0.9921875, recall: 1.0, precision: 0.9836065573770492, f_beta: 0.9917355371900827\n",
      "train: step: 1439, loss: 0.0104372538626194, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1440, loss: 0.013015958480536938, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1441, loss: 0.13321620225906372, acc: 0.9765625, recall: 0.9821428571428571, precision: 0.9649122807017544, f_beta: 0.9734513274336283\n",
      "train: step: 1442, loss: 0.05217678099870682, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1443, loss: 0.03370123729109764, acc: 0.9921875, recall: 0.9824561403508771, precision: 1.0, f_beta: 0.9911504424778761\n",
      "train: step: 1444, loss: 0.018557066097855568, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1445, loss: 0.08812369406223297, acc: 0.984375, recall: 0.9714285714285714, precision: 1.0, f_beta: 0.9855072463768115\n",
      "train: step: 1446, loss: 0.08171640336513519, acc: 0.9765625, recall: 0.971830985915493, precision: 0.9857142857142858, f_beta: 0.9787234042553192\n",
      "train: step: 1447, loss: 0.1297626793384552, acc: 0.96875, recall: 0.9649122807017544, precision: 0.9649122807017544, f_beta: 0.9649122807017544\n",
      "train: step: 1448, loss: 0.0099306870251894, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1449, loss: 0.009236913174390793, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1450, loss: 0.03711855039000511, acc: 0.9921875, recall: 1.0, precision: 0.9846153846153847, f_beta: 0.9922480620155039\n",
      "train: step: 1451, loss: 0.0341019406914711, acc: 0.9921875, recall: 1.0, precision: 0.9827586206896551, f_beta: 0.9913043478260869\n",
      "train: step: 1452, loss: 0.040605850517749786, acc: 0.9921875, recall: 1.0, precision: 0.9838709677419355, f_beta: 0.991869918699187\n",
      "train: step: 1453, loss: 0.039631761610507965, acc: 0.9921875, recall: 1.0, precision: 0.9864864864864865, f_beta: 0.9931972789115647\n",
      "train: step: 1454, loss: 0.08325186371803284, acc: 0.984375, recall: 0.9850746268656716, precision: 0.9850746268656716, f_beta: 0.9850746268656716\n",
      "train: step: 1455, loss: 0.0735621452331543, acc: 0.984375, recall: 0.9846153846153847, precision: 0.9846153846153847, f_beta: 0.9846153846153847\n",
      "train: step: 1456, loss: 0.05158406123518944, acc: 0.9921875, recall: 0.9855072463768116, precision: 1.0, f_beta: 0.9927007299270074\n",
      "train: step: 1457, loss: 0.09665794670581818, acc: 0.984375, recall: 1.0, precision: 0.972972972972973, f_beta: 0.9863013698630138\n",
      "train: step: 1458, loss: 0.13191543519496918, acc: 0.9765625, recall: 1.0, precision: 0.9473684210526315, f_beta: 0.972972972972973\n",
      "train: step: 1459, loss: 0.06641428917646408, acc: 0.984375, recall: 1.0, precision: 0.9692307692307692, f_beta: 0.9843749999999999\n",
      "train: step: 1460, loss: 0.042823173105716705, acc: 0.9921875, recall: 1.0, precision: 0.9833333333333333, f_beta: 0.9915966386554621\n",
      "train: step: 1461, loss: 0.07068905234336853, acc: 0.984375, recall: 0.9649122807017544, precision: 1.0, f_beta: 0.9821428571428572\n",
      "train: step: 1462, loss: 0.008131694979965687, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1463, loss: 0.07628127932548523, acc: 0.9765625, recall: 1.0, precision: 0.9552238805970149, f_beta: 0.9770992366412213\n",
      "train: step: 1464, loss: 0.01365202758461237, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1465, loss: 0.05324883759021759, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1466, loss: 0.0261955875903368, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1467, loss: 0.03523477539420128, acc: 0.984375, recall: 0.9655172413793104, precision: 1.0, f_beta: 0.9824561403508771\n",
      "train: step: 1468, loss: 0.14775091409683228, acc: 0.9609375, recall: 0.9333333333333333, precision: 0.9824561403508771, f_beta: 0.9572649572649572\n",
      "train: step: 1469, loss: 0.05123217776417732, acc: 0.984375, recall: 0.9841269841269841, precision: 0.9841269841269841, f_beta: 0.9841269841269841\n",
      "train: step: 1470, loss: 0.047045789659023285, acc: 0.9921875, recall: 1.0, precision: 0.9846153846153847, f_beta: 0.9922480620155039\n",
      "train: step: 1471, loss: 0.010696487501263618, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1472, loss: 0.10937193781137466, acc: 0.9765625, recall: 0.967741935483871, precision: 0.9836065573770492, f_beta: 0.975609756097561\n",
      "train: step: 1473, loss: 0.012216521427035332, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1474, loss: 0.023373931646347046, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1475, loss: 0.05687291920185089, acc: 0.984375, recall: 1.0, precision: 0.9714285714285714, f_beta: 0.9855072463768115\n",
      "train: step: 1476, loss: 0.023797892034053802, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1477, loss: 0.04129387065768242, acc: 0.9921875, recall: 0.9861111111111112, precision: 1.0, f_beta: 0.993006993006993\n",
      "train: step: 1478, loss: 0.010290121659636497, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1479, loss: 0.04602658748626709, acc: 0.9921875, recall: 1.0, precision: 0.9859154929577465, f_beta: 0.9929078014184397\n",
      "train: step: 1480, loss: 0.08462106436491013, acc: 0.984375, recall: 1.0, precision: 0.9682539682539683, f_beta: 0.9838709677419354\n",
      "train: step: 1481, loss: 0.011890384368598461, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1482, loss: 0.014101962558925152, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1483, loss: 0.0763980969786644, acc: 0.984375, recall: 0.9649122807017544, precision: 1.0, f_beta: 0.9821428571428572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1484, loss: 0.16163915395736694, acc: 0.96875, recall: 0.9402985074626866, precision: 1.0, f_beta: 0.9692307692307692\n",
      "train: step: 1485, loss: 0.06388534605503082, acc: 0.984375, recall: 0.9850746268656716, precision: 0.9850746268656716, f_beta: 0.9850746268656716\n",
      "train: step: 1486, loss: 0.08801238238811493, acc: 0.9765625, recall: 0.9682539682539683, precision: 0.9838709677419355, f_beta: 0.976\n",
      "train: step: 1487, loss: 0.08367089927196503, acc: 0.984375, recall: 0.9827586206896551, precision: 0.9827586206896551, f_beta: 0.9827586206896551\n",
      "train: step: 1488, loss: 0.009471505880355835, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1489, loss: 0.04764075204730034, acc: 0.9921875, recall: 0.9830508474576272, precision: 1.0, f_beta: 0.9914529914529915\n",
      "train: step: 1490, loss: 0.08743016421794891, acc: 0.984375, recall: 1.0, precision: 0.9636363636363636, f_beta: 0.9814814814814815\n",
      "train: step: 1491, loss: 0.07442249357700348, acc: 0.984375, recall: 0.9661016949152542, precision: 1.0, f_beta: 0.9827586206896551\n",
      "train: step: 1492, loss: 0.016992589458823204, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1493, loss: 0.014996401034295559, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1494, loss: 0.08007635921239853, acc: 0.9765625, recall: 0.96875, precision: 0.9841269841269841, f_beta: 0.9763779527559054\n",
      "train: step: 1495, loss: 0.02354077808558941, acc: 0.9921875, recall: 1.0, precision: 0.9841269841269841, f_beta: 0.9919999999999999\n",
      "train: step: 1496, loss: 0.009394814260303974, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1497, loss: 0.009788806550204754, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1498, loss: 0.09329710900783539, acc: 0.9765625, recall: 0.9841269841269841, precision: 0.96875, f_beta: 0.9763779527559054\n",
      "train: step: 1499, loss: 0.09836670756340027, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1500, loss: 0.05057453364133835, acc: 0.984375, recall: 0.9696969696969697, precision: 1.0, f_beta: 0.9846153846153847\n",
      "\n",
      "Evaluation:\n",
      "2019-07-26T15:23:41.893606, step: 1500, loss: 0.5486905574798584, acc: 0.8631810897435898,precision: 0.8509095287716366, recall: 0.8759914382148967, f_beta: 0.8624076973514923\n",
      "Saved model checkpoint to ../model/Bi-LSTM/model/my-model-1500\n",
      "\n",
      "train: step: 1501, loss: 0.008542600087821484, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1502, loss: 0.04735685884952545, acc: 0.984375, recall: 1.0, precision: 0.9682539682539683, f_beta: 0.9838709677419354\n",
      "train: step: 1503, loss: 0.007833058945834637, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1504, loss: 0.014414237812161446, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1505, loss: 0.04233085364103317, acc: 0.9921875, recall: 1.0, precision: 0.9821428571428571, f_beta: 0.9909909909909909\n",
      "train: step: 1506, loss: 0.05762512981891632, acc: 0.9921875, recall: 1.0, precision: 0.9833333333333333, f_beta: 0.9915966386554621\n",
      "train: step: 1507, loss: 0.04059375450015068, acc: 0.9921875, recall: 0.9855072463768116, precision: 1.0, f_beta: 0.9927007299270074\n",
      "train: step: 1508, loss: 0.009906642138957977, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1509, loss: 0.08607029169797897, acc: 0.9765625, recall: 0.95, precision: 1.0, f_beta: 0.9743589743589743\n",
      "train: step: 1510, loss: 0.06044793874025345, acc: 0.9921875, recall: 0.9852941176470589, precision: 1.0, f_beta: 0.9925925925925926\n",
      "train: step: 1511, loss: 0.008461466059088707, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1512, loss: 0.04896962270140648, acc: 0.9765625, recall: 0.9710144927536232, precision: 0.9852941176470589, f_beta: 0.9781021897810219\n",
      "train: step: 1513, loss: 0.05139685049653053, acc: 0.984375, recall: 0.9833333333333333, precision: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 1514, loss: 0.049217142164707184, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1515, loss: 0.04449326545000076, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1516, loss: 0.11202754825353622, acc: 0.9765625, recall: 0.953125, precision: 1.0, f_beta: 0.976\n",
      "train: step: 1517, loss: 0.015064726583659649, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1518, loss: 0.06393967568874359, acc: 0.984375, recall: 0.971830985915493, precision: 1.0, f_beta: 0.9857142857142858\n",
      "train: step: 1519, loss: 0.008219954557716846, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1520, loss: 0.010072024539113045, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1521, loss: 0.04581821709871292, acc: 0.9921875, recall: 1.0, precision: 0.9868421052631579, f_beta: 0.9933774834437086\n",
      "train: step: 1522, loss: 0.11537425220012665, acc: 0.9765625, recall: 1.0, precision: 0.9565217391304348, f_beta: 0.9777777777777777\n",
      "train: step: 1523, loss: 0.05229698494076729, acc: 0.9921875, recall: 1.0, precision: 0.9838709677419355, f_beta: 0.991869918699187\n",
      "train: step: 1524, loss: 0.011749538592994213, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1525, loss: 0.03462718427181244, acc: 0.9921875, recall: 1.0, precision: 0.9848484848484849, f_beta: 0.9923664122137404\n",
      "train: step: 1526, loss: 0.07492109388113022, acc: 0.984375, recall: 0.9852941176470589, precision: 0.9852941176470589, f_beta: 0.9852941176470589\n",
      "train: step: 1527, loss: 0.05069505795836449, acc: 0.984375, recall: 0.9649122807017544, precision: 1.0, f_beta: 0.9821428571428572\n",
      "train: step: 1528, loss: 0.09386063367128372, acc: 0.984375, recall: 0.96875, precision: 1.0, f_beta: 0.9841269841269841\n",
      "train: step: 1529, loss: 0.08957208693027496, acc: 0.984375, recall: 0.967741935483871, precision: 1.0, f_beta: 0.9836065573770492\n",
      "train: step: 1530, loss: 0.08778014779090881, acc: 0.9765625, recall: 0.9841269841269841, precision: 0.96875, f_beta: 0.9763779527559054\n",
      "train: step: 1531, loss: 0.05965970456600189, acc: 0.984375, recall: 0.9838709677419355, precision: 0.9838709677419355, f_beta: 0.9838709677419355\n",
      "train: step: 1532, loss: 0.06974393129348755, acc: 0.984375, recall: 0.9833333333333333, precision: 0.9833333333333333, f_beta: 0.9833333333333333\n",
      "train: step: 1533, loss: 0.022177457809448242, acc: 0.9921875, recall: 1.0, precision: 0.9827586206896551, f_beta: 0.9913043478260869\n",
      "train: step: 1534, loss: 0.03972698003053665, acc: 0.9921875, recall: 1.0, precision: 0.9830508474576272, f_beta: 0.9914529914529915\n",
      "train: step: 1535, loss: 0.018519597128033638, acc: 0.9921875, recall: 1.0, precision: 0.9855072463768116, f_beta: 0.9927007299270074\n",
      "train: step: 1536, loss: 0.11704809963703156, acc: 0.9609375, recall: 0.9852941176470589, precision: 0.9436619718309859, f_beta: 0.9640287769784172\n",
      "train: step: 1537, loss: 0.015382252633571625, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1538, loss: 0.03166505694389343, acc: 0.9921875, recall: 1.0, precision: 0.9830508474576272, f_beta: 0.9914529914529915\n",
      "train: step: 1539, loss: 0.05914109945297241, acc: 0.984375, recall: 1.0, precision: 0.9661016949152542, f_beta: 0.9827586206896551\n",
      "train: step: 1540, loss: 0.025597816333174706, acc: 0.9921875, recall: 0.9824561403508771, precision: 1.0, f_beta: 0.9911504424778761\n",
      "train: step: 1541, loss: 0.07331009209156036, acc: 0.9921875, recall: 0.9841269841269841, precision: 1.0, f_beta: 0.9919999999999999\n",
      "train: step: 1542, loss: 0.06389546394348145, acc: 0.984375, recall: 0.9848484848484849, precision: 0.9848484848484849, f_beta: 0.9848484848484849\n",
      "train: step: 1543, loss: 0.05463789775967598, acc: 0.9921875, recall: 1.0, precision: 0.9850746268656716, f_beta: 0.9924812030075187\n",
      "train: step: 1544, loss: 0.035093046724796295, acc: 0.984375, recall: 0.9836065573770492, precision: 0.9836065573770492, f_beta: 0.9836065573770492\n",
      "train: step: 1545, loss: 0.04825498163700104, acc: 0.9921875, recall: 0.9830508474576272, precision: 1.0, f_beta: 0.9914529914529915\n",
      "train: step: 1546, loss: 0.07259038835763931, acc: 0.984375, recall: 1.0, precision: 0.972972972972973, f_beta: 0.9863013698630138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1547, loss: 0.12703736126422882, acc: 0.9765625, recall: 0.96875, precision: 0.9841269841269841, f_beta: 0.9763779527559054\n",
      "train: step: 1548, loss: 0.08478166162967682, acc: 0.9765625, recall: 0.9710144927536232, precision: 0.9852941176470589, f_beta: 0.9781021897810219\n",
      "train: step: 1549, loss: 0.05510714650154114, acc: 0.984375, recall: 1.0, precision: 0.9692307692307692, f_beta: 0.9843749999999999\n",
      "train: step: 1550, loss: 0.007434182800352573, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1551, loss: 0.023899704217910767, acc: 0.9921875, recall: 1.0, precision: 0.9824561403508771, f_beta: 0.9911504424778761\n",
      "train: step: 1552, loss: 0.011967000551521778, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1553, loss: 0.08131159096956253, acc: 0.984375, recall: 1.0, precision: 0.9682539682539683, f_beta: 0.9838709677419354\n",
      "train: step: 1554, loss: 0.031063828617334366, acc: 0.984375, recall: 0.9859154929577465, precision: 0.9859154929577465, f_beta: 0.9859154929577465\n",
      "train: step: 1555, loss: 0.04729616641998291, acc: 0.9921875, recall: 0.9855072463768116, precision: 1.0, f_beta: 0.9927007299270074\n",
      "train: step: 1556, loss: 0.09977014362812042, acc: 0.9765625, recall: 1.0, precision: 0.9577464788732394, f_beta: 0.9784172661870503\n",
      "train: step: 1557, loss: 0.012352531775832176, acc: 1.0, recall: 1.0, precision: 1.0, f_beta: 1.0\n",
      "train: step: 1558, loss: 0.026147829368710518, acc: 0.9921875, recall: 1.0, precision: 0.9852941176470589, f_beta: 0.9925925925925926\n",
      "train: step: 1559, loss: 0.017189456149935722, acc: 0.9921875, recall: 1.0, precision: 0.9848484848484849, f_beta: 0.9923664122137404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0726 15:25:22.224742 140504067012352 deprecation.py:323] From <ipython-input-8-df1e8dbf2db4>:149: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0726 15:25:22.229662 140504067012352 deprecation.py:506] From <ipython-input-8-df1e8dbf2db4>:158: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Pass your op to the equivalent parameter main_op instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: step: 1560, loss: 0.032786302268505096, acc: 0.9921875, recall: 0.9859154929577465, precision: 1.0, f_beta: 0.9929078014184397\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "labelList = data.labelList\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.6  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = BiLSTM(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "        savedModelPath = \"../model/Bi-LSTM/savedModel\"\n",
    "        if os.path.exists(savedModelPath):\n",
    "            os.rmdir(savedModelPath)\n",
    "        builder = tf.saved_model.builder.SavedModelBuilder(savedModelPath)\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "                acc, recall, prec, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "                \n",
    "            elif config.numClasses > 1:\n",
    "                acc, recall, prec, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY,\n",
    "                                                              labels=labelList)\n",
    "                \n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, prec, recall, f_beta\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions],\n",
    "                feed_dict)\n",
    "            \n",
    "            if config.numClasses == 1:\n",
    "            \n",
    "                acc, precision, recall, f_beta = get_binary_metrics(pred_y=predictions, true_y=batchY)\n",
    "            elif config.numClasses > 1:\n",
    "                acc, precision, recall, f_beta = get_multi_metrics(pred_y=predictions, true_y=batchY, labels=labelList)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, precision, recall, f_beta\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                loss, acc, prec, recall, f_beta = trainStep(batchTrain[0], batchTrain[1])\n",
    "                \n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                print(\"train: step: {}, loss: {}, acc: {}, recall: {}, precision: {}, f_beta: {}\".format(\n",
    "                    currentStep, loss, acc, recall, prec, f_beta))\n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    f_betas = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, precision, recall, f_beta = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        f_betas.append(f_beta)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {},precision: {}, recall: {}, f_beta: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(precisions),\n",
    "                                                                                                       mean(recalls), mean(f_betas)))\n",
    "                    \n",
    "                if currentStep % config.training.checkpointEvery == 0:\n",
    "                    # 保存模型的另一种方法，保存checkpoint文件\n",
    "                    path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "                  \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "        outputs = {\"predictions\": tf.saved_model.utils.build_tensor_info(lstm.predictions)}\n",
    "\n",
    "        prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "                                                                                      method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "        legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "        builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "                                            signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "        builder.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n"
     ]
    }
   ],
   "source": [
    "x = \"this movie is full of references like mad max ii the wild one and many others the ladybug´s face it´s a clear reference or tribute to peter lorre this movie is a masterpiece we´ll talk much more about in the future\"\n",
    "\n",
    "# 注：下面两个词典要保证和当前加载的模型对应的词典是一致的\n",
    "with open(\"../data/wordJson/word2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx = json.load(f)\n",
    "        \n",
    "with open(\"../data/wordJson/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    label2idx = json.load(f)\n",
    "idx2label = {value: key for key, value in label2idx.items()}\n",
    "    \n",
    "xIds = [word2idx.get(item, word2idx[\"UNK\"]) for item in x.split(\" \")]\n",
    "if len(xIds) >= config.sequenceLength:\n",
    "    xIds = xIds[:config.sequenceLength]\n",
    "else:\n",
    "    xIds = xIds + [word2idx[\"PAD\"]] * (config.sequenceLength - len(xIds))\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False, gpu_options=gpu_options)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "\n",
    "    with sess.as_default():\n",
    "        checkpoint_file = tf.train.latest_checkpoint(\"../model/Bi-LSTM/model/\")\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # 获得需要喂给模型的参数，输出的结果依赖的输入值\n",
    "        inputX = graph.get_operation_by_name(\"inputX\").outputs[0]\n",
    "        dropoutKeepProb = graph.get_operation_by_name(\"dropoutKeepProb\").outputs[0]\n",
    "\n",
    "        # 获得输出的结果\n",
    "        predictions = graph.get_tensor_by_name(\"output/predictions:0\")\n",
    "\n",
    "        pred = sess.run(predictions, feed_dict={inputX: [xIds], dropoutKeepProb: 1.0})[0]\n",
    "        \n",
    "pred = [idx2label[item] for item in pred]     \n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
